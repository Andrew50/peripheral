name: Deploy to Kubernetes

on:
  # Deploy when pushing to stable branches
  push:
    branches:
      - dev
      - prod
  
  # Different trigger for pull requests with explicit configuration
  pull_request:
    types: [opened, synchronize, reopened]
    branches:
      - dev
      - prod
  
  # Manual triggering
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to deploy'
        required: true
        default: 'dev'

# Add permissions configuration here
permissions:
  contents: read
  packages: write
  id-token: write

jobs:
  # Add a dependency on the lint-and-build workflow
  check-build:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
      - name: Check if lint-and-build workflow passed
        run: |
          echo "This job ensures the lint-and-build workflow has passed before deployment"
          # This is a placeholder job that will be skipped for direct pushes
          # The actual dependency is handled by the workflow_run trigger

  deploy:
    runs-on: self-hosted  # Using self-hosted runner instead of ubuntu-latest
    needs: check-build
    if: github.event_name != 'pull_request' || success()
    # Add job-level permissions to ensure access to secrets
    permissions:
      contents: read
      packages: write
      id-token: write
    
    # Add environment variables for the job
    env:
      DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
      DOCKER_TOKEN: ${{ secrets.DOCKER_TOKEN }}
    
    steps:
      # Print debugging information before checkout
      - name: Debug Event Information
        run: |
          echo "Event name: ${{ github.event_name }}"
          echo "GitHub ref: ${{ github.ref }}"
          echo "GitHub head ref: ${{ github.head_ref }}"
          echo "GitHub base ref: ${{ github.base_ref }}"
          echo "GitHub ref name: ${{ github.ref_name }}"
          echo "Custom branch input: ${{ github.event.inputs.branch }}"

      # Determine which branch to checkout based on event type
      - name: Set checkout target
        id: set-target
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "CHECKOUT_REF=${{ github.head_ref }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.base_ref }}" >> $GITHUB_ENV
            echo "Using PR source branch for checkout: ${{ github.head_ref }}"
            echo "Using PR target branch for Docker tags: ${{ github.base_ref }}"
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ github.event.inputs.branch }}" ]; then
            echo "CHECKOUT_REF=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
            echo "Using manual input branch: ${{ github.event.inputs.branch }}"
          else
            echo "CHECKOUT_REF=${{ github.ref_name }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.ref_name }}" >> $GITHUB_ENV
            echo "Using current branch: ${{ github.ref_name }}"
          fi

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: ${{ env.CHECKOUT_REF }}
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 1
          lfs: false
      
      # Add diagnostic step to see what's available after checkout
      - name: Show Git Information
        run: |
          echo "Current directory: $(pwd)"
          echo "Directory contents: $(ls -la)"
          echo "Git status: $(git status)"
          echo "Current branch: $(git branch --show-current)"
          git --version
      
      # Verify secrets are available without exposing them
      - name: Verify Docker Credentials
        run: |
          echo "Docker username secret: $(if [ -n "${{ secrets.DOCKER_USERNAME }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Docker token secret: $(if [ -n "${{ secrets.DOCKER_TOKEN }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
      
      # Setup and configuration
      - name: Setup deployment
        run: |
          # Function to log messages with timestamps
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }

          # Function to log errors
          error_log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
          }

          # Print diagnostic information
          log "Deployment script diagnostics:"
          log "Current working directory: $(pwd)"
          log "Current user: $(whoami)"
          log "Current branch: ${{ env.TARGET_BRANCH }}"
          
          # Sanitize branch name for Docker tags (replace / with - and other invalid characters)
          DOCKER_TAG=$(echo "${{ env.TARGET_BRANCH }}" | sed 's/\//-/g' | sed 's/[^a-zA-Z0-9_.-]/-/g')
          echo "DOCKER_TAG=${DOCKER_TAG}" >> $GITHUB_ENV
          
          log "Docker tag: ${DOCKER_TAG}"
          log "Files in current directory: $(ls -la)"
          log "Starting deployment process for branch: ${{ env.TARGET_BRANCH }}..."
          log "Using already checked out code for deployment..."

      # Build Docker images
      - name: Build Docker Images
        run: |
          # Function to log messages with timestamps
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Function to log errors
          error_log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
          }
          
          # Modified check for Docker credentials using env context
          if [ -z "$DOCKER_USERNAME" ]; then
            error_log "DOCKER_USERNAME secret is not available"
            error_log "Please add the DOCKER_USERNAME secret in your GitHub repository:"
            error_log "1. Go to your repository on GitHub"
            error_log "2. Navigate to Settings > Secrets and variables > Actions"
            error_log "3. Click 'New repository secret'"
            error_log "4. Name: DOCKER_USERNAME"
            error_log "5. Value: Your Docker Hub username"
            error_log "6. Click 'Add secret'"
            exit 1
          fi
          
          log "Building Docker images with tag: ${{ env.DOCKER_TAG }}..."
          log "Using Docker username: $DOCKER_USERNAME"
          
          docker build -t $DOCKER_USERNAME/frontend:${{ env.DOCKER_TAG }} -f frontend/Dockerfile.prod frontend
          docker build -t $DOCKER_USERNAME/backend:${{ env.DOCKER_TAG }} -f backend/Dockerfile.prod backend
          docker build -t $DOCKER_USERNAME/worker:${{ env.DOCKER_TAG }} -f worker/Dockerfile.prod worker
          docker build -t $DOCKER_USERNAME/tf:${{ env.DOCKER_TAG }} -f tf/Dockerfile.prod tf
          docker build -t $DOCKER_USERNAME/db:${{ env.DOCKER_TAG }} -f db/Dockerfile.prod db
          
          # For prod branch, also tag as latest
          # For dev branch, also tag as development
          if [ "${{ env.TARGET_BRANCH }}" = "prod" ]; then
            log "Tagging images as 'latest' for production..."
            docker tag $DOCKER_USERNAME/frontend:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/frontend:latest
            docker tag $DOCKER_USERNAME/backend:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/backend:latest
            docker tag $DOCKER_USERNAME/worker:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/worker:latest
            docker tag $DOCKER_USERNAME/tf:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/tf:latest
            docker tag $DOCKER_USERNAME/db:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/db:latest
          elif [ "${{ env.TARGET_BRANCH }}" = "dev" ]; then
            log "Tagging images as 'development' for development environment..."
            docker tag $DOCKER_USERNAME/frontend:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/frontend:development
            docker tag $DOCKER_USERNAME/backend:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/backend:development
            docker tag $DOCKER_USERNAME/worker:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/worker:development
            docker tag $DOCKER_USERNAME/tf:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/tf:development
            docker tag $DOCKER_USERNAME/db:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/db:development
          fi

      # Push Docker images
      - name: Login to Docker Hub and Push Images
        run: |
          # Function to log messages with timestamps
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Function to log errors
          error_log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
          }
          
          log "Preparing to push Docker images to registry with tag: ${{ env.DOCKER_TAG }}..."
          
          # Check Docker token and fail if not set
          if [ -z "$DOCKER_TOKEN" ]; then
            error_log "DOCKER_TOKEN secret is not available"
            error_log "Please add the DOCKER_TOKEN secret in your GitHub repository:"
            error_log "1. Go to your repository on GitHub"
            error_log "2. Navigate to Settings > Secrets and variables > Actions"
            error_log "3. Click 'New repository secret'"
            error_log "4. Name: DOCKER_TOKEN"
            error_log "5. Value: Your Docker Hub access token"
            error_log "6. Click 'Add secret'"
            exit 1
          fi
          
          # Directly use environment variables for Docker login
          log "Logging into Docker Hub using username: $DOCKER_USERNAME..."
          echo "$DOCKER_TOKEN" | docker login -u "$DOCKER_USERNAME" --password-stdin || {
            error_log "Docker login failed. Please check your credentials."
            exit 1
          }
          
          log "Docker login successful, proceeding with image push..."
          
          # Push images with the branch-specific tag
          docker push $DOCKER_USERNAME/frontend:${{ env.DOCKER_TAG }}
          docker push $DOCKER_USERNAME/backend:${{ env.DOCKER_TAG }}
          docker push $DOCKER_USERNAME/worker:${{ env.DOCKER_TAG }}
          docker push $DOCKER_USERNAME/tf:${{ env.DOCKER_TAG }}
          docker push $DOCKER_USERNAME/db:${{ env.DOCKER_TAG }}
          
          # Push additional tags based on branch
          if [ "${{ env.TARGET_BRANCH }}" = "prod" ]; then
            log "Pushing 'latest' tagged images..."
            docker push $DOCKER_USERNAME/frontend:latest
            docker push $DOCKER_USERNAME/backend:latest
            docker push $DOCKER_USERNAME/worker:latest
            docker push $DOCKER_USERNAME/tf:latest
            docker push $DOCKER_USERNAME/db:latest
          elif [ "${{ env.TARGET_BRANCH }}" = "dev" ]; then
            log "Pushing 'development' tagged images..."
            docker push $DOCKER_USERNAME/frontend:development
            docker push $DOCKER_USERNAME/backend:development
            docker push $DOCKER_USERNAME/worker:development
            docker push $DOCKER_USERNAME/tf:development
            docker push $DOCKER_USERNAME/db:development
          fi

      # Deploy to Kubernetes
      - name: Deploy to Kubernetes
        continue-on-error: false  # Changed from true to false to ensure workflow fails if deployment fails
        run: |
          # Function to log messages with timestamps
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Function to log errors
          error_log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
          }
          
          # Function to check deployment status
          check_deployment_status() {
            local deployment=$1
            status=$(kubectl rollout status deployment/$deployment --timeout=10s 2>/dev/null || echo "FAILED")
            if [[ $status != *"FAILED"* ]]; then
              return 0  # Success
            else
              return 1  # Failed or still in progress
            fi
          }

          # Function to collect debug information for failed deployments
          collect_debug_info() {
            local deployment=$1
            log "Collecting debug information for $deployment..."
            kubectl describe deployment $deployment
            kubectl get pods -l app=$deployment
            
            # Capture more detailed logs with error filtering
            log "Last 50 lines of logs for $deployment pods:"
            kubectl logs -l app=$deployment --tail=50 || true
            
            # Additional debugging for Redis connection issues if this is worker deployment
            if [ "$deployment" = "worker" ]; then
              log "Checking Redis connectivity from worker perspective..."
              log "Redis pod status:"
              kubectl get pods -l app=cache
              kubectl describe pods -l app=cache
              log "Testing Redis connectivity:"
              # Try to get a shell in a worker pod to test redis connection manually
              WORKER_POD=$(kubectl get pods -l app=worker -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
              if [ -n "$WORKER_POD" ]; then
                kubectl exec -it $WORKER_POD -- sh -c "echo 'Testing Redis connection' && python3 -c 'import redis; import os; print(\"Redis Host:\", os.environ.get(\"REDIS_HOST\")); print(\"Redis Port:\", os.environ.get(\"REDIS_PORT\"));'" || true
              fi
              
              log "Checking Redis service endpoints:"
              kubectl get endpoints cache
            fi
          }
          
          # Function to record the current version of all deployments (for rollback)
          record_current_versions() {
            log "Recording current deployment versions for potential rollback..."
            mkdir -p /tmp/k8s-rollback
            
            for deployment in "${deployments[@]}"; do
              # Get the current image for each deployment
              current_image=$(kubectl get deployment $deployment -o jsonpath="{.spec.template.spec.containers[0].image}" 2>/dev/null || echo "")
              if [ -n "$current_image" ]; then
                echo "$current_image" > /tmp/k8s-rollback/$deployment.version
                log "Current $deployment version: $current_image"
              else
                log "Warning: Could not determine current version for $deployment"
              fi
            done
          }
          
          # Function to roll back all deployments if any fail
          rollback_all_deployments() {
            log "Rolling back all deployments to previous versions..."
            
            for deployment in "${deployments[@]}"; do
              if [ -f "/tmp/k8s-rollback/$deployment.version" ]; then
                previous_image=$(cat /tmp/k8s-rollback/$deployment.version)
                if [ -n "$previous_image" ]; then
                  log "Rolling back $deployment to $previous_image"
                  kubectl set image deployment/$deployment $deployment=$previous_image || log "Warning: Rollback failed for $deployment"
                  kubectl rollout status deployment/$deployment --timeout=60s || log "Warning: Rollback status check failed for $deployment"
                fi
              fi
            done
            
            log "Rollback completed. Some services may need manual attention."
          }
          
          # Apply Kubernetes configurations
          log "Applying Kubernetes configurations..."
          if [ "${{ env.TARGET_BRANCH }}" = "dev" ]; then
            # Dev-specific configurations (if they exist)
            if [ -d "prod/config/dev" ]; then
              kubectl apply -f prod/config/dev
            else
              # Fall back to prod config if no dev config exists
              kubectl apply -f prod/config
            fi
          else
            # Default to prod config for prod branch or any other branch
            kubectl apply -f prod/config
          fi
          
          # Add debugging information about the cluster and deployments
          log "Getting cluster information before deployment..."
          kubectl get nodes
          kubectl get pods -o wide
          kubectl get deployments
          
          # Check if backend deployment exists
          if ! kubectl get deployment backend &> /dev/null; then
            error_log "Backend deployment not found! Checking namespace..."
            kubectl get namespaces
            # Fail since we can't proceed without the backend
            log "Cannot proceed without backend deployment."
            exit 1
          fi
          
          # Define the deployments to update and record current versions for rollback
          deployments=("backend" "frontend" "worker" "tf")
          record_current_versions
          
          # Perform rolling updates for all deployments at once
          log "Performing rolling updates for zero downtime using images tagged with: ${{ env.DOCKER_TAG }}..."
          
          # Update all deployments at once
          for deployment in "${deployments[@]}"; do
            log "Updating $deployment deployment..."
            kubectl set image deployment/$deployment $deployment=$DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }}
          done
          
          # Monitor all deployments in a loop
          log "Monitoring all deployments..."
          
          # Set timeout values for each deployment (in seconds)
          declare -A timeouts
          timeouts=([backend]=300 [frontend]=180 [worker]=180 [tf]=180)
          
          # Track start time for each deployment
          declare -A start_times
          declare -A deployment_status
          for deployment in "${deployments[@]}"; do
            start_times[$deployment]=$(date +%s)
            deployment_status[$deployment]="pending"
          done
          
          # Continue monitoring until all deployments are completed or timed out
          deployment_failed=false
          while true; do
            all_completed=true
            
            for deployment in "${deployments[@]}"; do
              # Skip if this deployment is already completed or failed
              if [ "${deployment_status[$deployment]}" != "pending" ]; then
                continue
              fi
              
              # Check if the deployment is successful
              if check_deployment_status $deployment; then
                log "$deployment deployment succeeded."
                deployment_status[$deployment]="success"
                continue
              fi
              
              # Check if the deployment has timed out
              current_time=$(date +%s)
              elapsed=$((current_time - start_times[$deployment]))
              timeout=${timeouts[$deployment]}
              
              if [ $elapsed -gt $timeout ]; then
                error_log "$deployment deployment timed out after ${elapsed} seconds."
                collect_debug_info $deployment
                deployment_status[$deployment]="failed"
                deployment_failed=true
              else
                # Still waiting for this deployment
                all_completed=false
                log "Still waiting for $deployment deployment... (${elapsed}s elapsed, timeout: ${timeout}s)"
              fi
            done
            
            # Check if all deployments are completed (success or failed)
            if [ "$all_completed" = true ]; then
              break
            fi
            
            # Wait before checking again
            sleep 10
          done
          
          # Check for any failed deployments and rollback if needed
          if [ "$deployment_failed" = true ]; then
            error_log "One or more deployments failed. Rolling back all deployments to maintain synchronization."
            rollback_all_deployments
            
            # Report final status after rollback
            log "Deployment FAILED and rollback completed. Status summary:"
            for deployment in "${deployments[@]}"; do
              if [ "${deployment_status[$deployment]}" = "failed" ]; then
                log "  - $deployment: FAILED (rolled back)"
              else
                log "  - $deployment: ${deployment_status[$deployment]} (rolled back)"
              fi
            done
            
            # Final status check after rollback
            log "Checking final deployment status after rollback..."
            kubectl get deployments
            
            log "Deployment process failed for branch ${{ env.TARGET_BRANCH }}. All services have been rolled back to previous versions."
            exit 1  # Fail the GitHub action
          else
            # Report final status
            log "Deployment process completed. Status summary:"
            for deployment in "${deployments[@]}"; do
              log "  - $deployment: ${deployment_status[$deployment]}"
            done
            
            # Final status check
            log "Checking final deployment status..."
            kubectl get deployments
            
            log "Deployment process completed successfully for branch ${{ env.TARGET_BRANCH }}."
          fi

  deploy-if-build-succeeded:
    runs-on: self-hosted
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    # Add this environment section:
    environment:
      name: ${{ github.event.workflow_run.head_branch == 'prod' && 'production' || 'development' }}

    steps:
      # Print debugging information before checkout
      - name: Debug Event Information
        run: |
          echo "Event name: ${{ github.event_name }}"
          echo "GitHub ref: ${{ github.ref }}"
          echo "GitHub head ref: ${{ github.head_ref }}"
          echo "GitHub base ref: ${{ github.base_ref }}"
          echo "GitHub ref name: ${{ github.ref_name }}"
          echo "Custom branch input: ${{ github.event.inputs.branch }}"

      # Determine which branch to checkout based on event type
      - name: Set checkout target
        id: set-target
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "CHECKOUT_REF=${{ github.head_ref }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.base_ref }}" >> $GITHUB_ENV
            echo "Using PR source branch for checkout: ${{ github.head_ref }}"
            echo "Using PR target branch for Docker tags: ${{ github.base_ref }}"
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ github.event.inputs.branch }}" ]; then
            echo "CHECKOUT_REF=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
            echo "Using manual input branch: ${{ github.event.inputs.branch }}"
          else
            echo "CHECKOUT_REF=${{ github.ref_name }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.ref_name }}" >> $GITHUB_ENV
            echo "Using current branch: ${{ github.ref_name }}"
          fi

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: ${{ env.CHECKOUT_REF }}
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 1
          lfs: false
      
      # Add diagnostic step to see what's available after checkout
      - name: Show Git Information
        run: |
          echo "Current directory: $(pwd)"
          echo "Directory contents: $(ls -la)"
          echo "Git status: $(git status)"
          echo "Current branch: $(git branch --show-current)"
          git --version
      
      # Verify secrets are available without exposing them
      - name: Verify Docker Credentials
        run: |
          echo "Docker username secret: $(if [ -n "${{ secrets.DOCKER_USERNAME }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Docker token secret: $(if [ -n "${{ secrets.DOCKER_TOKEN }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
      
      # Setup and configuration
      - name: Setup deployment
        run: |
          # Function to log messages with timestamps
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }

          # Function to log errors
          error_log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
          }

          # Print diagnostic information
          log "Deployment script diagnostics:"
          log "Current working directory: $(pwd)"
          log "Current user: $(whoami)"
          log "Current branch: ${{ env.TARGET_BRANCH }}"
          
          # Sanitize branch name for Docker tags (replace / with - and other invalid characters)
          DOCKER_TAG=$(echo "${{ env.TARGET_BRANCH }}" | sed 's/\//-/g' | sed 's/[^a-zA-Z0-9_.-]/-/g')
          echo "DOCKER_TAG=${DOCKER_TAG}" >> $GITHUB_ENV
          
          log "Docker tag: ${DOCKER_TAG}"
          log "Files in current directory: $(ls -la)"
          log "Starting deployment process for branch: ${{ env.TARGET_BRANCH }}..."
          log "Using already checked out code for deployment..."

      # Build Docker images
      - name: Build Docker Images
        run: |
          # Function to log messages with timestamps
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Function to log errors
          error_log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
          }
          
          # Modified check for Docker credentials using env context
          if [ -z "$DOCKER_USERNAME" ]; then
            error_log "DOCKER_USERNAME secret is not available"
            error_log "Please add the DOCKER_USERNAME secret in your GitHub repository:"
            error_log "1. Go to your repository on GitHub"
            error_log "2. Navigate to Settings > Secrets and variables > Actions"
            error_log "3. Click 'New repository secret'"
            error_log "4. Name: DOCKER_USERNAME"
            error_log "5. Value: Your Docker Hub username"
            error_log "6. Click 'Add secret'"
            exit 1
          fi
          
          log "Building Docker images with tag: ${{ env.DOCKER_TAG }}..."
          log "Using Docker username: $DOCKER_USERNAME"
          
          docker build -t $DOCKER_USERNAME/frontend:${{ env.DOCKER_TAG }} -f frontend/Dockerfile.prod frontend
          docker build -t $DOCKER_USERNAME/backend:${{ env.DOCKER_TAG }} -f backend/Dockerfile.prod backend
          docker build -t $DOCKER_USERNAME/worker:${{ env.DOCKER_TAG }} -f worker/Dockerfile.prod worker
          docker build -t $DOCKER_USERNAME/tf:${{ env.DOCKER_TAG }} -f tf/Dockerfile.prod tf
          docker build -t $DOCKER_USERNAME/db:${{ env.DOCKER_TAG }} -f db/Dockerfile.prod db
          
          # For prod branch, also tag as latest
          # For dev branch, also tag as development
          if [ "${{ env.TARGET_BRANCH }}" = "prod" ]; then
            log "Tagging images as 'latest' for production..."
            docker tag $DOCKER_USERNAME/frontend:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/frontend:latest
            docker tag $DOCKER_USERNAME/backend:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/backend:latest
            docker tag $DOCKER_USERNAME/worker:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/worker:latest
            docker tag $DOCKER_USERNAME/tf:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/tf:latest
            docker tag $DOCKER_USERNAME/db:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/db:latest
          elif [ "${{ env.TARGET_BRANCH }}" = "dev" ]; then
            log "Tagging images as 'development' for development environment..."
            docker tag $DOCKER_USERNAME/frontend:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/frontend:development
            docker tag $DOCKER_USERNAME/backend:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/backend:development
            docker tag $DOCKER_USERNAME/worker:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/worker:development
            docker tag $DOCKER_USERNAME/tf:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/tf:development
            docker tag $DOCKER_USERNAME/db:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/db:development
          fi

      # Push Docker images
      - name: Login to Docker Hub and Push Images
        run: |
          # Function to log messages with timestamps
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Function to log errors
          error_log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
          }
          
          log "Preparing to push Docker images to registry with tag: ${{ env.DOCKER_TAG }}..."
          
          # Check Docker token and fail if not set
          if [ -z "$DOCKER_TOKEN" ]; then
            error_log "DOCKER_TOKEN secret is not available"
            error_log "Please add the DOCKER_TOKEN secret in your GitHub repository:"
            error_log "1. Go to your repository on GitHub"
            error_log "2. Navigate to Settings > Secrets and variables > Actions"
            error_log "3. Click 'New repository secret'"
            error_log "4. Name: DOCKER_TOKEN"
            error_log "5. Value: Your Docker Hub access token"
            error_log "6. Click 'Add secret'"
            exit 1
          fi

          
          # Directly use environment variables for Docker login
          log "Logging into Docker Hub using username: $DOCKER_USERNAME..."
          echo "$DOCKER_TOKEN" | docker login -u "$DOCKER_USERNAME" --password-stdin || {
            error_log "Docker login failed. Please check your credentials."
            exit 1
          }
          
          log "Docker login successful, proceeding with image push..."
          
          # Push images with the branch-specific tag
          docker push $DOCKER_USERNAME/frontend:${{ env.DOCKER_TAG }}
          docker push $DOCKER_USERNAME/backend:${{ env.DOCKER_TAG }}
          docker push $DOCKER_USERNAME/worker:${{ env.DOCKER_TAG }}
          docker push $DOCKER_USERNAME/tf:${{ env.DOCKER_TAG }}
          docker push $DOCKER_USERNAME/db:${{ env.DOCKER_TAG }}
          
          # Push additional tags based on branch
          if [ "${{ env.TARGET_BRANCH }}" = "prod" ]; then
            log "Pushing 'latest' tagged images..."
            docker push $DOCKER_USERNAME/frontend:latest
            docker push $DOCKER_USERNAME/backend:latest
            docker push $DOCKER_USERNAME/worker:latest
            docker push $DOCKER_USERNAME/tf:latest
            docker push $DOCKER_USERNAME/db:latest
          elif [ "${{ env.TARGET_BRANCH }}" = "dev" ]; then
            log "Pushing 'development' tagged images..."
            docker push $DOCKER_USERNAME/frontend:development
            docker push $DOCKER_USERNAME/backend:development
            docker push $DOCKER_USERNAME/worker:development
            docker push $DOCKER_USERNAME/tf:development
            docker push $DOCKER_USERNAME/db:development
          fi

      # Deploy to Kubernetes
      - name: Deploy to Kubernetes
        continue-on-error: false  # Changed from true to false to ensure workflow fails if deployment fails
        run: |
          # Function to log messages with timestamps
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Function to log errors
          error_log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1" >&2
          }
          
          # Function to check deployment status
          check_deployment_status() {
            local deployment=$1
            status=$(kubectl rollout status deployment/$deployment --timeout=10s 2>/dev/null || echo "FAILED")
            if [[ $status != *"FAILED"* ]]; then
              return 0  # Success
            else
              return 1  # Failed or still in progress
            fi
          }

          # Function to collect debug information for failed deployments
          collect_debug_info() {
            local deployment=$1
            log "Collecting debug information for $deployment..."
            kubectl describe deployment $deployment
            kubectl get pods -l app=$deployment
            
            # Capture more detailed logs with error filtering
            log "Last 50 lines of logs for $deployment pods:"
            kubectl logs -l app=$deployment --tail=50 || true
            
            # Additional debugging for Redis connection issues if this is worker deployment
            if [ "$deployment" = "worker" ]; then
              log "Checking Redis connectivity from worker perspective..."
              log "Redis pod status:"
              kubectl get pods -l app=cache
              kubectl describe pods -l app=cache
              log "Testing Redis connectivity:"
              # Try to get a shell in a worker pod to test redis connection manually
              WORKER_POD=$(kubectl get pods -l app=worker -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
              if [ -n "$WORKER_POD" ]; then
                kubectl exec -it $WORKER_POD -- sh -c "echo 'Testing Redis connection' && python3 -c 'import redis; import os; print(\"Redis Host:\", os.environ.get(\"REDIS_HOST\")); print(\"Redis Port:\", os.environ.get(\"REDIS_PORT\"));'" || true
              fi
              
              log "Checking Redis service endpoints:"
              kubectl get endpoints cache
            fi
          }
          
          # Function to record the current version of all deployments (for rollback)
          record_current_versions() {
            log "Recording current deployment versions for potential rollback..."
            mkdir -p /tmp/k8s-rollback
            
            for deployment in "${deployments[@]}"; do
              # Get the current image for each deployment
              current_image=$(kubectl get deployment $deployment -o jsonpath="{.spec.template.spec.containers[0].image}" 2>/dev/null || echo "")
              if [ -n "$current_image" ]; then
                echo "$current_image" > /tmp/k8s-rollback/$deployment.version
                log "Current $deployment version: $current_image"
              else
                log "Warning: Could not determine current version for $deployment"
              fi
            done
          }
          
          # Function to roll back all deployments if any fail
          rollback_all_deployments() {
            log "Rolling back all deployments to previous versions..."
            
            for deployment in "${deployments[@]}"; do
              if [ -f "/tmp/k8s-rollback/$deployment.version" ]; then
                previous_image=$(cat /tmp/k8s-rollback/$deployment.version)
                if [ -n "$previous_image" ]; then
                  log "Rolling back $deployment to $previous_image"
                  kubectl set image deployment/$deployment $deployment=$previous_image || log "Warning: Rollback failed for $deployment"
                  kubectl rollout status deployment/$deployment --timeout=60s || log "Warning: Rollback status check failed for $deployment"
                fi
              fi
            done
            
            log "Rollback completed. Some services may need manual attention."
          }
          
          # Apply Kubernetes configurations
          log "Applying Kubernetes configurations..."
          if [ "${{ env.TARGET_BRANCH }}" = "dev" ]; then
            # Dev-specific configurations (if they exist)
            if [ -d "prod/config/dev" ]; then
              kubectl apply -f prod/config/dev
            else
              # Fall back to prod config if no dev config exists
              kubectl apply -f prod/config
            fi
          else
            # Default to prod config for prod branch or any other branch
            kubectl apply -f prod/config
          fi
          
          # Add debugging information about the cluster and deployments
          log "Getting cluster information before deployment..."
          kubectl get nodes
          kubectl get pods -o wide
          kubectl get deployments
          
          # Check if backend deployment exists
          if ! kubectl get deployment backend &> /dev/null; then
            error_log "Backend deployment not found! Checking namespace..."
            kubectl get namespaces
            # Fail since we can't proceed without the backend
            log "Cannot proceed without backend deployment."
            exit 1
          fi
          
          # Define the deployments to update and record current versions for rollback
          deployments=("backend" "frontend" "worker" "tf")
          record_current_versions
          
          # Perform rolling updates for all deployments at once
          log "Performing rolling updates for zero downtime using images tagged with: ${{ env.DOCKER_TAG }}..."
          
          # Update all deployments at once
          for deployment in "${deployments[@]}"; do
            log "Updating $deployment deployment..."
            kubectl set image deployment/$deployment $deployment=$DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }}
          done
          
          # Monitor all deployments in a loop
          log "Monitoring all deployments..."
          
          # Set timeout values for each deployment (in seconds)
          declare -A timeouts
          timeouts=([backend]=300 [frontend]=180 [worker]=180 [tf]=180)
          
          # Track start time for each deployment
          declare -A start_times
          declare -A deployment_status
          for deployment in "${deployments[@]}"; do
            start_times[$deployment]=$(date +%s)
            deployment_status[$deployment]="pending"
          done
          
          # Continue monitoring until all deployments are completed or timed out
          deployment_failed=false
          while true; do
            all_completed=true
            
            for deployment in "${deployments[@]}"; do
              # Skip if this deployment is already completed or failed
              if [ "${deployment_status[$deployment]}" != "pending" ]; then
                continue
              fi
              
              # Check if the deployment is successful
              if check_deployment_status $deployment; then
                log "$deployment deployment succeeded."
                deployment_status[$deployment]="success"
                continue
              fi
              
              # Check if the deployment has timed out
              current_time=$(date +%s)
              elapsed=$((current_time - start_times[$deployment]))
              timeout=${timeouts[$deployment]}
              
              if [ $elapsed -gt $timeout ]; then
                error_log "$deployment deployment timed out after ${elapsed} seconds."
                collect_debug_info $deployment
                deployment_status[$deployment]="failed"
                deployment_failed=true
              else
                # Still waiting for this deployment
                all_completed=false
                log "Still waiting for $deployment deployment... (${elapsed}s elapsed, timeout: ${timeout}s)"
              fi
            done
            
            # Check if all deployments are completed (success or failed)
            if [ "$all_completed" = true ]; then
              break
            fi
            
            # Wait before checking again
            sleep 10
          done
          
          # Check for any failed deployments and rollback if needed
          if [ "$deployment_failed" = true ]; then
            error_log "One or more deployments failed. Rolling back all deployments to maintain synchronization."
            rollback_all_deployments
            
            # Report final status after rollback
            log "Deployment FAILED and rollback completed. Status summary:"
            for deployment in "${deployments[@]}"; do
              if [ "${deployment_status[$deployment]}" = "failed" ]; then
                log "  - $deployment: FAILED (rolled back)"
              else
                log "  - $deployment: ${deployment_status[$deployment]} (rolled back)"
              fi
            done
            
            # Final status check after rollback
            log "Checking final deployment status after rollback..."
            kubectl get deployments
            
            log "Deployment process failed for branch ${{ env.TARGET_BRANCH }}. All services have been rolled back to previous versions."
            exit 1  # Fail the GitHub action
          else
            # Report final status
            log "Deployment process completed. Status summary:"
            for deployment in "${deployments[@]}"; do
              log "  - $deployment: ${deployment_status[$deployment]}"
            done
            
            # Final status check
            log "Checking final deployment status..."
            kubectl get deployments
            
            log "Deployment process completed successfully for branch ${{ env.TARGET_BRANCH }}."
          fi 