name: Deploy to Production Environment

on:
  push:
    branches:
      - prod
  pull_request:
    branches:
      - prod

jobs:
  deploy-primary:
    runs-on: ubuntu-latest
    environment: prod
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      # Use ssh-agent for improved key management
      - name: Set up SSH key using ssh-agent
        uses: webfactory/ssh-agent@v0.7.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}
      
      # Set up Cloudflare access
      - name: Configure Cloudflare Access
        env:
          REMOTE_HOST: ${{ secrets.REMOTE_HOST || 'ssh.atlantis.trading' }}
          REMOTE_USER: ${{ secrets.REMOTE_USER || 'aj' }}
          CLOUDFLARE_CERT: ${{ secrets.CLOUDFLARE_CERT }}
        run: |
          # Install Cloudflared
          curl -L --output cloudflared.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
          sudo dpkg -i cloudflared.deb
          
          # Set up Cloudflare certificate
          mkdir -p ~/.cloudflared
          echo "$CLOUDFLARE_CERT" > ~/.cloudflared/cert.pem
          chmod 600 ~/.cloudflared/cert.pem
          
          # Create SSH config
          mkdir -p ~/.ssh
          echo "Host $REMOTE_HOST" > ~/.ssh/config
          echo "    ProxyCommand cloudflared access ssh --hostname %h" >> ~/.ssh/config
          echo "    User $REMOTE_USER" >> ~/.ssh/config
          echo "    StrictHostKeyChecking no" >> ~/.ssh/config
          echo "    UserKnownHostsFile /dev/null" >> ~/.ssh/config
          echo "    ServerAliveInterval 60" >> ~/.ssh/config
          chmod 600 ~/.ssh/config
          
          # Test SSH connection
          ssh -v "$REMOTE_HOST" "echo SSH connection successful" || {
            echo "SSH connection failed"
            exit 1
          }

      # Deploy to primary server
      - name: Deploy to production server
        env:
          REMOTE_USER: ${{ secrets.REMOTE_USER || 'aj' }}
          REMOTE_HOST: ${{ secrets.REMOTE_HOST || 'ssh.atlantis.trading' }}
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
          DB_ROOT_PASSWORD: ${{ secrets.DB_ROOT_PASSWORD }}
        run: |
          # Execute deployment script on remote server
          ssh "$REMOTE_HOST" '
            cd /home/aj/dev/study
            
            # Update code
            git fetch origin
            git checkout prod || git checkout -b prod origin/prod
            git pull origin prod
            
            # Generate tag from commit SHA
            SHA_TAG=$(git rev-parse --short HEAD)
            
            # Docker login
            docker login docker.io -u "'$DOCKERHUB_USERNAME'" -p "'$DOCKERHUB_TOKEN'" || {
              echo "Docker login failed"
              exit 1
            }
            
            # Ensure Minikube is running
            if ! minikube status &>/dev/null; then
              echo "Starting Minikube..."
              minikube start --driver=docker || minikube start --driver=none
            fi
            
            # Configure kubectl for Minikube
            eval $(minikube -p minikube docker-env)
            
            # Build and push images
            echo "Building and pushing images with tag: $SHA_TAG"
            
            # Backend
            docker build -t "'$DOCKERHUB_USERNAME'"/backend:latest -t "'$DOCKERHUB_USERNAME'"/backend:$SHA_TAG -f ./backend/Dockerfile.prod ./backend
            docker push "'$DOCKERHUB_USERNAME'"/backend:latest
            docker push "'$DOCKERHUB_USERNAME'"/backend:$SHA_TAG
            
            # Frontend
            docker build -t "'$DOCKERHUB_USERNAME'"/frontend:latest -t "'$DOCKERHUB_USERNAME'"/frontend:$SHA_TAG -f ./frontend/Dockerfile.prod ./frontend
            docker push "'$DOCKERHUB_USERNAME'"/frontend:latest
            docker push "'$DOCKERHUB_USERNAME'"/frontend:$SHA_TAG
            
            # Worker
            docker build -t "'$DOCKERHUB_USERNAME'"/worker:latest -t "'$DOCKERHUB_USERNAME'"/worker:$SHA_TAG -f ./worker/Dockerfile ./worker
            docker push "'$DOCKERHUB_USERNAME'"/worker:latest
            docker push "'$DOCKERHUB_USERNAME'"/worker:$SHA_TAG
            
            # Ensure prod/config directory exists
            mkdir -p prod/config
            
            # Debug: List files in prod/config directory
            echo "Listing files in prod/config directory:"
            ls -la prod/config/
            
            # Run the script to ensure all config files exist
            chmod +x scripts/ensure-config-files.sh
            ./scripts/ensure-config-files.sh "'$DOCKERHUB_USERNAME'" "'$DB_ROOT_PASSWORD'"
            
            # Verify config files exist
            echo "Verifying config files..."
            for file in db.yaml cache.yaml backend.yaml worker.yaml frontend.yaml nginx.yaml; do
              if [ ! -f "prod/config/$file" ]; then
                echo "Error: prod/config/$file not found"
                exit 1
              else
                echo "âœ“ Found prod/config/$file"
              fi
            done
            
            # Update image tags in manifests
            sed -i "s|image: "'$DOCKERHUB_USERNAME'"/backend:latest|image: "'$DOCKERHUB_USERNAME'"/backend:$SHA_TAG|g" prod/config/backend.yaml
            sed -i "s|image: "'$DOCKERHUB_USERNAME'"/frontend:latest|image: "'$DOCKERHUB_USERNAME'"/frontend:$SHA_TAG|g" prod/config/frontend.yaml
            sed -i "s|image: "'$DOCKERHUB_USERNAME'"/worker:latest|image: "'$DOCKERHUB_USERNAME'"/worker:$SHA_TAG|g" prod/config/worker.yaml
            
            # Create namespace if it doesn't exist
            kubectl create namespace default --dry-run=client -o yaml | kubectl apply -f -
            
            # Apply Kubernetes manifests in order with proper error handling
            echo "Applying Kubernetes manifests..."
            
            echo "Applying database configuration..."
            kubectl apply -f prod/config/db.yaml
            if [ $? -ne 0 ]; then
              echo "Failed to apply database configuration. Check the file and try again."
              exit 1
            fi
            
            echo "Waiting for database to be ready..."
            kubectl rollout status deployment/db --timeout=600s || {
              echo "Database deployment timed out. Running debug script..."
              ./scripts/debug-deployment.sh default
              exit 1
            }
            
            echo "Applying cache configuration..."
            kubectl apply -f prod/config/cache.yaml
            if [ $? -ne 0 ]; then
              echo "Failed to apply cache configuration. Check the file and try again."
              exit 1
            fi
            
            echo "Waiting for cache to be ready..."
            kubectl rollout status deployment/cache --timeout=600s || {
              echo "Cache deployment timed out. Running debug script..."
              ./scripts/debug-deployment.sh default
              exit 1
            }
            
            echo "Applying backend configuration..."
            kubectl apply -f prod/config/backend.yaml
            if [ $? -ne 0 ]; then
              echo "Failed to apply backend configuration. Check the file and try again."
              exit 1
            fi
            
            echo "Waiting for backend to be ready..."
            kubectl rollout status deployment/backend --timeout=600s || {
              echo "Backend deployment timed out. Running debug script..."
              ./scripts/debug-deployment.sh default
              exit 1
            }
            
            echo "Applying worker configuration..."
            kubectl apply -f prod/config/worker.yaml
            if [ $? -ne 0 ]; then
              echo "Failed to apply worker configuration. Check the file and try again."
              exit 1
            fi
            
            echo "Waiting for worker to be ready..."
            kubectl rollout status deployment/worker --timeout=600s || {
              echo "Worker deployment timed out. Running debug script..."
              ./scripts/debug-deployment.sh default
              exit 1
            }
            
            echo "Applying frontend configuration..."
            kubectl apply -f prod/config/frontend.yaml
            if [ $? -ne 0 ]; then
              echo "Failed to apply frontend configuration. Check the file and try again."
              exit 1
            fi
            
            echo "Waiting for frontend to be ready..."
            kubectl rollout status deployment/frontend --timeout=600s || {
              echo "Frontend deployment timed out. Running debug script..."
              ./scripts/debug-deployment.sh default
              exit 1
            }
            
            echo "Applying nginx configuration..."
            kubectl apply -f prod/config/nginx.yaml
            if [ $? -ne 0 ]; then
              echo "Failed to apply nginx configuration. Check the file and try again."
              exit 1
            fi
            
            # Expose ingress controller
            kubectl -n ingress-nginx expose deployment ingress-nginx-controller \
              --name=ingress-nginx-nodeport \
              --port=80 \
              --target-port=80 \
              --type=NodePort \
              --overrides='\{"spec":\{"ports":[{"port":80,"protocol":"TCP","targetPort":80,"nodePort":30081}]\}\}' \
              --dry-run=client -o yaml | kubectl apply -f -
            
            # Save deployment info for backup server
            echo "{\"primary_host\":\"$REMOTE_HOST\",\"deploy_id\":\"$(date +%s)-$(git rev-parse --short HEAD)\",\"namespace\":\"default\",\"environment\":\"prod\"}" > /tmp/replication-info.json
            
            echo "Primary deployment completed successfully!"
          ' 
          
          # Download replication info from primary server
          scp "$REMOTE_HOST:/tmp/replication-info.json" ./replication-info.json

      # Upload replication info for the backup server
      - name: Upload replication info
        uses: actions/upload-artifact@v4
        with:
          name: replication-info
          path: replication-info.json
          retention-days: 1
          
  deploy-backup:
    needs: deploy-primary
    runs-on: ubuntu-latest
    environment: prod
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        
      # Download replication info from the primary server
      - name: Download replication info
        uses: actions/download-artifact@v4
        with:
          name: replication-info
          path: ./
          
      # Use ssh-agent for backup server
      - name: Set up SSH key for backup server
        uses: webfactory/ssh-agent@v0.7.0
        with:
          ssh-private-key: ${{ secrets.BACKUP_SSH_PRIVATE_KEY }}
      
      # Set up Cloudflare access for backup server
      - name: Configure Cloudflare Access for backup server
        env:
          BACKUP_HOST: ${{ secrets.BACKUP_HOST }}
          BACKUP_USER: ${{ secrets.BACKUP_USER || 'aj' }}
          CLOUDFLARE_CERT: ${{ secrets.CLOUDFLARE_CERT }}
        run: |
          # Install Cloudflared
          curl -L --output cloudflared.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
          sudo dpkg -i cloudflared.deb
          
          # Set up Cloudflare certificate
          mkdir -p ~/.cloudflared
          echo "$CLOUDFLARE_CERT" > ~/.cloudflared/cert.pem
          chmod 600 ~/.cloudflared/cert.pem
          
          # Create SSH config
          mkdir -p ~/.ssh
          echo "Host $BACKUP_HOST" > ~/.ssh/config
          echo "    ProxyCommand cloudflared access ssh --hostname %h" >> ~/.ssh/config
          echo "    User $BACKUP_USER" >> ~/.ssh/config
          echo "    StrictHostKeyChecking no" >> ~/.ssh/config
          echo "    UserKnownHostsFile /dev/null" >> ~/.ssh/config
          echo "    ServerAliveInterval 60" >> ~/.ssh/config
          chmod 600 ~/.ssh/config
          
          # Test SSH connection
          ssh -v "$BACKUP_HOST" "echo SSH connection successful" || {
            echo "SSH connection failed"
            exit 1
          }
          
      # Deploy to backup server
      - name: Deploy to backup server
        env:
          BACKUP_HOST: ${{ secrets.BACKUP_HOST }}
          BACKUP_USER: ${{ secrets.BACKUP_USER || 'aj' }}
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
          DB_ROOT_PASSWORD: ${{ secrets.DB_ROOT_PASSWORD }}
        run: |
          # Load replication info
          MYSQL_FILE=$(cat replication-info.json | jq -r '.mysql_file')
          MYSQL_POS=$(cat replication-info.json | jq -r '.mysql_pos')
          
          # Deploy to backup server
          ssh "$BACKUP_HOST" "
            cd /home/aj/dev/study
            
            # Update code
            git checkout origin/prod
            git pull origin prod
            
            # Docker login
            docker login docker.io -u \"$DOCKERHUB_USERNAME\" -p \"$DOCKERHUB_TOKEN\" || {
              echo \"Docker login failed\"
              exit 1
            }
            
            # Ensure Minikube is running
            if ! minikube status &>/dev/null; then
              echo \"Starting Minikube...\"
              minikube start --driver=docker || minikube start --driver=none
            fi
            
            # Configure kubectl for Minikube
            eval \$(minikube -p minikube docker-env)
            
            # Create replica database configuration
            mkdir -p prod/config
            
            # Create StatefulSet for replica
            cat > prod/config/db-replica-statefulset.yaml << YAML
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: db-replica
              namespace: default
            spec:
              serviceName: "db-replica"
              replicas: 1
              selector:
                matchLabels:
                  app: db
                  role: replica
              template:
                metadata:
                  labels:
                    app: db
                    role: replica
                spec:
                  containers:
                  - name: postgres
                    image: postgres:15
                    env:
                    - name: POSTGRES_PASSWORD
                      value: "$DB_ROOT_PASSWORD"
                    - name: POSTGRES_DB
                      value: "app_db"
                    ports:
                    - containerPort: 5432
                      name: postgres
                    volumeMounts:
                    - name: postgres-replica-data
                      mountPath: /var/lib/postgresql/data
                    - name: replication-config
                      mountPath: /etc/postgresql/conf.d
              volumeClaimTemplates:
              - metadata:
                  name: postgres-replica-data
                spec:
                  accessModes: [ "ReadWriteOnce" ]
                  resources:
                    requests:
                      storage: 1Gi
            YAML
            
            # Create ConfigMap and Service
            cat > prod/config/db-replica-configmap.yaml << YAML
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: postgres-replica-config
              namespace: default
            data:
              postgresql.conf: |
                wal_level = replica
                max_wal_senders = 10
                hot_standby = on
            YAML
            
            cat > prod/config/db-replica-service.yaml << YAML
            apiVersion: v1
            kind: Service
            metadata:
              name: db-replica
              namespace: default
            spec:
              selector:
                app: db
                role: replica
              ports:
              - port: 5432
                targetPort: 5432
            YAML
            
            # Apply replica database configuration
            kubectl apply -f prod/config/db-replica-statefulset.yaml
            kubectl apply -f prod/config/db-replica-configmap.yaml
            kubectl apply -f prod/config/db-replica-service.yaml
            kubectl rollout status statefulset/db-replica --timeout=600s
            
            # Set up PostgreSQL replication
            kubectl exec -it \$(kubectl get pods -l app=db,role=replica -o jsonpath="{.items[0].metadata.name}") -- \
              bash -c "echo 'host replication replicator 0.0.0.0/0 md5' >> /var/lib/postgresql/data/pg_hba.conf && \
                      pg_ctl reload"
            
            # Apply remaining configurations
            kubectl apply -f prod/config/cache.yaml
            kubectl rollout status deployment/cache --timeout=600s
            
            kubectl apply -f prod/config/backend.yaml
            kubectl rollout status deployment/backend --timeout=600s
            
            kubectl apply -f prod/config/worker.yaml
            kubectl rollout status deployment/worker --timeout=600s
            
            kubectl apply -f prod/config/frontend.yaml
            kubectl rollout status deployment/frontend --timeout=600s
            
            # Create backup ingress config
            sed \"s/atlantis.trading/backup.atlantis.trading/g\" prod/config/nginx.yaml > prod/config/backup-nginx.yaml
            kubectl apply -f prod/config/backup-nginx.yaml
            
            # Expose ingress controller
            kubectl -n ingress-nginx expose deployment ingress-nginx-controller \
              --name=ingress-nginx-nodeport-backup \
              --port=80 \
              --target-port=80 \
              --type=NodePort \
              --overrides='{\"spec\":{\"ports\":[{\"port\":80,\"protocol\":\"TCP\",\"targetPort\":80,\"nodePort\":30083}]}}'
            
            echo \"Backup server deployment completed successfully!\"
          "
