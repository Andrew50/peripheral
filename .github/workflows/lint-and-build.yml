name: Lint and Build Check
on:
  pull_request:
    branches: [main, prod]
  
  # Only trigger on direct pushes to ben/aj branches that are not associated with PRs
  push:
    branches: [ben, aj]
    paths-ignore:
      - '.github/workflows/**'  # Avoid triggering workflow changes twice
    # Add condition to exclude PR-related pushes
    tags-ignore:
      - '**'  # Ignore tag pushes
  
  # Add workflow_run trigger to run after branch-protection workflow
  workflow_run:
    workflows: ["Branch Protection Check"]
    types:
      - completed

# Add concurrency to cancel in-progress jobs when a new commit is pushed
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  # Add a check to determine if we should run the tests
  check-workflow-conditions:
    # Use a lightweight container instead of full ubuntu
    runs-on: ubuntu-latest
    # Use ubuntu runner directly instead of Alpine container for better Git support
    # Add permissions for GitHub token
    permissions:
      actions: read
      contents: read
      pull-requests: read  # Add permission to read PRs
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      continue_on_error: ${{ steps.check.outputs.continue_on_error }}
      last_success_sha: ${{ steps.get-last-success.outputs.sha }}
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      
      - id: check
        env:
          GH_TOKEN: ${{ github.token }}  # Add the GitHub token
        run: |
          # Check if this is a push on a branch that has an open PR
          if [[ "${{ github.event_name }}" == "push" ]]; then
            # Get the current branch name
            BRANCH_NAME="${{ github.ref_name }}"
            echo "Current branch: $BRANCH_NAME"
            
            # Check if there's an open PR from this branch to any target
            # Use GitHub API to check for PRs more reliably
            PR_COUNT=$(gh api graphql -f query='
              query($owner:String!, $repo:String!, $headRef:String!) {
                repository(owner:$owner, name:$repo) {
                  pullRequests(headRefName:$headRef, states:OPEN) {
                    totalCount
                  }
                }
              }' -f owner="${{ github.repository_owner }}" -f repo="${{ github.event.repository.name }}" -f headRef="$BRANCH_NAME" --jq '.data.repository.pullRequests.totalCount')
            
            if [[ $PR_COUNT -gt 0 ]]; then
              echo "This push is on a branch with an open PR - skipping duplicate run"
              echo "should_run=false" >> $GITHUB_OUTPUT
              echo "continue_on_error=false" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          
          # For pushes to ben or aj branches, run but don't fail on errors
          if [[ "${{ github.event_name }}" == "push" && ("${{ github.ref }}" == "refs/heads/ben" || "${{ github.ref }}" == "refs/heads/aj") ]]; then
            echo "Running for push to ben/aj branch - will not fail on errors"
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "continue_on_error=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For direct PRs to main, always run
          if [[ "${{ github.event_name }}" == "pull_request" && "${{ github.base_ref }}" == "main" ]]; then
            echo "Running for PR to main branch"
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "continue_on_error=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For PRs to prod or workflow runs targeting prod, skip the lint and build
          if [[ ("${{ github.event_name }}" == "pull_request" && "${{ github.base_ref }}" == "prod") || 
                ("${{ github.event_name }}" == "workflow_run" && "${{ github.event.workflow_run.head_branch }}" == "prod") ]]; then
            echo "Skipping lint and build for prod deployment"
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "continue_on_error=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # For workflow_run events, only run if the previous workflow succeeded
          if [[ "${{ github.event_name }}" == "workflow_run" ]]; then
            if [[ "${{ github.event.workflow_run.conclusion }}" == "success" ]]; then
              echo "Branch protection check passed, proceeding with tests"
              echo "should_run=true" >> $GITHUB_OUTPUT
              echo "continue_on_error=false" >> $GITHUB_OUTPUT
            else
              echo "Branch protection check did not pass, skipping tests"
              echo "should_run=false" >> $GITHUB_OUTPUT
              echo "continue_on_error=false" >> $GITHUB_OUTPUT
            fi
            exit 0
          fi
          
          # Default case
          echo "Running tests by default"
          echo "should_run=true" >> $GITHUB_OUTPUT
          echo "continue_on_error=false" >> $GITHUB_OUTPUT

      # Get the SHA of the last successful run
      - id: get-last-success
        if: steps.check.outputs.should_run == 'true'
        run: |
          # Initialize SHA output
          echo "sha=0000000000000000000000000000000000000000" >> $GITHUB_OUTPUT
          
          # For pull requests, we want to compare against the base branch
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "sha=${{ github.event.pull_request.base.sha }}" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Try to get the last successful run
          echo "Getting last successful workflow run SHA..."
          # Use the workflow filename instead of the display name
          WORKFLOW_FILENAME="lint-and-build.yml"
          BRANCH_NAME="${{ github.ref_name }}"
          
          # Get the run ID of the last successful run of this workflow on this branch
          LAST_RUN_ID=$(gh api \
            -H "Accept: application/vnd.github+json" \
            "/repos/${{ github.repository }}/actions/workflows/$WORKFLOW_FILENAME/runs?branch=$BRANCH_NAME&status=success&per_page=1" \
            --jq '.workflow_runs[0].id')
          
          if [ -n "$LAST_RUN_ID" ] && [ "$LAST_RUN_ID" != "null" ]; then
            echo "Found last successful run: $LAST_RUN_ID"
            
            # Get the SHA of the commit for that run
            LAST_SUCCESS_SHA=$(gh api \
              -H "Accept: application/vnd.github+json" \
              "/repos/${{ github.repository }}/actions/runs/$LAST_RUN_ID" \
              --jq '.head_sha')
            
            if [ -n "$LAST_SUCCESS_SHA" ] && [ "$LAST_SUCCESS_SHA" != "null" ]; then
              echo "Last successful commit SHA: $LAST_SUCCESS_SHA"
              echo "sha=$LAST_SUCCESS_SHA" >> $GITHUB_OUTPUT
            else
              echo "Using previous commit as fallback"
              echo "sha=$(git rev-parse HEAD~1)" >> $GITHUB_OUTPUT
            fi
          else
            echo "No previous successful run found, using previous commit as fallback"
            echo "sha=$(git rev-parse HEAD~1)" >> $GITHUB_OUTPUT
          fi
        env:
          GH_TOKEN: ${{ github.token }}

  lint-and-build-backend:
    needs: [check-workflow-conditions]
    if: ${{ always() && needs.check-workflow-conditions.outputs.should_run == 'true' }}
    # Use ubuntu runner directly instead of a container
    runs-on: ubuntu-latest
    # Set continue-on-error based on the branch condition
    continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}
    # Add permissions block for annotations
    permissions:
      contents: read
      checks: write
      pull-requests: read
    steps:
      - uses: actions/checkout@v3
        with:
          # For workflow_run events, we need to check out the PR that triggered the workflow
          ref: ${{ github.event.workflow_run.head_branch || github.ref }}
          # Fetch base branch for delta analysis
          fetch-depth: 0
      
      # Set up Go instead of using a container
      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.24'
          cache: true
          cache-dependency-path: services/backend/go.sum
          
      # Ensure Go cache directories exist
      - name: Setup Go cache directories
        run: |
          mkdir -p ~/.cache/go-build
          mkdir -p ~/go/pkg/mod
      
      - name: Identify changed Go files
        id: changed-files
        run: |
          # Use the last successful SHA from the check-workflow-conditions job
          BASE_SHA="${{ needs.check-workflow-conditions.outputs.last_success_sha }}"
          HEAD_SHA=$(git rev-parse HEAD)
          
          echo "Comparing changes between $BASE_SHA and $HEAD_SHA"
          
          # Check if backend directory has any changes
          HAS_BACKEND_CHANGES=$(git diff --name-only $BASE_SHA $HEAD_SHA -- 'services/backend/' | wc -l)
          
          if [ "$HAS_BACKEND_CHANGES" -eq 0 ]; then
            echo "No changes in backend directory - skipping backend checks"
            echo "has_changes=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Find modified Go files
          CHANGED_GO_FILES=$(git diff --name-only $BASE_SHA $HEAD_SHA -- 'services/backend/**/*.go' | xargs)
          echo "Changed Go files: $CHANGED_GO_FILES"
          
          # Identify affected packages (directories)
          if [ -n "$CHANGED_GO_FILES" ]; then
            # Get all potentially affected package directories
            ALL_AFFECTED_PACKAGES=$(echo "$CHANGED_GO_FILES" | xargs dirname | sort -u)
            echo "All potentially affected packages: $ALL_AFFECTED_PACKAGES"
            
            # Filter out directories that don't exist anymore
            EXISTING_PACKAGES=""
            cd services/backend
            for pkg in $ALL_AFFECTED_PACKAGES; do
              # Convert from full path to relative path
              pkg_rel=$(echo "$pkg" | sed 's/^services\/backend\///')
              
              # Check if the directory still exists
              if [ -d "$pkg_rel" ]; then
                echo "Directory exists: $pkg"
                EXISTING_PACKAGES="$EXISTING_PACKAGES $pkg"
              else
                echo "Directory no longer exists (skipping): $pkg"
              fi
            done
            cd - > /dev/null  # Return to previous directory
            
            # Store only existing packages as affected packages
            AFFECTED_PACKAGES=$(echo "$EXISTING_PACKAGES" | xargs)
            echo "Final affected packages (existing only): $AFFECTED_PACKAGES"
            
            # Store the paths for reference in other steps
            echo "affected_packages=$AFFECTED_PACKAGES" >> $GITHUB_OUTPUT
            echo "affected_files=$CHANGED_GO_FILES" >> $GITHUB_OUTPUT
            echo "has_changes=true" >> $GITHUB_OUTPUT
          else
            echo "No Go files changed"
            echo "has_changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Install dependencies
        if: steps.changed-files.outputs.has_changes == 'true'
        run: |
          cd services/backend
          go mod download
          go install honnef.co/go/tools/cmd/staticcheck@latest

      - name: Go Lint
        if: steps.changed-files.outputs.has_changes == 'true'
        uses: golangci/golangci-lint-action@v3
        with:
          version: latest
          working-directory: services/backend
          # Only include packages that exist
          args: --timeout=3m --config=.golangci.yml --out-format=colored-line-number --max-issues-per-linter=10 --max-same-issues=5 --build-tags=all --path-prefix=services/backend --allow-parallel-runners --go=1.24
          skip-cache: true
          skip-pkg-cache: true
          skip-build-cache: true
          only-new-issues: true
        env:
          # Ensure Go modules are used
          GO111MODULE: "on"
          # Add verbose output for debugging
          VERBOSE: "true"
        # Individual step can continue on error for ben/aj branches
        continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}

      - name: Go Staticcheck
        if: steps.changed-files.outputs.has_changes == 'true'
        run: |
          cd services/backend
          # Find existing directories to check
          echo "Finding existing directories to check..."
          
          # Only check affected packages
          AFFECTED_PACKAGES="${{ steps.changed-files.outputs.affected_packages }}"
          
          if [ -n "$AFFECTED_PACKAGES" ]; then
            # Convert to relative paths for staticcheck
            REL_PACKAGES=""
            for pkg in $AFFECTED_PACKAGES; do
              # Convert absolute path to relative path
              pkg_rel=$(echo "$pkg" | sed 's/^services\/backend\//\.\//')
              REL_PACKAGES="$REL_PACKAGES $pkg_rel"
            done
            
            echo "Running staticcheck on affected packages: $REL_PACKAGES"
            # First run to print all issues
            staticcheck -tags=all -checks=all,-ST1000,-ST1003,-ST1016 $REL_PACKAGES || true
            # Second run to actually fail the build only on critical issues
            staticcheck -tags=all -checks=all,-ST1000,-ST1003,-ST1016,-SA4006,-SA5000,-SA6000 $REL_PACKAGES
          else
            echo "No affected packages found to check"
          fi
        # Individual step can continue on error for ben/aj branches
        continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}

      - name: Go Build
        if: steps.changed-files.outputs.has_changes == 'true'
        run: |
          # Ensure we're in the right directory with the go.mod file
          cd services/backend
          # Set environment variable to force Go module mode
          export GO111MODULE=on
          
          # Debug: Show current directory and repository structure
          echo "Current directory: $(pwd)"
          
          # Only build affected packages
          AFFECTED_PACKAGES="${{ steps.changed-files.outputs.affected_packages }}"
          
          if [ -n "$AFFECTED_PACKAGES" ]; then
            echo "Building affected packages..."
            
            # First validate all packages can be resolved
            echo "Validating packages can be resolved..."
            VALID_PACKAGES=""
            for pkg in $AFFECTED_PACKAGES; do
              # Convert absolute path to relative module path
              pkg_rel=$(echo "$pkg" | sed 's/^services\/backend\///')
              
              # Check if the directory exists and can be resolved as a Go package
              if [ -d "$pkg_rel" ] && go list "./$pkg_rel" &> /dev/null; then
                echo "Package is valid: $pkg_rel"
                VALID_PACKAGES="$VALID_PACKAGES $pkg_rel"
              else
                echo "WARNING: Package $pkg_rel cannot be resolved - skipping build"
              fi
            done
            
            # Build each valid package individually to isolate failures
            if [ -n "$VALID_PACKAGES" ]; then
              # Use multiple workers for faster parallel builds but process each package separately
              echo "Building packages with multiple workers..."
              
              # Fix: process each package separately rather than treating them as one argument
              for pkg in $VALID_PACKAGES; do
                echo "Building package: $pkg"
                go build -v -tags=all "./$pkg" || {
                  echo "ERROR: Package $pkg failed to build"
                  exit 1
                }
              done
            else
              echo "No valid packages to build"
            fi
          else
            echo "No packages to build"
          fi
        # Individual step can continue on error for ben/aj branches
        continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}

      # Add summary step to create job summary and record last successful state
      - name: Create job summary
        if: success() || needs.check-workflow-conditions.outputs.continue_on_error == 'true'
        run: |
          if [ "${{ steps.changed-files.outputs.has_changes }}" == "true" ]; then
            echo "### Backend CI Results ✅" >> $GITHUB_STEP_SUMMARY
            echo "✅ Successfully linted and built the following packages:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "${{ steps.changed-files.outputs.affected_packages }}" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "### Backend CI Results ➡️" >> $GITHUB_STEP_SUMMARY
            echo "➡️ No changes detected in backend directory since last successful run. Skipped checks." >> $GITHUB_STEP_SUMMARY
          fi

  lint-and-build-worker:
    needs: [check-workflow-conditions]
    if: ${{ always() && needs.check-workflow-conditions.outputs.should_run == 'true' }}
    # Use a specialized Python container
    runs-on: ubuntu-latest
    # Remove container section and run directly on the runner
    # Set continue-on-error based on the branch condition
    continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}
    steps:
      - uses: actions/checkout@v3
        with:
          # For workflow_run events, we need to check out the PR that triggered the workflow
          ref: ${{ github.event.workflow_run.head_branch || github.ref }}
          # Fetch base branch for delta analysis
          fetch-depth: 0

      # Set up Python instead of using a container
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: services/worker/requirements.txt

      # Ensure pip cache directory exists for caching
      - name: Setup pip cache
        run: |
          mkdir -p /home/runner/.cache/pip
          
      - name: Check for worker changes
        id: check-worker-changes
        run: |
          # Use the last successful SHA from the check-workflow-conditions job
          BASE_SHA="${{ needs.check-workflow-conditions.outputs.last_success_sha }}"
          HEAD_SHA=$(git rev-parse HEAD)
          
          echo "Comparing changes between $BASE_SHA and $HEAD_SHA"
          
          # Check if worker directory has any changes
          HAS_WORKER_CHANGES=$(git diff --name-only $BASE_SHA $HEAD_SHA -- 'services/worker/' | wc -l)
          
          if [ "$HAS_WORKER_CHANGES" -eq 0 ]; then
            echo "No changes in worker directory - skipping worker checks"
            echo "has_changes=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "has_changes=true" >> $GITHUB_OUTPUT

      - name: Install dependencies
        if: steps.check-worker-changes.outputs.has_changes == 'true'
        run: |
          cd services/worker
          # Create and activate a virtual environment
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          pip install flake8 black pytest
          pip install -r requirements.txt
          # Add venv to PATH for subsequent steps
          echo "$PWD/.venv/bin" >> $GITHUB_PATH

      - name: Identify changed Python files
        id: changed-py-files
        if: steps.check-worker-changes.outputs.has_changes == 'true'
        run: |
          # Use the last successful SHA from the check-workflow-conditions job
          BASE_SHA="${{ needs.check-workflow-conditions.outputs.last_success_sha }}"
          HEAD_SHA=$(git rev-parse HEAD)
          
          echo "Comparing changes between $BASE_SHA and $HEAD_SHA"
          
          # Find modified Python files
          CHANGED_PY_FILES=$(git diff --name-only $BASE_SHA $HEAD_SHA -- 'services/worker/**/*.py' | xargs)
          echo "Changed Python files: $CHANGED_PY_FILES"
          
          if [ -n "$CHANGED_PY_FILES" ]; then
            echo "changed_files=$CHANGED_PY_FILES" >> $GITHUB_OUTPUT
            echo "has_py_changes=true" >> $GITHUB_OUTPUT
          else
            echo "No Python files changed, but other worker files may have changed"
            echo "has_py_changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Lint with flake8
        if: steps.changed-py-files.outputs.has_py_changes == 'true'
        run: |
          cd services/worker
          # Ensure virtual environment is used
          source .venv/bin/activate
          
          # Only lint changed files
          CHANGED_FILES="${{ steps.changed-py-files.outputs.changed_files }}"
          if [ -n "$CHANGED_FILES" ]; then
            echo "Running flake8 on changed files: $CHANGED_FILES"
            # Use a more focused set of checks for speed
            flake8 $CHANGED_FILES \
              --count \
              --select=E9,F63,F7,F82 \
              --show-source \
              --statistics
          else
            echo "No Python files to lint"
          fi
        continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}

      - name: Format check with Black
        if: steps.changed-py-files.outputs.has_py_changes == 'true'
        run: |
          cd services/worker
          # Ensure virtual environment is used
          source .venv/bin/activate
          
          # Only check changed files
          CHANGED_FILES="${{ steps.changed-py-files.outputs.changed_files }}"
          if [ -n "$CHANGED_FILES" ]; then
            echo "Running black on changed files: $CHANGED_FILES"
            black --check --diff --verbose $CHANGED_FILES || echo "Black formatting issues found but continuing"
          else
            echo "No Python files to format check"
          fi
        # Always continue even if Black finds issues
        continue-on-error: true

      - name: Basic import check
        if: steps.check-worker-changes.outputs.has_changes == 'true'
        run: |
          cd services/worker
          # Ensure virtual environment is used
          source .venv/bin/activate
          
          # For changed files only, or a basic check if no changes
          if [ "${{ steps.changed-py-files.outputs.has_py_changes }}" == "true" ]; then
            echo "Checking imports for changed files..."
            CHANGED_FILES="${{ steps.changed-py-files.outputs.changed_files }}"
            # Process files in parallel (limit to 4 at a time)
            echo "$CHANGED_FILES" | xargs -n1 -P4 python -m py_compile
          else
            echo "Only non-Python files changed, skipping import check"
          fi
        # Individual step can continue on error for ben/aj branches
        continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}

      # Add summary step to create job summary and record last successful state
      - name: Create job summary
        if: success() || needs.check-workflow-conditions.outputs.continue_on_error == 'true'
        run: |
          if [ "${{ steps.check-worker-changes.outputs.has_changes }}" == "true" ]; then
            echo "### Worker CI Results ✅" >> $GITHUB_STEP_SUMMARY
            if [ "${{ steps.changed-py-files.outputs.has_py_changes }}" == "true" ]; then
              echo "✅ Successfully checked the following Python files:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "${{ steps.changed-py-files.outputs.changed_files }}" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
            else
              echo "✅ Non-Python worker files were changed and verified." >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### Worker CI Results ➡️" >> $GITHUB_STEP_SUMMARY
            echo "➡️ No changes detected in worker directory since last successful run. Skipped checks." >> $GITHUB_STEP_SUMMARY
          fi

  lint-and-build-frontend:
    needs: [check-workflow-conditions]
    if: ${{ always() && needs.check-workflow-conditions.outputs.should_run == 'true' }}
    # Use ubuntu runner directly instead of a container
    runs-on: ubuntu-latest
    # Set continue-on-error based on the branch condition
    continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}
    outputs:
      has_changes: ${{ steps.check-frontend-changes.outputs.has_changes }}
    steps:
      - uses: actions/checkout@v3
        with:
          # For workflow_run events, we need to check out the PR that triggered the workflow
          ref: ${{ github.event.workflow_run.head_branch || github.ref }}
          # Fetch base branch for delta analysis
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: services/frontend/package.json
          
      # Ensure npm cache directory exists
      - name: Setup npm cache directory
        run: |
          mkdir -p ~/.npm
      
      - name: Check for frontend changes
        id: check-frontend-changes
        run: |
          # Use the last successful SHA from the check-workflow-conditions job
          BASE_SHA="${{ needs.check-workflow-conditions.outputs.last_success_sha }}"
          HEAD_SHA=$(git rev-parse HEAD)
          
          echo "Comparing changes between $BASE_SHA and $HEAD_SHA"
          
          # Check if frontend directory has any changes
          HAS_FRONTEND_CHANGES=$(git diff --name-only $BASE_SHA $HEAD_SHA -- 'services/frontend/' | wc -l)
          
          if [ "$HAS_FRONTEND_CHANGES" -eq 0 ]; then
            echo "No changes in frontend directory - skipping frontend checks"
            echo "has_changes=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "has_changes=true" >> $GITHUB_OUTPUT

      - name: Install dependencies
        run: |
          cd services/frontend
          
          # Use npm install instead of npm ci to allow for dependency updates
          npm install
          
          # Install additional test dependencies
          npm install --save-dev jest@29 @types/jest ts-jest@29 svelte-jester@2 @testing-library/svelte @testing-library/jest-dom jsdom
          
          # Create Jest config if it doesn't exist
          if [ ! -f jest.config.js ]; then
            echo "Creating Jest configuration file..."
            echo 'export default {' > jest.config.js
            echo '  transform: {' >> jest.config.js
            echo '    "^.+\\.svelte$": "svelte-jester",' >> jest.config.js
            echo '    "^.+\\.ts$": "ts-jest",' >> jest.config.js
            echo '    "^.+\\.js$": "ts-jest"' >> jest.config.js
            echo '  },' >> jest.config.js
            echo '  moduleFileExtensions: ["js", "ts", "svelte"],' >> jest.config.js
            echo '  testEnvironment: "jsdom",' >> jest.config.js
            echo '  setupFilesAfterEnv: ["@testing-library/jest-dom/extend-expect"],' >> jest.config.js
            echo '  testMatch: ["**/*.test.ts", "**/*.test.js"],' >> jest.config.js
            echo '  moduleNameMapper: {' >> jest.config.js
            echo '    "^\\$lib/(.*)$": "<rootDir>/src/lib/$1"' >> jest.config.js
            echo '  },' >> jest.config.js
            echo '  extensionsToTreatAsEsm: [".ts", ".svelte"],' >> jest.config.js
            echo '  globals: {' >> jest.config.js
            echo '    "ts-jest": {' >> jest.config.js
            echo '      useESM: true' >> jest.config.js
            echo '    }' >> jest.config.js
            echo '  }' >> jest.config.js
            echo '};' >> jest.config.js
          fi

      - name: Create simplified Jest configuration
        run: |
          cd services/frontend
          
          # First, create a CommonJS version of the setup file for compatibility
          if [ -f "jest.setup.js" ]; then
            echo "Creating CommonJS version of jest.setup.js..."
            # Create a backup of the original file
            cp jest.setup.js jest.setup.js.original
            
            # Create a CommonJS version that works with both ESM and CJS
            echo "// CommonJS version of the Jest setup file (auto-converted)" > jest.setup.cjs
            echo "// Original file preserved as jest.setup.js" >> jest.setup.cjs
            echo "require('@testing-library/jest-dom');" >> jest.setup.cjs
          else
            # Create a simple setup file if none exists
            echo "// Jest setup file - CommonJS format" > jest.setup.cjs
            echo "require('@testing-library/jest-dom');" >> jest.setup.cjs
          fi
          
          # Create a dual-mode Jest config that works with both CJS and ESM
          echo "/** @type {import('jest').Config} */" > jest.config.cjs
          echo "module.exports = {" >> jest.config.cjs
          echo "  preset: 'ts-jest'," >> jest.config.cjs
          echo "  testEnvironment: 'jsdom'," >> jest.config.cjs
          echo "  setupFilesAfterEnv: ['<rootDir>/jest.setup.cjs']," >> jest.config.cjs
          echo "  transform: {" >> jest.config.cjs
          echo "    '^.+\\.svelte$': 'svelte-jester'," >> jest.config.cjs
          echo "    '^.+\\.(ts|js)$': 'ts-jest'" >> jest.config.cjs
          echo "  }," >> jest.config.cjs
          echo "  moduleNameMapper: {" >> jest.config.cjs
          echo "    '^\\$lib/(.*)$': '<rootDir>/src/lib/$1'" >> jest.config.cjs
          echo "  }," >> jest.config.cjs
          echo "  moduleFileExtensions: ['js', 'ts', 'svelte']" >> jest.config.cjs
          echo "};" >> jest.config.cjs
          
          # Create minimal SvelteKit type structure if it doesn't exist
          mkdir -p .svelte-kit/types
          
          # Create minimal app.d.ts if it doesn't exist
          if [ ! -f "src/app.d.ts" ]; then
            mkdir -p src
            echo "// Minimal type definitions for testing" > src/app.d.ts
            echo "declare global { namespace App {} }" >> src/app.d.ts
            echo "export {};" >> src/app.d.ts
          fi

      - name: Run tests
        run: |
          cd services/frontend
          
          # Ensure jest-environment-jsdom is installed
          if ! grep -q "jest-environment-jsdom" package.json; then
            npm install --save-dev jest-environment-jsdom
          fi
          
          # Run tests with the simplified CommonJS configuration
          echo "Running tests with simplified CommonJS configuration"
          npx jest --config=jest.config.cjs
        
        # Individual step can continue on error for ben/aj branches
        continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}
        
      # Add summary step to create job summary
      - name: Create job summary
        if: success() || needs.check-workflow-conditions.outputs.continue_on_error == 'true'
        run: |
          if [ "${{ steps.check-frontend-changes.outputs.has_changes }}" == "true" ]; then
            echo "### Frontend CI Results ✅" >> $GITHUB_STEP_SUMMARY
            if [ "${{ steps.changed-frontend-files.outputs.has_ts_changes }}" == "true" ]; then
              echo "✅ Successfully linted and built the following files:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "${{ steps.changed-frontend-files.outputs.changed_files }}" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
            else
              echo "✅ Non-TS/JS/Svelte frontend files were changed and verified." >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "### Frontend CI Results ➡️" >> $GITHUB_STEP_SUMMARY
            echo "➡️ No changes detected in frontend directory since last successful run. Skipped checks." >> $GITHUB_STEP_SUMMARY
          fi

  # Add new job to run frontend tests
  test-frontend:
    needs: [check-workflow-conditions, lint-and-build-frontend]
    if: ${{ always() && needs.check-workflow-conditions.outputs.should_run == 'true' && needs.lint-and-build-frontend.outputs.has_changes == 'true' }}
    # Use ubuntu runner directly instead of a container
    runs-on: ubuntu-latest
    # Set continue-on-error based on the branch condition
    continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}
    steps:
      - uses: actions/checkout@v3
        with:
          # For workflow_run events, we need to check out the PR that triggered the workflow
          ref: ${{ github.event.workflow_run.head_branch || github.ref }}
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: services/frontend/package.json

      - name: Install dependencies
        run: |
          cd services/frontend
          
          # Use npm install instead of npm ci to allow for dependency updates
          npm install
          
          # Install additional test dependencies
          npm install --save-dev jest@29 @types/jest ts-jest@29 svelte-jester@2 @testing-library/svelte @testing-library/jest-dom jsdom jest-environment-jsdom

      - name: Create simplified Jest configuration
        run: |
          cd services/frontend
          
          # First, create a CommonJS version of the setup file for compatibility
          if [ -f "jest.setup.js" ]; then
            echo "Creating CommonJS version of jest.setup.js..."
            # Create a backup of the original file
            cp jest.setup.js jest.setup.js.original
            
            # Create a CommonJS version that works with both ESM and CJS
            echo "// CommonJS version of the Jest setup file (auto-converted)" > jest.setup.cjs
            echo "// Original file preserved as jest.setup.js" >> jest.setup.cjs
            echo "require('@testing-library/jest-dom');" >> jest.setup.cjs
          else
            # Create a simple setup file if none exists
            echo "// Jest setup file - CommonJS format" > jest.setup.cjs
            echo "require('@testing-library/jest-dom');" >> jest.setup.cjs
          fi
          
          # Create a simplified Jest config file (using CJS format for maximum compatibility)
          echo "/** @type {import('jest').Config} */" > jest.config.cjs
          echo "module.exports = {" >> jest.config.cjs
          echo "  preset: 'ts-jest'," >> jest.config.cjs
          echo "  testEnvironment: 'jsdom'," >> jest.config.cjs
          echo "  setupFilesAfterEnv: ['<rootDir>/jest.setup.cjs']," >> jest.config.cjs
          echo "  transform: {" >> jest.config.cjs
          echo "    '^.+\\.svelte$': 'svelte-jester'," >> jest.config.cjs
          echo "    '^.+\\.(ts|js)$': 'ts-jest'" >> jest.config.cjs
          echo "  }," >> jest.config.cjs
          echo "  moduleNameMapper: {" >> jest.config.cjs
          echo "    '^\\$lib/(.*)$': '<rootDir>/src/lib/$1'" >> jest.config.cjs
          echo "  }," >> jest.config.cjs
          echo "  moduleFileExtensions: ['js', 'ts', 'svelte']" >> jest.config.cjs
          echo "};" >> jest.config.cjs
          
          # Create minimal SvelteKit type structure if it doesn't exist
          mkdir -p .svelte-kit/types
          
          # Create minimal app.d.ts if it doesn't exist
          if [ ! -f "src/app.d.ts" ]; then
            mkdir -p src
            echo "// Minimal type definitions for testing" > src/app.d.ts
            echo "declare global { namespace App {} }" >> src/app.d.ts
            echo "export {};" >> src/app.d.ts
          fi

      - name: Run tests
        run: |
          cd services/frontend
          
          # Run tests with the simplified CommonJS configuration
          echo "Running tests with simplified CommonJS configuration"
          npx jest --config=jest.config.cjs
        
        # Individual step can continue on error for ben/aj branches
        continue-on-error: ${{ needs.check-workflow-conditions.outputs.continue_on_error == 'true' }}
        
      # Add summary step to create job summary
      - name: Create job summary
        if: success() || needs.check-workflow-conditions.outputs.continue_on_error == 'true'
        run: |
          echo "### Frontend Test Results ✅" >> $GITHUB_STEP_SUMMARY
          echo "✅ Ran frontend tests with simplified configuration" >> $GITHUB_STEP_SUMMARY 
