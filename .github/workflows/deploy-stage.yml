name: Deploy to Kubernetes (Staging)

on:
  # Deploy when pushing to stable branches
  push:
    branches:
      - main
  
  # Different trigger for pull requests with explicit configuration
  pull_request:
    types: [opened, synchronize, reopened]
    branches:
      - main
  
  # Manual triggering
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to deploy'
        required: true
        default: 'main'

# Add permissions configuration here
permissions:
  contents: read
  packages: write
  id-token: write

# Add concurrency control to queue deployments instead of running in parallel
concurrency:
  group: kubernetes-deployment
  cancel-in-progress: false  # Queue deployments instead of canceling

jobs:
  # Add a dependency on the lint-and-build workflow
  check-build:
    runs-on: ubuntu-latest  # Using GitHub-hosted runner
    if: github.event_name == 'pull_request'
    steps:
      - name: Check if lint-and-build workflow passed
        run: echo "This job ensures the lint-and-build workflow has passed before deployment"

  deploy-stage:
    runs-on: self-hosted  # Keep deployment on self-hosted runner
    needs: [check-build]
    if: >-
      (github.event_name != 'pull_request' || success())
    # Add job-level permissions to ensure access to secrets
    
    # Add environment variables for the job
    env:
      DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
      DOCKER_TOKEN: ${{ secrets.DOCKER_TOKEN }}
      DB_ROOT_PASSWORD: ${{ secrets.STAGE_DB_ROOT_PASSWORD }}
      REDIS_PASSWORD: ${{ secrets.STAGE_REDIS_PASSWORD }}
      POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      GEMINI_FREE_KEYS: ${{ secrets.GEMINI_FREE_KEYS }}
      GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID }}
      GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET }}
      JWT_SECRET: ${{ secrets.STAGE_JWT_SECRET }}
      CLOUDFLARE_TUNNEL_TOKEN: ${{ secrets.STAGE_CLOUDFLARE_TUNNEL_TOKEN }}
      GOOGLE_REDIRECT_URL: "https://stage.atlantis.trading/auth/google/callback"
      ENVIRONMENT: "stage"
      K8S_CONTEXT: "stage-cluster"
      K8S_NAMESPACE: "stage"
    
    steps:
      # Print debugging information before checkout
      - name: Debug Event Information
        run: |
          echo "Event name: ${{ github.event_name }}"
          echo "GitHub ref: ${{ github.ref }}"
          echo "Target branch: ${{ github.base_ref || github.ref_name }}"

      # Determine which branch to checkout based on event type
      - name: Set checkout target
        id: set-target
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "CHECKOUT_REF=${{ github.head_ref }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.base_ref }}" >> $GITHUB_ENV
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ github.event.inputs.branch }}" ]; then
            echo "CHECKOUT_REF=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
          else
            echo "CHECKOUT_REF=${{ github.ref_name }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.ref_name }}" >> $GITHUB_ENV
          fi

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: ${{ env.CHECKOUT_REF }}
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 1
          lfs: false
      
      # Add diagnostic step to see what's available after checkout
      - name: Show Git Information
        run: |
          echo "Current directory: $(pwd)"
          echo "Directory contents: $(ls -la)"
          echo "Git status: $(git status)"
          echo "Current branch: $(git branch --show-current)"
          git --version
      
      # Verify secrets are available without exposing them
      - name: Verify Docker Credentials
        run: |
          echo "Docker username secret: $(if [ -n "${{ secrets.DOCKER_USERNAME }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Docker token secret: $(if [ -n "${{ secrets.DOCKER_TOKEN }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "DB password secret: $(if [ -n "${{ secrets.STAGE_DB_ROOT_PASSWORD }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Polygon API key: $(if [ -n "${{ secrets.POLYGON_API_KEY }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Gemini keys: $(if [ -n "${{ secrets.GEMINI_FREE_KEYS }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Google OAuth credentials: $(if [ -n "${{ secrets.GOOGLE_CLIENT_ID }}" ] && [ -n "${{ secrets.GOOGLE_CLIENT_SECRET }}" ]; then echo "are set"; else echo "are NOT set"; fi)"
          echo "Cloudflare tunnel token: $(if [ -n "${{ secrets.STAGE_CLOUDFLARE_TUNNEL_TOKEN }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
      
      # Setup and configuration
      - name: Setup deployment
        run: |
          # Sanitize branch name for Docker tags (replace / with - and other invalid characters)
          DOCKER_TAG=$(echo "${{ env.TARGET_BRANCH }}" | sed 's/\//-/g' | sed 's/[^a-zA-Z0-9_.-]/-/g')
          echo "DOCKER_TAG=${DOCKER_TAG}" >> $GITHUB_ENV
          echo "Using Docker tag: ${DOCKER_TAG}"

      # Build Docker images
      - name: Build Docker Images
        run: |
          # Helper function to check required secrets
          check_secret() {
            if [ -z "${!1}" ]; then
              echo "ERROR: $1 secret is not available"
              exit 1
            fi
          }
          
          # Check required secrets
          check_secret "DOCKER_USERNAME"
          
          # Define services to build
          services=("frontend" "backend" "worker" "worker-healthcheck" "tf" "nginx" "db")
          
          # Build all images
          echo "Building Docker images with tag: ${{ env.DOCKER_TAG }}..."
          for service in "${services[@]}"; do
            dockerfile="services/${service}/Dockerfile.prod"
            # Special case for nginx and worker-healthcheck
            if [ "$service" = "nginx" ]; then
              dockerfile="services/nginx/Dockerfile"
            elif [ "$service" = "worker-healthcheck" ]; then
              dockerfile="services/worker/Dockerfile.healthcheck"
              docker build -t $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} -f $dockerfile services/worker
              continue
            fi
            
            docker build -t $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} -f $dockerfile services/$service
          done
          
          # Tag images as 'stage'
          echo "Tagging images as 'stage'..."
          for service in "${services[@]}"; do
            docker tag $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/$service:stage
          done

      # Push Docker images
      - name: Login to Docker Hub and Push Images
        run: |
          # Check required secrets
          if [ -z "$DOCKER_TOKEN" ]; then
            echo "ERROR: DOCKER_TOKEN secret is not available"
            exit 1
          fi
          
          # Login to Docker Hub
          echo "$DOCKER_TOKEN" | docker login -u "$DOCKER_USERNAME" --password-stdin || {
            echo "ERROR: Docker login failed. Please check your credentials."
            exit 1
          }
          
          # Define services to push
          services=("frontend" "backend" "worker" "worker-healthcheck" "tf" "db" "nginx")
          
          # Push branch-specific tags
          echo "Pushing images with tag: ${{ env.DOCKER_TAG }}..."
          for service in "${services[@]}"; do
            docker push $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }}
          done
          
          # Push stage tag
          echo "Pushing 'stage' tagged images..."
          for service in "${services[@]}"; do
            docker push $DOCKER_USERNAME/$service:stage
          done

      # Setup Kubernetes context
      - name: Setup Kubernetes Context
        run: |
          # Helper function for retrying commands
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  echo "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            echo "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Switch to staging Kubernetes context
          echo "Switching to staging Kubernetes context ($K8S_CONTEXT)..."
          kubectl_with_retry "kubectl config use-context $K8S_CONTEXT" || exit 1
          
          # Verify correct context is active
          CURRENT_CONTEXT=$(kubectl config current-context)
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            echo "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          echo "Confirmed correct Kubernetes context: $CURRENT_CONTEXT"
          
          # Set Kubernetes namespace for all commands
          kubectl_with_retry "kubectl config set-context --current --namespace=$K8S_NAMESPACE" || exit 1
          echo "Set Kubernetes namespace to: $K8S_NAMESPACE"
          
          # Verify cluster connectivity
          kubectl_with_retry "kubectl cluster-info" || exit 1

      # Update Kubernetes secrets
      - name: Update Kubernetes Secrets
        run: |
          # Helper function to check required secrets
          check_secret() {
            if [ -z "${!1}" ]; then
              echo "ERROR: $1 secret is not available"
              exit 1
            fi
          }
          
          # Check required secrets
          required_secrets=("DB_ROOT_PASSWORD" "REDIS_PASSWORD" "POLYGON_API_KEY" "GEMINI_FREE_KEYS" 
                          "GOOGLE_CLIENT_ID" "GOOGLE_CLIENT_SECRET" "JWT_SECRET" "CLOUDFLARE_TUNNEL_TOKEN")
          for secret in "${required_secrets[@]}"; do
            check_secret "$secret"
          done
          
          # Retry function for kubernetes operations
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  echo "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            echo "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Create temporary file for secrets
          TEMP_DIR=$(mktemp -d)
          SECRETS_FILE="$TEMP_DIR/secrets.yaml"
          
          # Encode secrets
          DB_PASSWORD_B64=$(echo -n "${{ secrets.STAGE_DB_ROOT_PASSWORD }}" | base64 -w 0)
          REDIS_PASSWORD_B64=$(echo -n "${{ secrets.STAGE_REDIS_PASSWORD }}" | base64 -w 0)
          POLYGON_API_KEY_B64=$(echo -n "${{ secrets.POLYGON_API_KEY }}" | base64 -w 0)
          GEMINI_FREE_KEYS_B64=$(echo -n "${{ secrets.GEMINI_FREE_KEYS }}" | base64 -w 0)
          GOOGLE_CLIENT_ID_B64=$(echo -n "${{ secrets.GOOGLE_CLIENT_ID }}" | base64 -w 0)
          GOOGLE_CLIENT_SECRET_B64=$(echo -n "${{ secrets.GOOGLE_CLIENT_SECRET }}" | base64 -w 0)
          JWT_SECRET_B64=$(echo -n "${{ secrets.STAGE_JWT_SECRET }}" | base64 -w 0)
          CLOUDFLARE_TUNNEL_TOKEN_B64=$(echo -n "${{ secrets.STAGE_CLOUDFLARE_TUNNEL_TOKEN }}" | base64 -w 0)
          
          # Create secrets YAML
          cat > "$SECRETS_FILE" << EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: redis-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            REDIS_PASSWORD: ${REDIS_PASSWORD_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: db-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            DB_ROOT_PASSWORD: ${DB_PASSWORD_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: polygon-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            api-key: ${POLYGON_API_KEY_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: gemini-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            GEMINI_FREE_KEYS: ${GEMINI_FREE_KEYS_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: google-oauth-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID_B64}
            GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: jwt-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            JWT_SECRET: ${JWT_SECRET_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: cloudflare-tunnel-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            CLOUDFLARE_TUNNEL_TOKEN: ${CLOUDFLARE_TUNNEL_TOKEN_B64}
          EOF
          
          # Apply secrets to Kubernetes
          echo "Applying secrets to Kubernetes..."
          kubectl_with_retry "kubectl apply -f \"$SECRETS_FILE\"" || kubectl_with_retry "kubectl apply -f \"$SECRETS_FILE\" --validate=false"
          
          # Clean up
          rm -rf "$TEMP_DIR"
          
          # Check if deployments exist and restart them if they do
          echo "Checking and restarting deployments if they exist..."
          if kubectl get deployment backend &>/dev/null; then
            kubectl rollout restart deployment/backend
          else
            echo "Note: backend deployment not found - skipping restart"
          fi
          
          if kubectl get deployment worker &>/dev/null; then
            kubectl rollout restart deployment/worker
          else
            echo "Note: worker deployment not found - skipping restart"
          fi

      # Run database migrations
      - name: Deploy Database Migrations
        run: |
          # Verify cluster connectivity with detailed output
          echo "Checking Kubernetes cluster connectivity..."
          if ! kubectl cluster-info; then
            echo "ERROR: Cannot connect to Kubernetes cluster."
            echo "Current kubectl context: $(kubectl config current-context || echo 'NONE')"
            exit 1
          fi
          
          # Verify correct context
          CURRENT_CONTEXT=$(kubectl config current-context)
          echo "Current Kubernetes context: $CURRENT_CONTEXT"
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            echo "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          # Check if namespace exists
          echo "Checking if namespace $K8S_NAMESPACE exists..."
          if ! kubectl get namespace "$K8S_NAMESPACE" &>/dev/null; then
            echo "ERROR: Namespace $K8S_NAMESPACE does not exist."
            echo "Available namespaces:"
            kubectl get namespaces
            exit 1
          fi
          
          # Get all pods to help diagnose issues
          echo "Listing all pods in namespace $K8S_NAMESPACE:"
          kubectl get pods -n "$K8S_NAMESPACE"
          
          # Save current replica counts with better error handling
          echo "Saving current deployment replica counts..."
          for deployment in worker backend tf; do
            echo "Checking $deployment deployment..."
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              replicas=$(kubectl get deployment $deployment -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "1")
              echo "${deployment}_REPLICAS=$replicas" >> $GITHUB_ENV
              echo "$deployment has $replicas replicas"
            else
              echo "Note: $deployment deployment not found, will use default of 1 replica"
              echo "${deployment}_REPLICAS=1" >> $GITHUB_ENV
            fi
          done
          
          # Check if deployments exist before scaling
          echo "Checking deployments before scaling down..."
          DEPLOYMENTS_TO_SCALE=""
          for deployment in worker backend tf; do
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              DEPLOYMENTS_TO_SCALE="$DEPLOYMENTS_TO_SCALE $deployment"
            else
              echo "Note: $deployment deployment not found - skipping scale down"
            fi
          done
          
          # Scale down services if they exist
          if [ -n "$DEPLOYMENTS_TO_SCALE" ]; then
            echo "Entering maintenance mode: Scaling down services that access the database..."
            kubectl scale deployment $DEPLOYMENTS_TO_SCALE --replicas=0 || {
              echo "WARNING: Failed to scale down some deployments, proceeding anyway..."
            }
            
            # Wait for pods to terminate with status check
            echo "Waiting for pods to terminate..."
            sleep 5
            echo "Checking if pods are terminated:"
            kubectl get pods -l "app in (worker,backend,tf)" -n "$K8S_NAMESPACE" || echo "No matching pods found"
          else
            echo "No deployments found to scale down, skipping this step"
          fi
          
          # Check if database pod exists
          echo "Looking for database pod..."
          # Add set +e to prevent command from causing the script to exit on error
          set +e
          DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
          POD_CHECK_STATUS=$?
          set -e
          
          echo "Database pod lookup complete, status: $POD_CHECK_STATUS"
          echo "DB_POD value: ${DB_POD:-'(empty)'}"
          
          if [ -z "$DB_POD" ]; then
            echo "Database pod not found - this could be the first deployment."
            echo "Checking if DB deployment exists..."
            
            # Add set +e to prevent command from causing the script to exit on error
            set +e
            DB_DEPLOYMENT=$(kubectl get deployment db -n "$K8S_NAMESPACE" 2>/dev/null)
            DEPLOYMENT_CHECK_STATUS=$?
            set -e
            
            echo "DB deployment check complete, status: $DEPLOYMENT_CHECK_STATUS"
            
            if [ $DEPLOYMENT_CHECK_STATUS -eq 0 ]; then
              echo "DB deployment exists but no pods found. Checking events:"
              kubectl get events -n "$K8S_NAMESPACE" | grep db || echo "No DB events found"
              
              echo "Attempting to recreate the DB pod by restarting the deployment..."
              # Add set +e to prevent command from causing the script to exit on error
              set +e
              kubectl rollout restart deployment/db -n "$K8S_NAMESPACE"
              RESTART_STATUS=$?
              set -e
              
              if [ $RESTART_STATUS -ne 0 ]; then
                echo "WARNING: Failed to restart DB deployment, will proceed to next steps"
              fi
              
              # Wait for DB pod to appear
              echo "Waiting for DB pod to appear..."
              for i in {1..24}; do
                sleep 5
                # Add set +e to prevent command from causing the script to exit on error
                set +e
                NEW_DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
                POD_APPEAR_STATUS=$?
                set -e
                
                echo "DB pod check attempt $i, status: $POD_APPEAR_STATUS, result: ${NEW_DB_POD:-'(none)'}"
                
                if [ -n "$NEW_DB_POD" ]; then
                  echo "DB pod appeared: $NEW_DB_POD"
                  DB_POD=$NEW_DB_POD  # Set DB_POD for use in subsequent steps
                  break
                fi
                echo "Still waiting for DB pod... (attempt $i of 24)"
                
                if [ $i -eq 24 ]; then
                  echo "WARNING: Failed to find DB pod after 2 minutes"
                  echo "Will proceed to K8s deployment step which should create the database resources"
                  echo "Database migration step completed with warnings"
                  exit 0
                fi
              done
            else
              echo "DB deployment doesn't exist yet. This is likely the first deployment."
              echo "Will proceed to K8s deployment step which should create the database resources"
              echo "Database migration step completed with warnings"
              exit 0  # Exit here, don't continue to steps that depend on DB_POD
            fi
          fi
          
          # Only continue with the migration steps if we have a database pod
          if [ -z "$DB_POD" ]; then
            echo "No database pod available for migration steps. Skipping remaining migration tasks."
            echo "Database migration step completed with warnings"
            exit 0
          fi
          
          # We have a valid DB_POD at this point
          echo "Found database pod: $DB_POD"
          
          # Restart the database pod to trigger migrations
          echo "Restarting database pod to apply migrations..."
          # Add set +e to prevent command from causing the script to exit on error
          set +e
          kubectl delete pod $DB_POD -n "$K8S_NAMESPACE"
          POD_DELETE_STATUS=$?
          set -e
          
          if [ $POD_DELETE_STATUS -ne 0 ]; then
            echo "ERROR: Failed to delete database pod $DB_POD"
            kubectl describe pod $DB_POD -n "$K8S_NAMESPACE" || echo "Could not get pod details"
            echo "Will continue with deployment anyway as the DB pod might be replaced in later steps"
            echo "Database migration step completed with warnings"
            exit 0
          fi
          
          # Wait with better diagnostics
          echo "Waiting for new database pod to be created..."
          NEW_DB_POD=""
          for i in {1..12}; do
            sleep 5
            # Add set +e to prevent command from causing the script to exit on error
            set +e
            NEW_DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
            POD_NEW_STATUS=$?
            set -e
            
            echo "New DB pod check attempt $i, status: $POD_NEW_STATUS, result: ${NEW_DB_POD:-'(none)'}"
            
            if [ -n "$NEW_DB_POD" ] && [ "$NEW_DB_POD" != "$DB_POD" ]; then
              echo "New database pod created: $NEW_DB_POD"
              break
            elif [ -n "$NEW_DB_POD" ] && [ "$NEW_DB_POD" = "$DB_POD" ]; then
              echo "Old pod still exists, waiting... (attempt $i of 12)"
            else
              echo "No database pod found, waiting... (attempt $i of 12)"
            fi
            
            if [ $i -eq 12 ]; then
              echo "WARNING: Failed to find new database pod after 60 seconds"
              echo "Current pods:"
              kubectl get pods -n "$K8S_NAMESPACE" || echo "Could not list pods"
              echo "Will continue with deployment anyway as the DB pod might be created in later steps"
              echo "Database migration step completed with warnings"
              exit 0
            fi
          done
          
          # Check if NEW_DB_POD exists before continuing
          if [ -z "$NEW_DB_POD" ]; then
            echo "No new database pod found. Skipping remaining migration tasks."
            echo "Database migration step completed with warnings"
            exit 0
          fi
          
          # Wait for the pod to be ready with better diagnostics
          echo "Waiting for database pod to become ready..."
          # Add set +e to prevent command from causing the script to exit on error
          set +e
          kubectl describe pod/$NEW_DB_POD -n "$K8S_NAMESPACE" || echo "Could not describe pod $NEW_DB_POD"
          
          # Wait for the pod to be ready
          kubectl wait --for=condition=ready pod/$NEW_DB_POD --timeout=180s -n "$K8S_NAMESPACE"
          POD_READY_STATUS=$?
          set -e
          
          if [ $POD_READY_STATUS -ne 0 ]; then
            echo "WARNING: Database pod did not become ready in time"
            echo "Pod events:"
            kubectl describe pod/$NEW_DB_POD -n "$K8S_NAMESPACE" || echo "Could not describe pod"
            echo "Pod logs:"
            kubectl logs $NEW_DB_POD --tail=50 -n "$K8S_NAMESPACE" || echo "Could not retrieve logs"
            echo "Will continue with deployment anyway as the database might need more time"
            echo "Database migration step completed with warnings"
            exit 0
          fi
          
          # Check migration logs with better diagnostics
          echo "Migration logs:"
          set +e
          kubectl logs $NEW_DB_POD -n "$K8S_NAMESPACE" | grep -A 5 "MIGRATION" 
          MIGRATION_LOG_STATUS=$?
          set -e
          
          if [ $MIGRATION_LOG_STATUS -ne 0 ]; then
            echo "Migration log not found"
            echo "Full database pod logs:"
            kubectl logs $NEW_DB_POD --tail=50 -n "$K8S_NAMESPACE" || echo "Could not retrieve logs"
          fi
          
          # Scale services back up
          echo "Exiting maintenance mode: Scaling services back up..."
          set +e
          
          if kubectl get deployment worker -n "$K8S_NAMESPACE" &>/dev/null; then
            kubectl scale deployment worker --replicas=$WORKER_REPLICAS -n "$K8S_NAMESPACE" || echo "WARNING: Failed to scale worker back up"
          fi
          
          if kubectl get deployment backend -n "$K8S_NAMESPACE" &>/dev/null; then
            kubectl scale deployment backend --replicas=$BACKEND_REPLICAS -n "$K8S_NAMESPACE" || echo "WARNING: Failed to scale backend back up"
          fi
          
          if kubectl get deployment tf -n "$K8S_NAMESPACE" &>/dev/null; then
            kubectl scale deployment tf --replicas=$TF_REPLICAS -n "$K8S_NAMESPACE" || echo "WARNING: Failed to scale tf back up"
          fi
          set -e
          
          echo "Database migration step completed"

      # Deploy to Kubernetes
      - name: Deploy to Kubernetes
        run: |
          # Helper functions
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  echo "Retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            echo "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Verify cluster connectivity and context
          set +e
          kubectl_with_retry "kubectl cluster-info" || {
            echo "WARNING: Cluster connectivity issues detected, but continuing anyway"
          }
          
          # Verify correct context
          CURRENT_CONTEXT=$(kubectl config current-context)
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            echo "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          # Create namespace if it doesn't exist
          if ! kubectl get namespace "$K8S_NAMESPACE" &>/dev/null; then
            echo "Creating namespace $K8S_NAMESPACE..."
            kubectl create namespace "$K8S_NAMESPACE"
          fi
          
          # Apply Kubernetes configurations, excluding secrets.yaml
          echo "Applying Kubernetes configurations..."
          find config/prod/config -type f -name "*.yaml" ! -name "secrets.yaml" -exec kubectl apply -f {} --validate=false \; || {
            echo "WARNING: Some configuration files may have had errors - continuing anyway for first deployment"
          }
          
          # Give some time for resources to be created
          echo "Waiting for resources to initialize..."
          sleep 30
          
          # First, apply infrastructure components that others depend on
          echo "Updating and restarting infrastructure components first..."
          infra_components=("db" "cache")
          
          for deployment in "${infra_components[@]}"; do
            echo "Updating $deployment deployment..."
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              kubectl set image deployment/$deployment $deployment=$DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }} || {
                echo "WARNING: Failed to update $deployment image, but continuing"
              }
              
              # Restart the deployment
              kubectl rollout restart deployment/$deployment || {
                echo "WARNING: Failed to restart $deployment, but continuing"
              }
            else
              echo "Deployment $deployment doesn't exist yet - this is likely the first deployment"
            fi
          done
          
          # Wait for infra components to stabilize
          echo "Waiting for infrastructure components to stabilize..."
          sleep 90
          
          # Define the application deployments to update, in dependency order
          deployments=("tf" "backend" "worker" "frontend" "nginx")
          
          # Update image tags for all deployments
          echo "Updating deployments with images tagged: ${{ env.DOCKER_TAG }}..."
          
          for deployment in "${deployments[@]}"; do
            echo "Updating $deployment deployment..."
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              kubectl set image deployment/$deployment $deployment=$DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }} || {
                echo "WARNING: Failed to update $deployment image, but continuing"
              }
              
              # Special handling for worker deployment healthcheck container
              if [ "$deployment" = "worker" ]; then
                CONTAINERS=$(kubectl get deployment/worker -o jsonpath='{.spec.template.spec.containers[*].name}' 2>/dev/null)
                if [[ $CONTAINERS == *"db-healthcheck"* ]]; then
                  kubectl set image deployment/worker db-healthcheck=$DOCKER_USERNAME/worker-healthcheck:${{ env.DOCKER_TAG }}
                elif [[ $CONTAINERS == *"worker-healthcheck"* ]]; then
                  kubectl set image deployment/worker worker-healthcheck=$DOCKER_USERNAME/worker-healthcheck:${{ env.DOCKER_TAG }}
                fi
              fi
            else
              echo "Deployment $deployment doesn't exist yet - this is likely the first deployment"
            fi
          done
          
          # Apply rollout restart to ensure the current version of the image is used
          echo "Restarting deployments sequentially..."
          for deployment in "${deployments[@]}"; do
            kubectl rollout restart deployment/$deployment
            echo "Waiting briefly after restarting $deployment..."
            sleep 15
          done
          
          # Wait for image pull
          echo "Waiting for image pulls to complete..."
          sleep 60
          
          # Monitor deployments with increased timeout for first deployment
          echo "Monitoring deployments for successful rollout..."
          
          # Set timeout values for each deployment (in seconds) - increased for first deployment
          declare -A timeouts
          timeouts=([backend]=600 [frontend]=300 [worker]=600 [tf]=300 [nginx]=300)
          
          # Track deployment status
          FAILED_DEPLOYMENTS=""
          
          for deployment in "${deployments[@]}"; do
            echo "Waiting for $deployment deployment..."
            set +e
            kubectl rollout status deployment/$deployment --timeout=${timeouts[$deployment]}s
            ROLLOUT_STATUS=$?
            set -e
            
            if [ $ROLLOUT_STATUS -ne 0 ]; then
              echo "WARNING: $deployment deployment failed or timed out."
              echo "This might be expected during the first deployment due to dependencies."
              FAILED_DEPLOYMENTS="$FAILED_DEPLOYMENTS $deployment"
              
              echo "Details for $deployment deployment:"
              kubectl describe deployment $deployment
              echo "Pods for $deployment:"
              kubectl get pods -l app=$deployment
              echo "Logs for $deployment (if available):"
              kubectl logs -l app=$deployment --tail=50 || true
            else  
              echo "$deployment deployment succeeded."
            fi
          done
          
          # Check infrastructure component status as well
          for deployment in "${infra_components[@]}"; do
            echo "Checking status of $deployment..."
            kubectl get deployment $deployment -n "$K8S_NAMESPACE" || {
              echo "WARNING: $deployment deployment may not be ready yet"
            }
          done
          
          # Final status report
          echo "===== DEPLOYMENT STATUS REPORT ====="
          kubectl get deployments -n "$K8S_NAMESPACE"
          kubectl get pods -n "$K8S_NAMESPACE"
          
          if [ -n "$FAILED_DEPLOYMENTS" ]; then
            echo "The following deployments had issues: $FAILED_DEPLOYMENTS"
            echo "This might be expected during a first-time deployment since services depend on each other."
            echo "You may need to restart these services once the database and other dependencies are fully ready."
            
            # Don't fail the workflow for first deployment
            if kubectl get pod -l app=db -n "$K8S_NAMESPACE" | grep -q "Running"; then
              echo "DB pod is running, so this was not a first deployment. Marking as failure."
              exit 1
            else
              echo "DB pod is not yet running, this appears to be a first deployment. Marking as success with warnings."
              echo "Deployment completed with warnings for branch ${{ env.TARGET_BRANCH }}."
            fi
          else
            echo "Deployment completed successfully for branch ${{ env.TARGET_BRANCH }}."
          fi
      
      # Cleanup after deployment
      - name: Cleanup
        if: always()  # Run cleanup even if previous steps failed
        run: |
          echo "Cleaning up resources after deployment..."
          
          # Clean up Docker images to free space
          echo "Cleaning up unused Docker images..."
          # Remove images older than 24 hours that aren't tagged as latest, development, or stage
          docker image prune -af --filter "until=24h" --filter "label!=stage" --filter "label!=latest" --filter "label!=development"
          
          # Clean up system resources
          echo "Cleaning up system resources..."
          docker system prune -f --volumes
          
          echo "Cleanup completed." 