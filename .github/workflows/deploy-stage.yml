name: Deploy to Kubernetes (Staging)

on:
  # Deploy when pushing to stable branches
  push:
    branches:
      - main
  
  # Different trigger for pull requests with explicit configuration
  pull_request:
    types: [opened, synchronize, reopened]
    branches:
      - main
  
  # Manual triggering
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to deploy'
        required: true
        default: 'main'

# Add permissions configuration here
permissions:
  contents: read
  packages: write
  id-token: write

# Update concurrency control to cancel in-progress jobs
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  # Add a dependency on the lint-and-build workflow
  check-build:
    runs-on: ubuntu-latest  # Using GitHub-hosted runner
    if: github.event_name == 'pull_request'
    steps:
      - name: Check if lint-and-build workflow passed
        run: echo "This job ensures the lint-and-build workflow has passed before deployment"

  deploy-stage:
    runs-on: self-hosted  # Keep deployment on self-hosted runner
    needs: [check-build]
    if: >-
      (github.event_name != 'pull_request' || success())
    # Add job-level permissions to ensure access to secrets
    
    # Add environment variables for the job
    env:
      DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
      DOCKER_TOKEN: ${{ secrets.DOCKER_TOKEN }}
      DB_ROOT_PASSWORD: ${{ secrets.STAGE_DB_ROOT_PASSWORD }}
      REDIS_PASSWORD: ${{ secrets.STAGE_REDIS_PASSWORD }}
      POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      GEMINI_FREE_KEYS: ${{ secrets.GEMINI_FREE_KEYS }}
      GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID }}
      GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET }}
      JWT_SECRET: ${{ secrets.STAGE_JWT_SECRET }}
      CLOUDFLARE_TUNNEL_TOKEN: ${{ secrets.STAGE_CLOUDFLARE_TUNNEL_TOKEN }}
      GOOGLE_REDIRECT_URL: "https://stage.atlantis.trading/auth/google/callback"
      ENVIRONMENT: "stage"
      K8S_CONTEXT: "stage-cluster"
      K8S_NAMESPACE: "stage"
    
    steps:
      # Print debugging information before checkout
      - name: Debug Event Information
        run: |
          echo "Event name: ${{ github.event_name }}"
          echo "GitHub ref: ${{ github.ref }}"
          echo "Target branch: ${{ github.base_ref || github.ref_name }}"

      # Determine which branch to checkout based on event type
      - name: Set checkout target
        id: set-target
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "CHECKOUT_REF=${{ github.head_ref }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.base_ref }}" >> $GITHUB_ENV
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ github.event.inputs.branch }}" ]; then
            echo "CHECKOUT_REF=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
          else
            echo "CHECKOUT_REF=${{ github.ref_name }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.ref_name }}" >> $GITHUB_ENV
          fi

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: ${{ env.CHECKOUT_REF }}
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 1
          lfs: false
      
      # Add diagnostic step to see what's available after checkout
      - name: Show Git Information
        run: |
          echo "Current directory: $(pwd)"
          echo "Directory contents: $(ls -la)"
          echo "Git status: $(git status)"
          echo "Current branch: $(git branch --show-current)"
          git --version
      
      # Verify secrets are available without exposing them
      - name: Verify Docker Credentials
        run: |
          echo "Docker username secret: $(if [ -n "${{ secrets.DOCKER_USERNAME }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Docker token secret: $(if [ -n "${{ secrets.DOCKER_TOKEN }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "DB password secret: $(if [ -n "${{ secrets.STAGE_DB_ROOT_PASSWORD }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Polygon API key: $(if [ -n "${{ secrets.POLYGON_API_KEY }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Gemini keys: $(if [ -n "${{ secrets.GEMINI_FREE_KEYS }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Google OAuth credentials: $(if [ -n "${{ secrets.GOOGLE_CLIENT_ID }}" ] && [ -n "${{ secrets.GOOGLE_CLIENT_SECRET }}" ]; then echo "are set"; else echo "are NOT set"; fi)"
          echo "Cloudflare tunnel token: $(if [ -n "${{ secrets.STAGE_CLOUDFLARE_TUNNEL_TOKEN }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
      
      # Setup and configuration
      - name: Setup deployment
        run: |
          # Sanitize branch name for Docker tags (replace / with - and other invalid characters)
          DOCKER_TAG=$(echo "${{ env.TARGET_BRANCH }}" | sed 's/\//-/g' | sed 's/[^a-zA-Z0-9_.-]/-/g')
          echo "DOCKER_TAG=${DOCKER_TAG}" >> $GITHUB_ENV
          echo "Using Docker tag: ${DOCKER_TAG}"

      # Build Docker images
      - name: Build Docker Images
        run: |
          # Helper function to check required secrets
          check_secret() {
            if [ -z "${!1}" ]; then
              echo "ERROR: $1 secret is not available"
              exit 1
            fi
          }
          
          # Check required secrets
          check_secret "DOCKER_USERNAME"
          
          # Define services to build
          services=("frontend" "backend" "worker" "worker-healthcheck" "tf" "nginx" "db")
          
          # Build all images
          echo "Building Docker images with tag: ${{ env.DOCKER_TAG }}..."
          for service in "${services[@]}"; do
            dockerfile="services/${service}/Dockerfile.prod"
            # Special case for nginx and worker-healthcheck
            if [ "$service" = "nginx" ]; then
              dockerfile="services/nginx/Dockerfile"
            elif [ "$service" = "worker-healthcheck" ]; then
              dockerfile="services/worker/Dockerfile.healthcheck"
              docker build -t $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} -f $dockerfile services/worker
              continue
            fi
            
            docker build -t $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} -f $dockerfile services/$service
          done
          
          # Tag images as 'stage'
          echo "Tagging images as 'stage'..."
          for service in "${services[@]}"; do
            docker tag $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/$service:stage
          done

      # Push Docker images
      - name: Login to Docker Hub and Push Images
        run: |
          # Check required secrets
          if [ -z "$DOCKER_TOKEN" ]; then
            echo "ERROR: DOCKER_TOKEN secret is not available"
            exit 1
          fi
          
          # Login to Docker Hub
          echo "$DOCKER_TOKEN" | docker login -u "$DOCKER_USERNAME" --password-stdin || {
            echo "ERROR: Docker login failed. Please check your credentials."
            exit 1
          }
          
          # Define services to push
          services=("frontend" "backend" "worker" "worker-healthcheck" "tf" "db" "nginx")
          
          # Push branch-specific tags
          echo "Pushing images with tag: ${{ env.DOCKER_TAG }}..."
          for service in "${services[@]}"; do
            docker push $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }}
          done
          
          # Push stage tag
          echo "Pushing 'stage' tagged images..."
          for service in "${services[@]}"; do
            docker push $DOCKER_USERNAME/$service:stage
          done

      # Setup Kubernetes context
      - name: Setup Kubernetes Context
        run: |
          # Helper function for retrying commands
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  echo "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            echo "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Switch to staging Kubernetes context
          echo "Switching to staging Kubernetes context ($K8S_CONTEXT)..."
          kubectl_with_retry "kubectl config use-context $K8S_CONTEXT" || exit 1
          
          # Verify correct context is active
          CURRENT_CONTEXT=$(kubectl config current-context)
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            echo "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          echo "Confirmed correct Kubernetes context: $CURRENT_CONTEXT"
          
          # Set Kubernetes namespace for all commands
          kubectl_with_retry "kubectl config set-context --current --namespace=$K8S_NAMESPACE" || exit 1
          echo "Set Kubernetes namespace to: $K8S_NAMESPACE"
          
          # Verify cluster connectivity
          kubectl_with_retry "kubectl cluster-info" || exit 1

      # Update Kubernetes secrets
      - name: Update Kubernetes Secrets
        run: |
          # Helper function to check required secrets
          check_secret() {
            if [ -z "${!1}" ]; then
              echo "ERROR: $1 secret is not available"
              exit 1
            fi
          }
          
          # Check required secrets
          required_secrets=("DB_ROOT_PASSWORD" "REDIS_PASSWORD" "POLYGON_API_KEY" "GEMINI_FREE_KEYS" 
                          "GOOGLE_CLIENT_ID" "GOOGLE_CLIENT_SECRET" "JWT_SECRET" "CLOUDFLARE_TUNNEL_TOKEN")
          for secret in "${required_secrets[@]}"; do
            check_secret "$secret"
          done
          
          # Retry function for kubernetes operations
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  echo "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            echo "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Create temporary file for secrets
          TEMP_DIR=$(mktemp -d)
          SECRETS_FILE="$TEMP_DIR/secrets.yaml"
          
          # Encode secrets
          DB_PASSWORD_B64=$(echo -n "${{ secrets.STAGE_DB_ROOT_PASSWORD }}" | base64 -w 0)
          REDIS_PASSWORD_B64=$(echo -n "${{ secrets.STAGE_REDIS_PASSWORD }}" | base64 -w 0)
          POLYGON_API_KEY_B64=$(echo -n "${{ secrets.POLYGON_API_KEY }}" | base64 -w 0)
          GEMINI_FREE_KEYS_B64=$(echo -n "${{ secrets.GEMINI_FREE_KEYS }}" | base64 -w 0)
          GOOGLE_CLIENT_ID_B64=$(echo -n "${{ secrets.GOOGLE_CLIENT_ID }}" | base64 -w 0)
          GOOGLE_CLIENT_SECRET_B64=$(echo -n "${{ secrets.GOOGLE_CLIENT_SECRET }}" | base64 -w 0)
          JWT_SECRET_B64=$(echo -n "${{ secrets.STAGE_JWT_SECRET }}" | base64 -w 0)
          CLOUDFLARE_TUNNEL_TOKEN_B64=$(echo -n "${{ secrets.STAGE_CLOUDFLARE_TUNNEL_TOKEN }}" | base64 -w 0)
          
          # Create secrets YAML
          cat > "$SECRETS_FILE" << EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: redis-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            REDIS_PASSWORD: ${REDIS_PASSWORD_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: db-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            DB_ROOT_PASSWORD: ${DB_PASSWORD_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: polygon-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            api-key: ${POLYGON_API_KEY_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: gemini-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            GEMINI_FREE_KEYS: ${GEMINI_FREE_KEYS_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: google-oauth-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID_B64}
            GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: jwt-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            JWT_SECRET: ${JWT_SECRET_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: cloudflare-tunnel-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            CLOUDFLARE_TUNNEL_TOKEN: ${CLOUDFLARE_TUNNEL_TOKEN_B64}
          EOF
          
          # Apply secrets to Kubernetes
          echo "Applying secrets to Kubernetes..."
          kubectl_with_retry "kubectl apply -f \"$SECRETS_FILE\"" || kubectl_with_retry "kubectl apply -f \"$SECRETS_FILE\" --validate=false"
          
          # Clean up
          rm -rf "$TEMP_DIR"
          
          # Check if deployments exist and restart them if they do
          echo "Checking and restarting deployments if they exist..."
          if kubectl get deployment backend &>/dev/null; then
            kubectl rollout restart deployment/backend
          else
            echo "Note: backend deployment not found - skipping restart"
          fi
          
          if kubectl get deployment worker &>/dev/null; then
            kubectl rollout restart deployment/worker
          else
            echo "Note: worker deployment not found - skipping restart"
          fi

      # Run database migrations
      - name: Deploy Database Migrations
        run: |
          # Log function for better visibility
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Helper function for retrying commands
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  log "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            log "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Verify cluster connectivity with improved error handling
          log "Checking Kubernetes cluster connectivity..."
          if ! kubectl_with_retry "kubectl cluster-info"; then
            log "ERROR: Cannot connect to Kubernetes cluster."
            log "Current kubectl context: $(kubectl config current-context || echo 'NONE')"
            exit 1
          fi
          
          # Verify correct context
          CURRENT_CONTEXT=$(kubectl config current-context)
          log "Current Kubernetes context: $CURRENT_CONTEXT"
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            log "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          # Check cluster node status before proceeding
          log "Checking cluster node status..."
          NODE_STATUS=$(kubectl get nodes -o wide)
          log "Node status: $NODE_STATUS"
          
          # Check for any cluster-wide issues
          log "Checking for cluster-wide issues..."
          CLUSTER_EVENTS=$(kubectl get events --sort-by='.lastTimestamp' | grep -i "error\|warn\|fail" || echo "No recent error events")
          if [ "$CLUSTER_EVENTS" != "No recent error events" ]; then
            log "Recent cluster error events found:"
            echo "$CLUSTER_EVENTS"
          else
            log "No recent cluster error events found"
          fi
          
          # Check if namespace exists
          log "Checking if namespace $K8S_NAMESPACE exists..."
          if ! kubectl get namespace "$K8S_NAMESPACE" &>/dev/null; then
            log "Creating namespace $K8S_NAMESPACE..."
            kubectl create namespace "$K8S_NAMESPACE" || {
              log "ERROR: Failed to create namespace $K8S_NAMESPACE"
              log "Available namespaces:"
              kubectl get namespaces
              exit 1
            }
          fi
          
          # Get all pods to help diagnose issues
          log "Listing all pods in namespace $K8S_NAMESPACE:"
          kubectl get pods -n "$K8S_NAMESPACE"
          
          # Add check to ensure there's enough capacity
          log "Checking node capacity..."
          NODE_PRESSURE=$(kubectl describe nodes | grep -A5 "Conditions:" | grep -i "pressure\|true" || echo "No pressure conditions found")
          if [ "$NODE_PRESSURE" != "No pressure conditions found" ]; then
            log "WARNING: Node pressure conditions detected:"
            echo "$NODE_PRESSURE"
            log "This may affect pod scheduling"
          fi
          
          # Save current replica counts with better error handling
          log "Saving current deployment replica counts..."
          for deployment in worker backend tf; do
            log "Checking $deployment deployment..."
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              replicas=$(kubectl get deployment $deployment -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "1")
              echo "${deployment}_REPLICAS=$replicas" >> $GITHUB_ENV
              log "$deployment has $replicas replicas"
            else
              log "Note: $deployment deployment not found, will use default of 1 replica"
              echo "${deployment}_REPLICAS=1" >> $GITHUB_ENV
            fi
          done
          
          # Check if deployments exist before scaling
          log "Checking deployments before scaling down..."
          DEPLOYMENTS_TO_SCALE=""
          for deployment in worker backend tf; do
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              DEPLOYMENTS_TO_SCALE="$DEPLOYMENTS_TO_SCALE $deployment"
            else
              log "Note: $deployment deployment not found - skipping scale down"
            fi
          done
          
          # Scale down services if they exist - use faster approach
          if [ -n "$DEPLOYMENTS_TO_SCALE" ]; then
            log "Entering maintenance mode: Scaling down services that access the database..."
            kubectl scale deployment $DEPLOYMENTS_TO_SCALE --replicas=0 -n "$K8S_NAMESPACE" || {
              log "WARNING: Failed to scale down some deployments, proceeding anyway..."
            }
            
            # Wait for pods to terminate with status check - use shorter timeout
            log "Waiting for pods to terminate..."
            
            # Check pod termination with timeout
            end_time=$(($(date +%s) + 60))  # 60-second timeout
            while [ $(date +%s) -lt $end_time ]; do
              POD_COUNT=$(kubectl get pods -l "app in (worker,backend,tf)" --field-selector=status.phase=Running -n "$K8S_NAMESPACE" 2>/dev/null | wc -l)
              if [ "$POD_COUNT" -le 1 ]; then  # Account for header row
                log "All services pods terminated"
                break
              fi
              log "Waiting for pods to terminate... ($POD_COUNT remaining)"
              sleep 5
            done
            
            # Final check
            log "Checking if pods are terminated:"
            kubectl get pods -l "app in (worker,backend,tf)" -n "$K8S_NAMESPACE" || log "No matching pods found"
          else
            log "No deployments found to scale down, skipping this step"
          fi
          
          # Check if database pod exists with improved error handling
          log "Looking for database pod..."
          
          # Use set +e to prevent command from failing the script
          set +e
          DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' -n "$K8S_NAMESPACE" 2>/dev/null)
          DB_POD_STATUS=$?
          set -e
          
          if [ $DB_POD_STATUS -ne 0 ] || [ -z "$DB_POD" ]; then
            log "Database pod not found, checking if deployment exists..."
            
            set +e
            DB_DEPLOYMENT=$(kubectl get deployment db -n "$K8S_NAMESPACE" 2>/dev/null)
            DB_DEPLOYMENT_STATUS=$?
            set -e
            
            if [ $DB_DEPLOYMENT_STATUS -eq 0 ]; then
              log "DB deployment exists but no pods found. Checking events:"
              kubectl get events -n "$K8S_NAMESPACE" | grep db
              
              log "Attempting to recreate the DB pod by restarting the deployment..."
              kubectl rollout restart deployment/db -n "$K8S_NAMESPACE" || {
                log "WARNING: Failed to restart DB deployment"
              }
              
              # Wait for DB pod to appear with a 60-second timeout
              log "Waiting for DB pod to appear (60-second timeout)..."
              end_time=$(($(date +%s) + 60))
              while [ $(date +%s) -lt $end_time ]; do
                set +e
                NEW_DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' -n "$K8S_NAMESPACE" 2>/dev/null)
                NEW_DB_POD_STATUS=$?
                set -e
                
                if [ $NEW_DB_POD_STATUS -eq 0 ] && [ -n "$NEW_DB_POD" ]; then
                  log "DB pod appeared: $NEW_DB_POD"
                  DB_POD=$NEW_DB_POD
                  break
                fi
                log "Still waiting for DB pod to appear..."
                sleep 5
              done
              
              if [ -z "$DB_POD" ]; then
                log "WARNING: Failed to find DB pod after 60 seconds"
                log "Will proceed with K8s deployment step which should create necessary resources"
                
                # Force clear any stuck resources
                log "Checking for stuck resources..."
                STUCK_DB_PODS=$(kubectl get pods -l app=db -n "$K8S_NAMESPACE" 2>/dev/null || echo "None")
                if [ "$STUCK_DB_PODS" != "None" ]; then
                  log "Found potentially stuck DB pods, attempting to force delete:"
                  kubectl delete pods -l app=db -n "$K8S_NAMESPACE" --grace-period=0 --force || log "Could not force delete pods"
                fi
                
                # Create deployment by applying config
                log "Ensuring DB deployment exists by applying config..."
                find config/prod/config -type f -name "*.yaml" | xargs grep -l "app: db" | while read dbfile; do
                  log "Applying DB file: $dbfile"
                  kubectl apply -f "$dbfile" -n "$K8S_NAMESPACE" || log "Warning: Failed to apply $dbfile"
                done
                
                # Continue to the K8s deployment step
                echo "Database migration step skipped - will be handled in deployment"
                exit 0
              fi
            else
              log "DB deployment doesn't exist yet. This is likely the first deployment."
              log "Will proceed to K8s deployment step which should create the database resources"
              echo "Database migration step skipped - will be handled in deployment"
              exit 0
            fi
          fi
          
          log "Found database pod: $DB_POD"
          
          # Check DB pod status before proceeding
          log "Checking database pod status..."
          DB_POD_STATUS=$(kubectl get pod $DB_POD -n "$K8S_NAMESPACE" -o jsonpath='{.status.phase}')
          log "Database pod status: $DB_POD_STATUS"
          
          if [ "$DB_POD_STATUS" != "Running" ]; then
            log "Database pod is not running (status: $DB_POD_STATUS)"
            log "Pod details:"
            kubectl describe pod $DB_POD -n "$K8S_NAMESPACE"
            log "Attempting to fix by restarting the pod..."
          fi
          
          # Restart the database pod to trigger migrations (with retry)
          MAX_RESTART_ATTEMPTS=3
          for attempt in $(seq 1 $MAX_RESTART_ATTEMPTS); do
            log "Restarting database pod to apply migrations (attempt $attempt of $MAX_RESTART_ATTEMPTS)..."
            
            set +e
            kubectl delete pod $DB_POD -n "$K8S_NAMESPACE"
            DELETE_STATUS=$?
            set -e
            
            if [ $DELETE_STATUS -eq 0 ]; then
              log "Successfully deleted database pod $DB_POD"
              break
            elif [ $attempt -eq $MAX_RESTART_ATTEMPTS ]; then
              log "ERROR: Failed to delete database pod $DB_POD after $MAX_RESTART_ATTEMPTS attempts"
              kubectl describe pod $DB_POD -n "$K8S_NAMESPACE"
              log "Will try to force delete the pod..."
              kubectl delete pod $DB_POD -n "$K8S_NAMESPACE" --grace-period=0 --force || {
                log "Failed to force delete the pod. Will proceed with deployment anyway."
                echo "Database migration step completed with warnings"
                exit 0
              }
            else
              log "Failed to delete pod, retrying in 5 seconds..."
              sleep 5
            fi
          done
          
          # Wait for new database pod with improved diagnostics and shorter timeout
          log "Waiting for new database pod to be created (60-second timeout)..."
          end_time=$(($(date +%s) + 60))
          found_new_pod=false
          
          while [ $(date +%s) -lt $end_time ] && [ "$found_new_pod" = false ]; do
            set +e
            NEW_DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' -n "$K8S_NAMESPACE" 2>/dev/null)
            NEW_DB_POD_STATUS=$?
            set -e
            
            if [ $NEW_DB_POD_STATUS -eq 0 ] && [ -n "$NEW_DB_POD" ] && [ "$NEW_DB_POD" != "$DB_POD" ]; then
              log "New database pod created: $NEW_DB_POD"
              found_new_pod=true
            elif [ $NEW_DB_POD_STATUS -eq 0 ] && [ -n "$NEW_DB_POD" ] && [ "$NEW_DB_POD" = "$DB_POD" ]; then
              log "Old pod still exists, waiting..."
            else
              log "No database pod found, waiting..."
            fi
            
            if [ "$found_new_pod" = false ]; then
              sleep 5
            fi
          done
          
          if [ "$found_new_pod" = false ]; then
            log "WARNING: Failed to find new database pod after 60 seconds"
            log "Current pods:"
            kubectl get pods -n "$K8S_NAMESPACE"
            log "Will proceed with deployment anyway - database may need to be recreated"
            echo "Database migration step completed with warnings"
            exit 0
          fi
          
          # Wait for the pod to be ready with better diagnostics
          log "Waiting for database pod to become ready..."
          kubectl describe pod/$NEW_DB_POD -n "$K8S_NAMESPACE"
          
          set +e
          kubectl wait --for=condition=ready pod/$NEW_DB_POD --timeout=180s -n "$K8S_NAMESPACE"
          POD_READY_STATUS=$?
          set -e
          
          if [ $POD_READY_STATUS -ne 0 ]; then
            log "WARNING: Database pod did not become ready in time"
            log "Pod events:"
            kubectl describe pod/$NEW_DB_POD -n "$K8S_NAMESPACE"
            log "Pod logs:"
            kubectl logs $NEW_DB_POD --tail=50 -n "$K8S_NAMESPACE" || log "Could not retrieve logs"
            
            # Check for common issues
            EVENTS=$(kubectl describe pod/$NEW_DB_POD -n "$K8S_NAMESPACE" | grep -A10 "Events:")
            if [[ "$EVENTS" == *"unbound immediate PersistentVolumeClaims"* ]]; then
              log "PVC binding issue detected. Attempting to fix..."
              
              # Re-apply PV and PVC
              log "Re-applying PersistentVolume and PersistentVolumeClaim..."
              find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolume" | while read pvfile; do
                log "Re-applying PV file: $pvfile"
                kubectl apply -f "$pvfile" -n "$K8S_NAMESPACE" || log "Warning: Failed to apply $pvfile"
              done
              
              sleep 5
              
              find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolumeClaim" | while read pvcfile; do
                log "Re-applying PVC file: $pvcfile"
                kubectl apply -f "$pvcfile" -n "$K8S_NAMESPACE" || log "Warning: Failed to apply $pvcfile"
              done
              
              log "Deleting problematic database pod to trigger recreation..."
              kubectl delete pod $NEW_DB_POD -n "$K8S_NAMESPACE" --grace-period=0 --force || log "Warning: Failed to delete pod"
            fi
            
            log "Will continue with deployment anyway as the database might need more time"
            echo "Database migration step completed with warnings"
            exit 0
          fi
          
          # Check migration logs with better diagnostics
          log "Migration logs:"
          set +e
          kubectl logs $NEW_DB_POD -n "$K8S_NAMESPACE" | grep -A 5 "MIGRATION" 
          MIGRATION_LOG_STATUS=$?
          set -e
          
          if [ $MIGRATION_LOG_STATUS -ne 0 ]; then
            log "Migration log entries not found"
            log "Full database pod logs:"
            kubectl logs $NEW_DB_POD --tail=50 -n "$K8S_NAMESPACE" || log "Could not retrieve logs"
          fi
          
          # Scale services back up
          log "Exiting maintenance mode: Scaling services back up..."
          set +e
          
          for deployment in worker backend tf; do
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              replicas_var="${deployment}_REPLICAS"
              replicas=${!replicas_var}
              
              log "Scaling $deployment back to $replicas replicas..."
              kubectl scale deployment $deployment --replicas=$replicas -n "$K8S_NAMESPACE" || {
                log "WARNING: Failed to scale $deployment back up to $replicas replicas"
                # Try again with at least 1 replica
                kubectl scale deployment $deployment --replicas=1 -n "$K8S_NAMESPACE" || {
                  log "WARNING: Failed to scale $deployment to even 1 replica"
                }
              }
            fi
          done
          set -e
          
          log "Database migration step completed"

      # Deploy to Kubernetes
      - name: Deploy to Kubernetes
        run: |
          # Log function for better visibility
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Helper function for retrying commands
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  log "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            log "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Verify cluster connectivity with better error handling
          log "Checking Kubernetes cluster connectivity..."
          if ! kubectl_with_retry "kubectl cluster-info"; then
            log "ERROR: Cannot connect to Kubernetes cluster."
            log "Current kubectl context: $(kubectl config current-context || echo 'NONE')"
            exit 1
          fi
          
          # Verify correct context
          CURRENT_CONTEXT=$(kubectl config current-context)
          log "Current Kubernetes context: $CURRENT_CONTEXT"
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            log "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          # Create namespace if it doesn't exist
          if ! kubectl get namespace "$K8S_NAMESPACE" &>/dev/null; then
            log "Creating namespace $K8S_NAMESPACE..."
            kubectl_with_retry "kubectl create namespace \"$K8S_NAMESPACE\""
          fi
          
          # Clear any conflicting configurations first 
          log "Checking for conflicting deployments..."
          CONFLICTING_DEPLOYMENTS=$(kubectl get deployments -o jsonpath='{.items[?(@.metadata.annotations.deployment\.kubernetes\.io/revision)].metadata.name}' -n "$K8S_NAMESPACE" 2>/dev/null || echo "")
          if [ -n "$CONFLICTING_DEPLOYMENTS" ]; then
            log "Found potential conflicting deployments, resolving conflicts..."
            for deployment in $CONFLICTING_DEPLOYMENTS; do
              log "Rolling back $deployment to clean state..."
              kubectl_with_retry "kubectl rollout undo deployment/$deployment -n \"$K8S_NAMESPACE\"" || true
              sleep 2
              kubectl_with_retry "kubectl rollout restart deployment/$deployment -n \"$K8S_NAMESPACE\"" || true
            done
            sleep 5
          fi
          
          # Check for stuck PVCs and PVs from previous runs before proceeding
          log "Checking for stuck PersistentVolumeClaims..."
          STUCK_PVCS=$(kubectl get pvc -n "$K8S_NAMESPACE" -o jsonpath='{.items[?(@.status.phase=="Pending")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$STUCK_PVCS" ]; then
            log "Found stuck PVCs: $STUCK_PVCS"
            log "Will attempt to clean them up..."
            for pvc in $STUCK_PVCS; do
              log "Deleting stuck PVC: $pvc"
              kubectl delete pvc "$pvc" -n "$K8S_NAMESPACE" --force --grace-period=0 || log "Warning: Could not delete PVC $pvc"
            done
          fi
          
          # Check for orphaned PVs
          log "Checking for orphaned PersistentVolumes..."
          ORPHANED_PVS=$(kubectl get pv -o jsonpath='{.items[?(@.status.phase=="Released")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$ORPHANED_PVS" ]; then
            log "Found orphaned PVs: $ORPHANED_PVS"
            log "Will attempt to clean them up..."
            for pv in $ORPHANED_PVS; do
              log "Deleting orphaned PV: $pv"
              kubectl delete pv "$pv" --force --grace-period=0 || log "Warning: Could not delete PV $pv"
            done
          fi
          
          # First, find all PV yaml files
          log "Finding all PersistentVolume yaml files..."
          PV_FILES=$(find config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolume" 2>/dev/null || echo "")
          
          if [ -z "$PV_FILES" ]; then
            log "No PersistentVolume files found. Looking in different locations..."
            # Try alternative paths where PV files might be located
            PV_FILES=$(find . -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolume" 2>/dev/null || echo "")
          fi
          
          # Apply PVs first
          if [ -n "$PV_FILES" ]; then
            log "Applying PersistentVolumes first..."
            for pv_file in $PV_FILES; do
              log "Applying PV file: $pv_file"
              kubectl_with_retry "kubectl apply -f \"$pv_file\" --validate=false" || log "Warning: Failed to apply PV file $pv_file"
            done
            
            # Wait a moment for PVs to be registered in the system
            log "Waiting for PVs to be registered (10 seconds)..."
            sleep 10
            
            # List available PVs
            kubectl get pv
          else
            log "WARNING: No PersistentVolume yaml files found. This may cause PVC binding issues."
          fi
          
          # Find all PVC yaml files
          log "Finding all PersistentVolumeClaim yaml files..."
          PVC_FILES=$(find config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolumeClaim" 2>/dev/null || echo "")
          
          if [ -z "$PVC_FILES" ]; then
            log "No PersistentVolumeClaim files found. Looking in different locations..."
            # Try alternative paths where PVC files might be located
            PVC_FILES=$(find . -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolumeClaim" 2>/dev/null || echo "")
          fi
          
          # Apply PVCs
          if [ -n "$PVC_FILES" ]; then
            log "Applying PersistentVolumeClaims..."
            for pvc_file in $PVC_FILES; do
              log "Applying PVC file: $pvc_file"
              kubectl_with_retry "kubectl apply -f \"$pvc_file\" --validate=false" || log "Warning: Failed to apply PVC file $pvc_file"
            done
          else
            log "WARNING: No PersistentVolumeClaim yaml files found. This may cause deployment issues."
          fi
          
          # Wait for PVCs to be bound with more aggressive checking
          log "Waiting for PVCs to be bound..."
          
          # Get all PVC names
          PVC_NAMES=$(kubectl get pvc -n "$K8S_NAMESPACE" -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$PVC_NAMES" ]; then
            # Create a flag to track if all PVCs are bound
            all_pvcs_bound=false
            max_attempts=30
            attempt=1
            
            while [ "$all_pvcs_bound" = "false" ] && [ $attempt -le $max_attempts ]; do
              log "PVC binding check attempt $attempt of $max_attempts..."
              all_pvcs_bound=true
              
              for pvc in $PVC_NAMES; do
                pvc_status=$(kubectl get pvc "$pvc" -n "$K8S_NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                
                if [ "$pvc_status" != "Bound" ]; then
                  log "PVC $pvc is not bound yet (status: $pvc_status)"
                  all_pvcs_bound=false
                  
                  # If we're on later attempts, try to diagnose and fix issues
                  if [ $attempt -gt 10 ]; then
                    log "Trying to diagnose issues with PVC $pvc..."
                    kubectl describe pvc "$pvc" -n "$K8S_NAMESPACE"
                    
                    # Check for PV that should bind to this PVC
                    log "Looking for matching PersistentVolumes..."
                    pvc_storage_class=$(kubectl get pvc "$pvc" -n "$K8S_NAMESPACE" -o jsonpath='{.spec.storageClassName}' 2>/dev/null || echo "")
                    pvc_size=$(kubectl get pvc "$pvc" -n "$K8S_NAMESPACE" -o jsonpath='{.spec.resources.requests.storage}' 2>/dev/null || echo "")
                    
                    log "PVC $pvc requests storageClass: ${pvc_storage_class:-'(default)'}, size: ${pvc_size:-'(unknown)'}"
                    
                    # Find available PVs that could match
                    matching_pvs=$(kubectl get pv -o json | jq -r '.items[] | select(.status.phase == "Available") | .metadata.name' 2>/dev/null || echo "")
                    log "Available PVs: ${matching_pvs:-'(none)'}"
                    
                    # If this is the last attempt, try recreating the PVC
                    if [ $attempt -eq $max_attempts ]; then
                      log "Last attempt, trying to recreate PVC $pvc..."
                      pvc_yaml=$(kubectl get pvc "$pvc" -n "$K8S_NAMESPACE" -o yaml)
                      kubectl delete pvc "$pvc" -n "$K8S_NAMESPACE" --force --grace-period=0
                      sleep 5
                      echo "$pvc_yaml" | kubectl apply -f - --validate=false || log "Failed to recreate PVC $pvc"
                    fi
                  fi
                else
                  log "PVC $pvc is bound"
                fi
              done
              
              # If not all PVCs are bound, wait before the next check
              if [ "$all_pvcs_bound" = "false" ]; then
                log "Not all PVCs are bound yet, waiting 10 seconds..."
                kubectl get pvc -n "$K8S_NAMESPACE"
                kubectl get pv
                sleep 10
                attempt=$((attempt + 1))
              fi
            done
            
            if [ "$all_pvcs_bound" = "false" ]; then
              log "WARNING: Not all PVCs could be bound after $max_attempts attempts."
              log "Will continue with deployment anyway, but some pods may not start correctly."
            else
              log "All PVCs are bound successfully!"
            fi
          else
            log "No PVCs found in namespace $K8S_NAMESPACE."
          fi
          
          # Apply ConfigMaps and Secrets
          log "Applying ConfigMaps and Secrets..."
          find config -type f -name "*.yaml" | xargs grep -l "kind: ConfigMap\|kind: Secret" 2>/dev/null | while read config_file; do
            log "Applying ConfigMap/Secret file: $config_file"
            kubectl_with_retry "kubectl apply -f \"$config_file\" --validate=false -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to apply $config_file"
          done
          
          # Apply Services before Deployments
          log "Applying Services and other non-Deployment resources..."
          find config -type f -name "*.yaml" | grep -v "deployment" | xargs grep -l "kind: Service" 2>/dev/null | while read svc_file; do
            log "Applying Service file: $svc_file"
            kubectl_with_retry "kubectl apply -f \"$svc_file\" --validate=false -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to apply $svc_file"
          done
          
          # Apply all other configurations excluding previously applied resources
          log "Applying remaining Kubernetes configurations..."
          find config -type f -name "*.yaml" | while read config_file; do
            if ! grep -q "kind: PersistentVolume\|kind: PersistentVolumeClaim\|kind: ConfigMap\|kind: Secret\|kind: Service" "$config_file" 2>/dev/null; then
              log "Applying config file: $config_file"
              kubectl_with_retry "kubectl apply -f \"$config_file\" --validate=false -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to apply $config_file"
            fi
          done
          
          # Define deployments in order of dependency
          log "Setting up deployment updates..."
          infra_components=("db" "cache")
          app_deployments=("tf" "backend" "worker" "frontend" "nginx")
          ALL_DEPLOYMENTS=("${infra_components[@]}" "${app_deployments[@]}")
          
          # Log deployments that will be handled
          log "Planning to update these deployments: ${ALL_DEPLOYMENTS[*]}"
          
          # Image tag to use
          log "Using image tag: ${{ env.DOCKER_TAG }}"
          
          # Check for missing deployments and apply any deployment files not already applied
          log "Checking for missing deployments..."
          for deployment in "${ALL_DEPLOYMENTS[@]}"; do
            if ! kubectl get deployment "$deployment" -n "$K8S_NAMESPACE" &>/dev/null; then
              log "Deployment $deployment does not exist, looking for deployment file..."
              deployment_files=$(find config -type f -name "*${deployment}*deployment*.yaml" 2>/dev/null || echo "")
              
              if [ -n "$deployment_files" ]; then
                for deployment_file in $deployment_files; do
                  log "Applying missing deployment file: $deployment_file"
                  kubectl_with_retry "kubectl apply -f \"$deployment_file\" --validate=false -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to apply $deployment_file"
                done
              else
                log "No deployment file found for $deployment"
              fi
            fi
          done
          
          # First, update infrastructure components in parallel with timeouts
          log "Updating infrastructure components..."
          for deployment in "${infra_components[@]}"; do
            {
              if kubectl get deployment "$deployment" -n "$K8S_NAMESPACE" &>/dev/null; then
                log "Setting $deployment image to $DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }}"
                if timeout 30s kubectl set image deployment/$deployment $deployment=$DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }} -n "$K8S_NAMESPACE" 2>/dev/null; then
                  log "Successfully updated $deployment image"
                else
                  log "Warning: Failed to update $deployment image, will try with rollout restart"
                fi
              else
                log "Note: $deployment deployment doesn't exist yet - looking for deployment file"
                deployment_files=$(find config -type f -name "*${deployment}*deployment*.yaml" 2>/dev/null || echo "")
                
                if [ -n "$deployment_files" ]; then
                  for deployment_file in $deployment_files; do
                    log "Applying missing deployment file: $deployment_file"
                    kubectl_with_retry "kubectl apply -f \"$deployment_file\" --validate=false -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to apply $deployment_file"
                  done
                fi
              fi
            } &
          done
          wait
          
          # Update app deployments in parallel with timeouts
          log "Updating application deployments in parallel..."
          for deployment in "${app_deployments[@]}"; do
            {
              if kubectl get deployment "$deployment" -n "$K8S_NAMESPACE" &>/dev/null; then
                log "Setting $deployment image to $DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }}"
                if timeout 30s kubectl set image deployment/$deployment $deployment=$DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }} -n "$K8S_NAMESPACE" 2>/dev/null; then
                  log "Successfully updated $deployment image"
                  
                  # Special handling for worker deployment healthcheck container
                  if [ "$deployment" = "worker" ]; then
                    CONTAINERS=$(kubectl get deployment/worker -o jsonpath='{.spec.template.spec.containers[*].name}' -n "$K8S_NAMESPACE" 2>/dev/null || echo "")
                    if [[ $CONTAINERS == *"db-healthcheck"* ]]; then
                      kubectl set image deployment/worker db-healthcheck=$DOCKER_USERNAME/worker-healthcheck:${{ env.DOCKER_TAG }} -n "$K8S_NAMESPACE"
                    elif [[ $CONTAINERS == *"worker-healthcheck"* ]]; then
                      kubectl set image deployment/worker worker-healthcheck=$DOCKER_USERNAME/worker-healthcheck:${{ env.DOCKER_TAG }} -n "$K8S_NAMESPACE"
                    fi
                  fi
                else
                  log "Warning: Failed to update $deployment image, will try with rollout restart"
                fi
              else
                log "Note: $deployment deployment doesn't exist yet - looking for deployment file"
                deployment_files=$(find config -type f -name "*${deployment}*deployment*.yaml" 2>/dev/null || echo "")
                
                if [ -n "$deployment_files" ]; then
                  for deployment_file in $deployment_files; do
                    log "Applying missing deployment file: $deployment_file"
                    kubectl_with_retry "kubectl apply -f \"$deployment_file\" --validate=false -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to apply $deployment_file"
                  done
                fi
              fi
            } &
          done
          wait
          
          # Apply rollout restart to all deployments in parallel
          log "Restarting all deployments in parallel for faster rollout..."
          
          # Make sure PVs and PVCs are correctly bound before proceeding
          log "Final check of PVC binding status before restarting deployments..."
          UNBOUND_PVCS=$(kubectl get pvc -n "$K8S_NAMESPACE" -o jsonpath='{.items[?(@.status.phase!="Bound")].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$UNBOUND_PVCS" ]; then
            log "WARNING: There are still unbound PVCs: $UNBOUND_PVCS"
            log "Displaying PV and PVC status:"
            kubectl get pv
            kubectl get pvc -n "$K8S_NAMESPACE"
            log "This may cause issues with the deployment."
          else
            log "All PVCs appear to be bound correctly!"
          fi
          
          # Restart infrastructure first with forced deletion if they're stuck in Pending
          log "Restarting infrastructure components..."
          for deployment in "${infra_components[@]}"; do
            {
              if kubectl get deployment "$deployment" -n "$K8S_NAMESPACE" &>/dev/null; then
                # Check if there are stuck pods
                STUCK_PODS=$(kubectl get pods -l app="$deployment" -n "$K8S_NAMESPACE" | grep -E "Pending|Error|CrashLoopBackOff" || echo "")
                if [ -n "$STUCK_PODS" ]; then
                  log "Found stuck pods for $deployment, forcing deletion..."
                  kubectl delete pods -l app="$deployment" -n "$K8S_NAMESPACE" --force --grace-period=0 || true
                  sleep 5
                fi
                
                log "Restarting $deployment..."
                kubectl_with_retry "kubectl rollout restart deployment/$deployment -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to restart $deployment"
              else
                log "Deployment $deployment doesn't exist yet - skipping restart"
              fi
            } &
          done
          wait
          
          # Brief wait for infrastructure components to be recreated after restart
          log "Waiting for infrastructure components to initialize (30 seconds)..."
          sleep 30
          
          # Force recreate any stuck pending pods
          log "Checking for stuck pods before application restart..."
          STUCK_PODS=$(kubectl get pods -n "$K8S_NAMESPACE" | grep -E "Pending|Error|CrashLoopBackOff" || echo "")
          if [ -n "$STUCK_PODS" ]; then
            log "Found stuck pods, attempting to force delete:"
            echo "$STUCK_PODS"
            kubectl get pods -n "$K8S_NAMESPACE" | grep -E "Pending|Error|CrashLoopBackOff" | awk '{print $1}' | xargs -r kubectl delete pod -n "$K8S_NAMESPACE" --force --grace-period=0 || true
            sleep 10
          fi
          
          # Restart all application deployments in parallel
          log "Restarting application deployments..."
          for deployment in "${app_deployments[@]}"; do
            {
              if kubectl get deployment "$deployment" -n "$K8S_NAMESPACE" &>/dev/null; then
                log "Restarting $deployment..."
                kubectl_with_retry "kubectl rollout restart deployment/$deployment -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to restart $deployment"
              else
                log "Deployment $deployment doesn't exist yet - skipping restart"
              fi
            } &
          done
          wait
          
          # Monitor deployment status
          log "Waiting for deployments to restart..."
          sleep 30
          
          # Set up monitoring for all deployments
          pids=()
          deployment_failures=()
          
          log "Monitoring deployment status..."
          for deployment in "${ALL_DEPLOYMENTS[@]}"; do
            if kubectl get deployment "$deployment" -n "$K8S_NAMESPACE" &>/dev/null; then
              {
                log "Monitoring $deployment deployment..."
                if ! timeout 300s kubectl rollout status deployment/"$deployment" -n "$K8S_NAMESPACE"; then
                  log "WARNING: $deployment rollout did not complete in time"
                  # Save to track deployment failures
                  echo "$deployment" > "/tmp/deployment_failures_$deployment"
                  deployment_failures+=("$deployment")
                else
                  log "$deployment rollout completed successfully"
                fi
              } &
              pids+=($!)
            fi
          done
          
          # Wait for all monitoring to complete
          for pid in "${pids[@]}"; do
            wait $pid || true
          done
          
          # Check for any deployment failures
          FAILURE_FILES=$(ls /tmp/deployment_failures_* 2>/dev/null || echo "")
          if [ -n "$FAILURE_FILES" ]; then
            log "Some deployments failed. See details above."
            
            # Try to salvage the deployment by retrying failed deployments
            log "Attempting to salvage deployment by retrying failed deployments..."
            for file in $FAILURE_FILES; do
              deployment=$(cat "$file")
              log "Retrying $deployment deployment..."
              kubectl_with_retry "kubectl rollout restart deployment/$deployment -n \"$K8S_NAMESPACE\"" || log "Warning: Failed to restart $deployment again"
              rm "$file"
            done
            
            # Check pod status for failures
            log "Checking pod status to diagnose failures..."
            kubectl get pods -n "$K8S_NAMESPACE"
            
            # Try to get logs from failed pods
            FAILED_PODS=$(kubectl get pods -n "$K8S_NAMESPACE" | grep -v "Running\|Completed" | tail -n +2 | awk '{print $1}')
            for pod in $FAILED_PODS; do
              log "Details for failed pod $pod:"
              kubectl describe pod "$pod" -n "$K8S_NAMESPACE" | grep -A 10 "Events:"
            done
            
            # Don't fail the workflow, but warn that there were issues
            log "WARNING: There were deployment issues that were attempted to be resolved."
            log "Check the application status manually to ensure it's functioning correctly."
          else
            log "All deployments completed successfully!"
          fi
          
          # Final status check
          log "Final deployment status:"
          kubectl get deployments -n "$K8S_NAMESPACE"
          kubectl get pods -n "$K8S_NAMESPACE"
          kubectl get pvc -n "$K8S_NAMESPACE"
          kubectl get pv
          
          log "Deployment process completed for branch ${{ env.TARGET_BRANCH }}."
      
      # Cleanup after deployment
      - name: Cleanup
        if: always()  # Run cleanup even if previous steps failed
        run: |
          echo "Cleaning up resources after deployment..."
          
          # Clean up Docker images to free space
          echo "Cleaning up unused Docker images..."
          # Remove images older than 24 hours that aren't tagged as latest, development, or stage
          docker image prune -af --filter "until=24h" --filter "label!=stage" --filter "label!=latest" --filter "label!=development"
          
          # Clean up system resources
          echo "Cleaning up system resources..."
          docker system prune -f --volumes
          
          echo "Cleanup completed." 