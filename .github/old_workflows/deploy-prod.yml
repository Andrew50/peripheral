name: Deploy to Kubernetes

on:
  # Deploy when pushing to stable branches
  push:
    branches:
      - dev
      - prod
  
  # Different trigger for pull requests with explicit configuration
  pull_request:
    types: [opened, synchronize, reopened]
    branches:
      - dev
      - prod
  
  # Manual triggering
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to deploy'
        required: true
        default: 'dev'

# Add permissions configuration here
permissions:
  contents: read
  packages: write
  id-token: write

# Update concurrency control to cancel in-progress jobs
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  # Add a dependency on the lint-and-build workflow
  check-build:
    runs-on: ubuntu-latest  # Using GitHub-hosted runner
    if: github.event_name == 'pull_request'
    steps:
      - name: Check if lint-and-build workflow passed
        run: echo "This job ensures the lint-and-build workflow has passed before deployment"

  deploy-prod:
    runs-on: self-hosted  # Keep deployment on self-hosted runner
    needs: [check-build]
    if: >-
      (github.event_name != 'pull_request' || success()) && 
      !(github.event_name == 'pull_request' && github.base_ref == 'main' && github.head_ref == 'prod') && 
      !(github.event_name == 'push' && github.ref == 'refs/heads/main' && 
        github.event.before != '0000000000000000000000000000000000000000' && 
        github.event.commits != null && 
        github.event.commits[0] != null && 
        contains(github.event.commits[0].message || '', 'Merge pull request') && 
        contains(github.event.commits[0].message || '', 'from prod'))
    # Add job-level permissions to ensure access to secrets
    
    # Add environment variables for the job
    env:
      DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
      DOCKER_TOKEN: ${{ secrets.DOCKER_TOKEN }}
      DB_ROOT_PASSWORD: ${{ secrets.DB_ROOT_PASSWORD }}
      REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
      POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
      GEMINI_FREE_KEYS: ${{ secrets.GEMINI_FREE_KEYS }}
      GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID }}
      GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET }}
      JWT_SECRET: ${{ secrets.JWT_SECRET }}
      GOOGLE_REDIRECT_URL: "https://atlantis.trading/auth/google/callback"
      K8S_CONTEXT: "prod-cluster"
      K8S_NAMESPACE: "default"
    
    steps:
      # Print debugging information before checkout
      - name: Debug Event Information
        run: |
          echo "Event name: ${{ github.event_name }}"
          echo "GitHub ref: ${{ github.ref }}"
          echo "Target branch: ${{ github.base_ref || github.ref_name }}"

      # Determine which branch to checkout based on event type
      - name: Set checkout target
        id: set-target
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "CHECKOUT_REF=${{ github.head_ref }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.base_ref }}" >> $GITHUB_ENV
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ github.event.inputs.branch }}" ]; then
            echo "CHECKOUT_REF=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.event.inputs.branch }}" >> $GITHUB_ENV
          else
            echo "CHECKOUT_REF=${{ github.ref_name }}" >> $GITHUB_ENV
            echo "TARGET_BRANCH=${{ github.ref_name }}" >> $GITHUB_ENV
          fi

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          ref: ${{ env.CHECKOUT_REF }}
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 1
          lfs: false
      
      # Add diagnostic step to see what's available after checkout
      - name: Show Git Information
        run: |
          echo "Current directory: $(pwd)"
          echo "Directory contents: $(ls -la)"
          echo "Git status: $(git status)"
          echo "Current branch: $(git branch --show-current)"
          git --version
      
      # Verify secrets are available without exposing them
      - name: Verify Docker Credentials
        run: |
          echo "Docker username secret: $(if [ -n "${{ secrets.DOCKER_USERNAME }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Docker token secret: $(if [ -n "${{ secrets.DOCKER_TOKEN }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "DB password secret: $(if [ -n "${{ secrets.DB_ROOT_PASSWORD }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Polygon API key: $(if [ -n "${{ secrets.POLYGON_API_KEY }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Gemini keys: $(if [ -n "${{ secrets.GEMINI_FREE_KEYS }}" ]; then echo "is set"; else echo "is NOT set"; fi)"
          echo "Google OAuth credentials: $(if [ -n "${{ secrets.GOOGLE_CLIENT_ID }}" ] && [ -n "${{ secrets.GOOGLE_CLIENT_SECRET }}" ]; then echo "are set"; else echo "are NOT set"; fi)"
      
      # Setup and configuration
      - name: Setup deployment
        run: |
          # Log function for better readability
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Sanitize branch name for Docker tags (replace / with - and other invalid characters)
          DOCKER_TAG=$(echo "${{ env.TARGET_BRANCH }}" | sed 's/\//-/g' | sed 's/[^a-zA-Z0-9_.-]/-/g')
          echo "DOCKER_TAG=${DOCKER_TAG}" >> $GITHUB_ENV
          
          log "Using Docker tag: ${DOCKER_TAG}"
          log "Starting deployment process for branch: ${{ env.TARGET_BRANCH }}..."

      # Build Docker images
      - name: Build Docker Images
        run: |
          # Helper function to check required secrets
          check_secret() {
            if [ -z "${!1}" ]; then
              echo "ERROR: $1 secret is not available"
              exit 1
            fi
          }
          
          # Log function for better visibility
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Check required secrets
          check_secret "DOCKER_USERNAME"
          
          # Define services to build
          services=("frontend" "backend" "worker" "worker-healthcheck" "tf" "nginx" "db")
          
          # Build all images in parallel for faster deployment
          log "Building Docker images with tag: ${{ env.DOCKER_TAG }}..."
          
          build_service() {
            local service=$1
            local dockerfile="services/${service}/Dockerfile.prod"
            
            # Special case for nginx and worker-healthcheck
            if [ "$service" = "nginx" ]; then
              dockerfile="services/nginx/Dockerfile"
            elif [ "$service" = "worker-healthcheck" ]; then
              dockerfile="services/worker/Dockerfile.healthcheck"
              log "Building $service image..."
              docker build -t $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} -f $dockerfile services/worker
              return
            fi
            
            log "Building $service image..."
            docker build -t $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} -f $dockerfile services/$service
          }
          
          # Launch builds in parallel with a limit of 3 concurrent builds
          pids=()
          for service in "${services[@]}"; do
            # Control concurrency to avoid overwhelming the system
            if [ ${#pids[@]} -ge 3 ]; then
              wait "${pids[0]}"
              pids=("${pids[@]:1}")
            fi
            
            build_service "$service" &
            pids+=($!)
          done
          
          # Wait for all background processes to finish
          for pid in "${pids[@]}"; do
            wait $pid || { log "ERROR: A build process failed"; exit 1; }
          done
          
          # Tag images appropriately for environment
          if [ "${{ env.TARGET_BRANCH }}" = "prod" ]; then
            log "Tagging images as 'latest' for production..."
            for service in "${services[@]}"; do
              docker tag $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/$service:latest
            done
          elif [ "${{ env.TARGET_BRANCH }}" = "dev" ]; then
            log "Tagging images as 'development'..."
            for service in "${services[@]}"; do
              docker tag $DOCKER_USERNAME/$service:${{ env.DOCKER_TAG }} $DOCKER_USERNAME/$service:development
            done
          fi

      # Push Docker images
      - name: Login to Docker Hub and Push Images
        run: |
          # Log function for better visibility
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Check required secrets
          if [ -z "$DOCKER_TOKEN" ]; then
            echo "ERROR: DOCKER_TOKEN secret is not available"
            exit 1
          fi
          
          # Login to Docker Hub
          log "Logging into Docker Hub..."
          echo "$DOCKER_TOKEN" | docker login -u "$DOCKER_USERNAME" --password-stdin || {
            echo "ERROR: Docker login failed. Please check your credentials."
            exit 1
          }
          
          # Define services to push
          services=("frontend" "backend" "worker" "worker-healthcheck" "tf" "db" "nginx")
          
          # Function to push an image
          push_image() {
            local service=$1
            local tag=$2
            log "Pushing $service:$tag image..."
            docker push $DOCKER_USERNAME/$service:$tag
          }
          
          # Push branch-specific tags in parallel
          log "Pushing images with tag: ${{ env.DOCKER_TAG }}..."
          pids=()
          for service in "${services[@]}"; do
            # Control concurrency to avoid overwhelming DockerHub
            if [ ${#pids[@]} -ge 3 ]; then
              wait "${pids[0]}"
              pids=("${pids[@]:1}")
            fi
            
            push_image "$service" "${{ env.DOCKER_TAG }}" &
            pids+=($!)
          done
          
          # Wait for all branch-specific tags to finish pushing
          for pid in "${pids[@]}"; do
            wait $pid || { log "ERROR: A push process failed"; exit 1; }
          done
          
          # Push environment-specific tags in parallel
          if [ "${{ env.TARGET_BRANCH }}" = "prod" ]; then
            log "Pushing 'latest' tagged images..."
            pids=()
            for service in "${services[@]}"; do
              # Control concurrency
              if [ ${#pids[@]} -ge 3 ]; then
                wait "${pids[0]}"
                pids=("${pids[@]:1}")
              fi
              
              push_image "$service" "latest" &
              pids+=($!)
            done
          elif [ "${{ env.TARGET_BRANCH }}" = "dev" ]; then
            log "Pushing 'development' tagged images..."
            pids=()
            for service in "${services[@]}"; do
              # Control concurrency
              if [ ${#pids[@]} -ge 3 ]; then
                wait "${pids[0]}"
                pids=("${pids[@]:1}")
              fi
              
              push_image "$service" "development" &
              pids+=($!)
            done
          fi
          
          # Wait for all environment-specific tags to finish pushing
          for pid in "${pids[@]}"; do
            wait $pid || { log "ERROR: A push process failed"; exit 1; }
          done
          
          log "All images pushed successfully"

      # Setup Kubernetes context
      - name: Setup Kubernetes Context
        run: |
          # Helper function for retrying commands
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  echo "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            echo "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Log function for better visibility
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Switch to production Kubernetes context
          log "Switching to production Kubernetes context ($K8S_CONTEXT)..."
          kubectl_with_retry "kubectl config use-context $K8S_CONTEXT" || exit 1
          
          # Verify correct context is active
          CURRENT_CONTEXT=$(kubectl config current-context)
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            echo "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          log "Confirmed correct Kubernetes context: $CURRENT_CONTEXT"
          
          # Set Kubernetes namespace for all commands
          kubectl_with_retry "kubectl config set-context --current --namespace=$K8S_NAMESPACE" || exit 1
          log "Set Kubernetes namespace to: $K8S_NAMESPACE"
          
          # Verify cluster connectivity
          kubectl_with_retry "kubectl cluster-info" || exit 1
          
          # List all nodes and their status
          log "Checking cluster nodes and capacity..."
          kubectl get nodes -o wide
          kubectl describe nodes | grep -A5 "Capacity\|Allocatable"

      # Check and ensure PVCs are bound
      - name: Verify PersistentVolumeClaims
        run: |
          # Log function for better visibility
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Helper function for retrying commands
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  log "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            log "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          log "Checking existing PersistentVolumeClaims..."
          kubectl get pvc || log "No PVCs found yet"
          
          log "Checking existing PersistentVolumes..."
          kubectl get pv || log "No PVs found yet"
          
          # Extract and apply only PV and PVC files first
          log "Ensuring PersistentVolumes exist before PersistentVolumeClaims..."
          
          # Find all PV files and apply them
          find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolume" | while read pvfile; do
            log "Applying PV file: $pvfile"
            kubectl_with_retry "kubectl apply -f \"$pvfile\"" || log "Warning: Failed to apply $pvfile"
          done
          
          # Give time for PVs to be registered
          log "Waiting for PVs to be registered..."
          sleep 5
          
          # Find all PVC files and apply them
          find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolumeClaim" | while read pvcfile; do
            log "Applying PVC file: $pvcfile"
            kubectl_with_retry "kubectl apply -f \"$pvcfile\"" || log "Warning: Failed to apply $pvcfile"
          done
          
          # Wait for PVCs to be bound
          log "Waiting for PVCs to be bound..."
          
          # Get all PVC names
          PVC_NAMES=$(kubectl get pvc -o jsonpath='{.items[*].metadata.name}')
          
          if [ -n "$PVC_NAMES" ]; then
            # Check each PVC status
            for pvc in $PVC_NAMES; do
              log "Checking status of PVC: $pvc"
              
              # Wait for the PVC to be bound with a timeout
              if ! kubectl wait --for=condition=Bound pvc/$pvc --timeout=60s; then
                log "WARNING: PVC $pvc not bound after 60 seconds"
                
                # Get details for debugging
                kubectl describe pvc/$pvc
                
                # Check if there are any related pod scheduling issues
                POD_ISSUES=$(kubectl get pods | grep "PVC" | grep "Pending" || echo "None")
                if [ "$POD_ISSUES" != "None" ]; then
                  log "Found pods with PVC issues: $POD_ISSUES"
                  kubectl describe pods | grep -A15 "PVC" | grep -A10 "Events"
                fi
              else
                log "PVC $pvc is bound"
              fi
            done
          else
            log "No PVCs found in the namespace"
          fi
          
          # Show final PVC status
          log "Current PVC status:"
          kubectl get pvc
          
          log "Current PV status:"
          kubectl get pv

      # Update Kubernetes secrets
      - name: Update Kubernetes Secrets
        run: |
          # Helper function to check required secrets
          check_secret() {
            if [ -z "${!1}" ]; then
              echo "ERROR: $1 secret is not available"
              exit 1
            fi
          }
          
          # Check required secrets
          required_secrets=("DB_ROOT_PASSWORD" "REDIS_PASSWORD" "POLYGON_API_KEY" "GEMINI_FREE_KEYS" 
                          "GOOGLE_CLIENT_ID" "GOOGLE_CLIENT_SECRET" "JWT_SECRET")
          for secret in "${required_secrets[@]}"; do
            check_secret "$secret"
          done
          
          # Retry function for kubernetes operations
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  echo "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            echo "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Create temporary file for secrets
          TEMP_DIR=$(mktemp -d)
          SECRETS_FILE="$TEMP_DIR/secrets.yaml"
          
          # Encode secrets
          DB_PASSWORD_B64=$(echo -n "${{ secrets.DB_ROOT_PASSWORD }}" | base64 -w 0)
          REDIS_PASSWORD_B64=$(echo -n "${{ secrets.REDIS_PASSWORD }}" | base64 -w 0)
          POLYGON_API_KEY_B64=$(echo -n "${{ secrets.POLYGON_API_KEY }}" | base64 -w 0)
          GEMINI_FREE_KEYS_B64=$(echo -n "${{ secrets.GEMINI_FREE_KEYS }}" | base64 -w 0)
          GOOGLE_CLIENT_ID_B64=$(echo -n "${{ secrets.GOOGLE_CLIENT_ID }}" | base64 -w 0)
          GOOGLE_CLIENT_SECRET_B64=$(echo -n "${{ secrets.GOOGLE_CLIENT_SECRET }}" | base64 -w 0)
          JWT_SECRET_B64=$(echo -n "${{ secrets.JWT_SECRET }}" | base64 -w 0)
          
          # Create secrets YAML
          cat > "$SECRETS_FILE" << EOF
          apiVersion: v1
          kind: Secret
          metadata:
            name: redis-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            REDIS_PASSWORD: ${REDIS_PASSWORD_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: db-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            DB_ROOT_PASSWORD: ${DB_PASSWORD_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: polygon-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            api-key: ${POLYGON_API_KEY_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: gemini-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            GEMINI_FREE_KEYS: ${GEMINI_FREE_KEYS_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: google-oauth-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID_B64}
            GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET_B64}
          ---
          apiVersion: v1
          kind: Secret
          metadata:
            name: jwt-secret
            namespace: $K8S_NAMESPACE
          type: Opaque
          data:
            JWT_SECRET: ${JWT_SECRET_B64}
          EOF
          
          # Apply secrets to Kubernetes
          echo "Applying secrets to Kubernetes..."
          kubectl_with_retry "kubectl apply -f \"$SECRETS_FILE\"" || kubectl_with_retry "kubectl apply -f \"$SECRETS_FILE\" --validate=false"
          
          # Clean up
          rm -rf "$TEMP_DIR"
          
          # Restart pods to apply the new secrets
          kubectl rollout restart deployment/backend deployment/worker

      # Run database migrations
      - name: Deploy Database Migrations
        run: |
          # Log function for better visibility
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          # Helper function for retrying commands
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  log "Command failed, retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            log "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Verify cluster connectivity with improved error handling
          log "Checking Kubernetes cluster connectivity..."
          if ! kubectl cluster-info; then
            log "ERROR: Cannot connect to Kubernetes cluster."
            log "Current kubectl context: $(kubectl config current-context || echo 'NONE')"
            exit 1
          fi
          
          # Verify correct context
          CURRENT_CONTEXT=$(kubectl config current-context)
          log "Current Kubernetes context: $CURRENT_CONTEXT"
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            log "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          # Check cluster node status before proceeding
          log "Checking cluster node status..."
          NODE_STATUS=$(kubectl get nodes -o wide)
          log "Node status: $NODE_STATUS"
          
          # Check for any cluster-wide issues
          log "Checking for cluster-wide issues..."
          CLUSTER_EVENTS=$(kubectl get events --sort-by='.lastTimestamp' | grep -i "error\|warn\|fail" || echo "No recent error events")
          if [ "$CLUSTER_EVENTS" != "No recent error events" ]; then
            log "Recent cluster error events found:"
            echo "$CLUSTER_EVENTS"
          else
            log "No recent cluster error events found"
          fi
          
          # Check if namespace exists
          log "Checking if namespace $K8S_NAMESPACE exists..."
          if ! kubectl get namespace "$K8S_NAMESPACE" &>/dev/null; then
            log "ERROR: Namespace $K8S_NAMESPACE does not exist."
            log "Available namespaces:"
            kubectl get namespaces
            exit 1
          fi
          
          # Get all pods to help diagnose issues
          log "Listing all pods in namespace $K8S_NAMESPACE:"
          kubectl get pods -n "$K8S_NAMESPACE"
          
          # Add check to ensure there's enough capacity
          log "Checking node capacity..."
          NODE_PRESSURE=$(kubectl describe nodes | grep -A5 "Conditions:" | grep -i "pressure\|true" || echo "No pressure conditions found")
          if [ "$NODE_PRESSURE" != "No pressure conditions found" ]; then
            log "WARNING: Node pressure conditions detected:"
            echo "$NODE_PRESSURE"
            log "This may affect pod scheduling"
          fi
          
          # Save current replica counts with better error handling
          log "Saving current deployment replica counts..."
          for deployment in worker backend tf; do
            log "Checking $deployment deployment..."
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              replicas=$(kubectl get deployment $deployment -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "1")
              echo "${deployment}_REPLICAS=$replicas" >> $GITHUB_ENV
              log "$deployment has $replicas replicas"
            else
              log "Note: $deployment deployment not found, will use default of 1 replica"
              echo "${deployment}_REPLICAS=1" >> $GITHUB_ENV
            fi
          done
          
          # Check if deployments exist before scaling
          log "Checking deployments before scaling down..."
          DEPLOYMENTS_TO_SCALE=""
          for deployment in worker backend tf; do
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              DEPLOYMENTS_TO_SCALE="$DEPLOYMENTS_TO_SCALE $deployment"
            else
              log "Note: $deployment deployment not found - skipping scale down"
            fi
          done
          
          # Scale down services if they exist - use faster approach
          if [ -n "$DEPLOYMENTS_TO_SCALE" ]; then
            log "Entering maintenance mode: Scaling down services that access the database..."
            kubectl scale deployment $DEPLOYMENTS_TO_SCALE --replicas=0 || {
              log "WARNING: Failed to scale down some deployments, proceeding anyway..."
            }
            
            # Wait for pods to terminate with status check - use shorter timeout
            log "Waiting for pods to terminate..."
            
            # Check pod termination with timeout
            end_time=$(($(date +%s) + 60))  # 60-second timeout
            while [ $(date +%s) -lt $end_time ]; do
              POD_COUNT=$(kubectl get pods -l "app in (worker,backend,tf)" --field-selector=status.phase=Running -n "$K8S_NAMESPACE" 2>/dev/null | wc -l)
              if [ "$POD_COUNT" -le 1 ]; then  # Account for header row
                log "All services pods terminated"
                break
              fi
              log "Waiting for pods to terminate... ($POD_COUNT remaining)"
              sleep 5
            done
            
            # Final check
            log "Checking if pods are terminated:"
            kubectl get pods -l "app in (worker,backend,tf)" -n "$K8S_NAMESPACE" || log "No matching pods found"
          else
            log "No deployments found to scale down, skipping this step"
          fi
          
          # Check if database pod exists with improved error handling
          log "Looking for database pod..."
          
          # Use set +e to prevent command from failing the script
          set +e
          DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
          DB_POD_STATUS=$?
          set -e
          
          if [ $DB_POD_STATUS -ne 0 ] || [ -z "$DB_POD" ]; then
            log "Database pod not found, checking if deployment exists..."
            
            set +e
            DB_DEPLOYMENT=$(kubectl get deployment db -n "$K8S_NAMESPACE" 2>/dev/null)
            DB_DEPLOYMENT_STATUS=$?
            set -e
            
            if [ $DB_DEPLOYMENT_STATUS -eq 0 ]; then
              log "DB deployment exists but no pods found. Checking events:"
              kubectl get events -n "$K8S_NAMESPACE" | grep db
              
              log "Attempting to recreate the DB pod by restarting the deployment..."
              kubectl rollout restart deployment/db -n "$K8S_NAMESPACE" || {
                log "WARNING: Failed to restart DB deployment"
              }
              
              # Wait for DB pod to appear with a 60-second timeout
              log "Waiting for DB pod to appear (60-second timeout)..."
              end_time=$(($(date +%s) + 60))
              while [ $(date +%s) -lt $end_time ]; do
                set +e
                NEW_DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
                NEW_DB_POD_STATUS=$?
                set -e
                
                if [ $NEW_DB_POD_STATUS -eq 0 ] && [ -n "$NEW_DB_POD" ]; then
                  log "DB pod appeared: $NEW_DB_POD"
                  DB_POD=$NEW_DB_POD
                  break
                fi
                log "Still waiting for DB pod to appear..."
                sleep 5
              done
              
              if [ -z "$DB_POD" ]; then
                log "WARNING: Failed to find DB pod after 60 seconds"
                log "Will proceed with K8s deployment step which should create necessary resources"
                
                # Force clear any stuck resources
                log "Checking for stuck resources..."
                STUCK_DB_PODS=$(kubectl get pods -l app=db -n "$K8S_NAMESPACE" 2>/dev/null || echo "None")
                if [ "$STUCK_DB_PODS" != "None" ]; then
                  log "Found potentially stuck DB pods, attempting to force delete:"
                  kubectl delete pods -l app=db -n "$K8S_NAMESPACE" --grace-period=0 --force || log "Could not force delete pods"
                fi
                
                # Create deployment by applying config
                log "Ensuring DB deployment exists by applying config..."
                find config/prod/config -type f -name "*.yaml" | xargs grep -l "app: db" | while read dbfile; do
                  log "Applying DB file: $dbfile"
                  kubectl apply -f "$dbfile" || log "Warning: Failed to apply $dbfile"
                done
                
                # Continue to the K8s deployment step
                echo "Database migration step skipped - will be handled in deployment"
                exit 0
              fi
            else
              log "DB deployment doesn't exist yet. This is likely the first deployment."
              log "Will proceed to K8s deployment step which should create the database resources"
              echo "Database migration step skipped - will be handled in deployment"
              exit 0
            fi
          fi
          
          log "Found database pod: $DB_POD"
          
          # Check DB pod status before proceeding
          log "Checking database pod status..."
          DB_POD_STATUS=$(kubectl get pod $DB_POD -n "$K8S_NAMESPACE" -o jsonpath='{.status.phase}')
          log "Database pod status: $DB_POD_STATUS"
          
          if [ "$DB_POD_STATUS" != "Running" ]; then
            log "Database pod is not running (status: $DB_POD_STATUS)"
            log "Pod details:"
            kubectl describe pod $DB_POD -n "$K8S_NAMESPACE"
            log "Attempting to fix by restarting the pod..."
          fi
          
          # Restart the database pod to trigger migrations (with retry)
          MAX_RESTART_ATTEMPTS=3
          for attempt in $(seq 1 $MAX_RESTART_ATTEMPTS); do
            log "Restarting database pod to apply migrations (attempt $attempt of $MAX_RESTART_ATTEMPTS)..."
            
            set +e
            kubectl delete pod $DB_POD -n "$K8S_NAMESPACE"
            DELETE_STATUS=$?
            set -e
            
            if [ $DELETE_STATUS -eq 0 ]; then
              log "Successfully deleted database pod $DB_POD"
              break
            elif [ $attempt -eq $MAX_RESTART_ATTEMPTS ]; then
              log "ERROR: Failed to delete database pod $DB_POD after $MAX_RESTART_ATTEMPTS attempts"
              kubectl describe pod $DB_POD -n "$K8S_NAMESPACE"
              log "Will try to force delete the pod..."
              kubectl delete pod $DB_POD -n "$K8S_NAMESPACE" --grace-period=0 --force || {
                log "Failed to force delete the pod. Will proceed with deployment anyway."
                echo "Database migration step completed with warnings"
                exit 0
              }
            else
              log "Failed to delete pod, retrying in 5 seconds..."
              sleep 5
            fi
          done
          
          # Wait for new database pod with improved diagnostics and shorter timeout
          log "Waiting for new database pod to be created (60-second timeout)..."
          end_time=$(($(date +%s) + 60))
          found_new_pod=false
          
          while [ $(date +%s) -lt $end_time ] && [ "$found_new_pod" = false ]; do
            set +e
            NEW_DB_POD=$(kubectl get pods -l app=db -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
            NEW_DB_POD_STATUS=$?
            set -e
            
            if [ $NEW_DB_POD_STATUS -eq 0 ] && [ -n "$NEW_DB_POD" ] && [ "$NEW_DB_POD" != "$DB_POD" ]; then
              log "New database pod created: $NEW_DB_POD"
              found_new_pod=true
            elif [ $NEW_DB_POD_STATUS -eq 0 ] && [ -n "$NEW_DB_POD" ] && [ "$NEW_DB_POD" = "$DB_POD" ]; then
              log "Old pod still exists, waiting..."
            else
              log "No database pod found, waiting..."
            fi
            
            if [ "$found_new_pod" = false ]; then
              sleep 5
            fi
          done
          
          if [ "$found_new_pod" = false ]; then
            log "WARNING: Failed to find new database pod after 60 seconds"
            log "Current pods:"
            kubectl get pods -n "$K8S_NAMESPACE"
            log "Will proceed with deployment anyway - database may need to be recreated"
            echo "Database migration step completed with warnings"
            exit 0
          fi
          
          # Wait for the pod to be ready with better diagnostics
          log "Waiting for database pod to become ready..."
          kubectl describe pod/$NEW_DB_POD -n "$K8S_NAMESPACE"
          
          set +e
          kubectl wait --for=condition=ready pod/$NEW_DB_POD --timeout=180s -n "$K8S_NAMESPACE"
          POD_READY_STATUS=$?
          set -e
          
          if [ $POD_READY_STATUS -ne 0 ]; then
            log "WARNING: Database pod did not become ready in time"
            log "Pod events:"
            kubectl describe pod/$NEW_DB_POD -n "$K8S_NAMESPACE"
            log "Pod logs:"
            kubectl logs $NEW_DB_POD --tail=50 -n "$K8S_NAMESPACE" || log "Could not retrieve logs"
            
            # Check for common issues
            EVENTS=$(kubectl describe pod/$NEW_DB_POD -n "$K8S_NAMESPACE" | grep -A10 "Events:")
            if [[ "$EVENTS" == *"unbound immediate PersistentVolumeClaims"* ]]; then
              log "PVC binding issue detected. Attempting to fix..."
              
              # Re-apply PV and PVC
              log "Re-applying PersistentVolume and PersistentVolumeClaim..."
              find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolume" | while read pvfile; do
                log "Re-applying PV file: $pvfile"
                kubectl apply -f "$pvfile" || log "Warning: Failed to apply $pvfile"
              done
              
              sleep 5
              
              find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolumeClaim" | while read pvcfile; do
                log "Re-applying PVC file: $pvcfile"
                kubectl apply -f "$pvcfile" || log "Warning: Failed to apply $pvcfile"
              done
              
              log "Deleting problematic database pod to trigger recreation..."
              kubectl delete pod $NEW_DB_POD -n "$K8S_NAMESPACE" --grace-period=0 --force || log "Warning: Failed to delete pod"
            fi
            
            log "Will continue with deployment anyway as the database might need more time"
            echo "Database migration step completed with warnings"
            exit 0
          fi
          
          # Check migration logs with better diagnostics
          log "Migration logs:"
          set +e
          kubectl logs $NEW_DB_POD -n "$K8S_NAMESPACE" | grep -A 5 "MIGRATION" 
          MIGRATION_LOG_STATUS=$?
          set -e
          
          if [ $MIGRATION_LOG_STATUS -ne 0 ]; then
            log "Migration log entries not found"
            log "Full database pod logs:"
            kubectl logs $NEW_DB_POD --tail=50 -n "$K8S_NAMESPACE" || log "Could not retrieve logs"
          fi
          
          # Scale services back up
          log "Exiting maintenance mode: Scaling services back up..."
          set +e
          
          for deployment in worker backend tf; do
            if kubectl get deployment $deployment -n "$K8S_NAMESPACE" &>/dev/null; then
              replicas_var="${deployment}_REPLICAS"
              replicas=${!replicas_var}
              
              log "Scaling $deployment back to $replicas replicas..."
              kubectl scale deployment $deployment --replicas=$replicas -n "$K8S_NAMESPACE" || {
                log "WARNING: Failed to scale $deployment back up to $replicas replicas"
                # Try again with at least 1 replica
                kubectl scale deployment $deployment --replicas=1 -n "$K8S_NAMESPACE" || {
                  log "WARNING: Failed to scale $deployment to even 1 replica"
                }
              }
            fi
          done
          set -e
          
          log "Database migration step completed"

      # Deploy to Kubernetes
      - name: Deploy to Kubernetes
        run: |
          # Helper functions
          log() {
            echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
          }
          
          kubectl_with_retry() {
            cmd=$1
            max_retries=${2:-3}
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              if eval "$cmd"; then
                return 0
              else
                retry_count=$((retry_count + 1))
                if [ $retry_count -lt $max_retries ]; then
                  log "Retrying in 5 seconds... (Attempt $retry_count of $max_retries)"
                  sleep 5
                fi
              fi
            done
            
            log "ERROR: Command failed after $max_retries attempts: $cmd"
            return 1
          }
          
          # Verify cluster connectivity and context
          kubectl_with_retry "kubectl cluster-info" || exit 1
          
          # Verify correct context
          CURRENT_CONTEXT=$(kubectl config current-context)
          if [ "$CURRENT_CONTEXT" != "$K8S_CONTEXT" ]; then
            log "ERROR: Wrong Kubernetes context! Expected $K8S_CONTEXT but got $CURRENT_CONTEXT"
            exit 1
          fi
          
          # Clear any conflicting configurations first 
          log "Checking for conflicting deployments..."
          CONFLICTING_DEPLOYMENTS=$(kubectl get deployments -o jsonpath='{.items[?(@.metadata.annotations.deployment\.kubernetes\.io/revision)].metadata.name}')
          if [ -n "$CONFLICTING_DEPLOYMENTS" ]; then
            log "Found potential conflicting deployments, resolving conflicts..."
            for deployment in $CONFLICTING_DEPLOYMENTS; do
              kubectl rollout restart deployment/$deployment 2>/dev/null || true
            done
            sleep 5
          fi
          
          # Apply Kubernetes configurations efficiently
          log "Applying Kubernetes configurations..."
          
          # First, apply infrastructure components that others depend on (PVs, PVCs, ConfigMaps, etc.)
          log "Applying persistent volumes and claims first..."
          find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolume" | while read pvfile; do
            kubectl apply -f "$pvfile" --validate=false || log "Warning: Failed to apply $pvfile"
          done
          
          sleep 3
          
          find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: PersistentVolumeClaim" | while read pvcfile; do
            kubectl apply -f "$pvcfile" --validate=false || log "Warning: Failed to apply $pvcfile"
          done
          
          log "Applying ConfigMaps and Secrets..."
          find config/prod/config -type f -name "*.yaml" | xargs grep -l "kind: ConfigMap" | while read cmfile; do
            kubectl apply -f "$cmfile" --validate=false || log "Warning: Failed to apply $cmfile"
          done
          
          # Apply Services and other non-Deployment resources
          log "Applying Services and other resources..."
          find config/prod/config -type f -name "*.yaml" | grep -v "deployment" | xargs grep -l "kind: Service" | while read svcfile; do
            kubectl apply -f "$svcfile" --validate=false || log "Warning: Failed to apply $svcfile"
          done
          
          # Apply the rest of the configurations excluding secrets.yaml
          log "Applying remaining Kubernetes configurations..."
          find config/prod/config -type f -name "*.yaml" ! -name "secrets.yaml" | while read config_file; do
            if ! grep -q "kind: PersistentVolume\|kind: PersistentVolumeClaim\|kind: ConfigMap\|kind: Service" "$config_file"; then
              kubectl apply -f "$config_file" --validate=false || log "Warning: Failed to apply $config_file"
            fi
          done
          
          # Define the deployments to update in order of dependency
          log "Preparing deployment updates..."
          infrastructure=("db" "cache")
          deployments=("tf" "backend" "worker" "frontend" "nginx")
          
          # Image tag to use
          log "Using image tag: ${{ env.DOCKER_TAG }}"
          
          # First update infrastructure components
          log "Updating infrastructure components..."
          for deployment in "${infrastructure[@]}"; do
            log "Setting $deployment image to $DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }}"
            kubectl set image deployment/$deployment $deployment=$DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }} &
          done
          wait
          
          # Update image tags for all deployments in parallel
          log "Updating application deployments in parallel..."
          for deployment in "${deployments[@]}"; do
            {
              log "Setting $deployment image to $DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }}"
              kubectl set image deployment/$deployment $deployment=$DOCKER_USERNAME/$deployment:${{ env.DOCKER_TAG }} 
              
              # Special handling for worker deployment healthcheck container
              if [ "$deployment" = "worker" ]; then
                CONTAINERS=$(kubectl get deployment/worker -o jsonpath='{.spec.template.spec.containers[*].name}' 2>/dev/null)
                if [[ $CONTAINERS == *"db-healthcheck"* ]]; then
                  kubectl set image deployment/worker db-healthcheck=$DOCKER_USERNAME/worker-healthcheck:${{ env.DOCKER_TAG }}
                elif [[ $CONTAINERS == *"worker-healthcheck"* ]]; then
                  kubectl set image deployment/worker worker-healthcheck=$DOCKER_USERNAME/worker-healthcheck:${{ env.DOCKER_TAG }}
                fi
              fi
            } &
          done
          wait
          
          # Apply rollout restart to all deployments simultaneously (much faster)
          log "Restarting all deployments simultaneously for faster rollout..."
          
          # Restart infrastructure first
          for deployment in "${infrastructure[@]}"; do
            kubectl rollout restart deployment/$deployment &
          done
          wait
          
          # Brief wait for infrastructure to start restarting
          sleep 10
          
          # Restart all deployments in parallel
          for deployment in "${deployments[@]}"; do
            kubectl rollout restart deployment/$deployment &
          done
          wait
          
          # Wait for image pull
          log "Waiting for image pulls to complete (30 seconds)..."
          sleep 30
          
          # Monitor deployments with shorter timeouts
          log "Monitoring deployments for successful rollout..."
          
          # Set timeout values for each deployment (in seconds)
          declare -A timeouts
          timeouts=([backend]=180 [frontend]=120 [worker]=180 [tf]=120 [nginx]=120 [db]=180 [cache]=120)
          
          # Monitor infrastructure components first
          log "Monitoring infrastructure deployments..."
          for deployment in "${infrastructure[@]}"; do
            log "Waiting for $deployment deployment..."
            timeout=${timeouts[$deployment]:-120}
            
            kubectl rollout status deployment/$deployment --timeout=${timeout}s &
          done
          wait
          
          # Monitor application deployments in parallel with improved error handling
          log "Monitoring application deployments..."
          
          # Launch a background process to monitor each deployment
          pids=()
          for deployment in "${deployments[@]}"; do
            {
              log "Checking $deployment deployment..."
              timeout=${timeouts[$deployment]:-180}
              
              # Check if deployment exists and has the right image
              DEPLOY_IMAGE=$(kubectl get deployment/$deployment -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null)
              if [[ "$DEPLOY_IMAGE" != *"${{ env.DOCKER_TAG }}"* ]]; then
                log "WARNING: $deployment deployment image doesn't match ${{ env.DOCKER_TAG }}"
                log "Current image: $DEPLOY_IMAGE"
              fi
              
              # Wait for rollout to complete
              if ! kubectl rollout status deployment/$deployment --timeout=${timeout}s; then
                log "ERROR: $deployment deployment failed or timed out after ${timeout}s."
                log "Deployment details:"
                kubectl describe deployment $deployment
                log "Pod status:"
                kubectl get pods -l app=$deployment
                log "Pod logs:"
                kubectl logs -l app=$deployment --tail=20 || echo "Could not retrieve logs"
                
                # Try to diagnose common issues
                EVENTS=$(kubectl get events --sort-by='.lastTimestamp' | grep $deployment | tail -5)
                log "Recent events for $deployment: $EVENTS"
                
                # Create a failure file so we can check for failures after all monitors complete
                echo "$deployment" > "/tmp/deployment_failures_$deployment"
              else
                log "$deployment deployment succeeded."
              fi
            } &
            pids+=($!)
          done
          
          # Wait for all monitors to complete
          for pid in "${pids[@]}"; do
            wait $pid
          done
          
          # Check for any deployment failures
          FAILURE_FILES=$(ls /tmp/deployment_failures_* 2>/dev/null || echo "")
          if [ -n "$FAILURE_FILES" ]; then
            log "Some deployments failed. See details above."
            
            # Try to salvage the deployment by retrying failed deployments one more time
            log "Attempting to salvage deployment by retrying failed deployments..."
            for file in $FAILURE_FILES; do
              deployment=$(cat "$file")
              log "Retrying $deployment deployment..."
              kubectl rollout restart deployment/$deployment
              rm "$file"
            done
            
            # Wait a bit for retries to start
            sleep 10
            
            # Don't fail the workflow, but warn that there were issues
            log "WARNING: There were deployment issues that were attempted to be resolved."
            log "Check the application status manually to ensure it's functioning correctly."
          else
            log "All deployments completed successfully!"
          fi
          
          # Final status check
          log "Final deployment status:"
          kubectl get deployments -o wide
          kubectl get pods
          
          log "Deployment process completed for branch ${{ env.TARGET_BRANCH }}."
      
      # Cleanup after deployment
      - name: Cleanup
        if: always()  # Run cleanup even if previous steps failed
        run: |
          echo "Cleaning up resources after deployment..."
          
          # Clean up Docker images to free space
          echo "Cleaning up unused Docker images..."
          # Remove images older than 24 hours that aren't tagged as latest, development, or stage
          docker image prune -af --filter "until=24h" --filter "label!=stage" --filter "label!=latest" --filter "label!=development"
          
          # Clean up system resources
          echo "Cleaning up system resources..."
          docker system prune -f --volumes
          
          echo "Cleanup completed." 