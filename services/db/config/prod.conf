#prod.conf
#assume 8 cpus, 32gb ram, 160gb disk, as specified in production deployment

# TimescaleDB Core Configuration
shared_preload_libraries = 'timescaledb,pg_stat_statements'
timescaledb.max_background_workers = 8  # FIXED: Increased to handle all background jobs
timescaledb.telemetry_level = 'off'

# CRITICAL: Network Configuration for Docker
listen_addresses = '*'                  # Listen on all interfaces for container networking

# CRITICAL: Data Integrity & Corruption Prevention
full_page_writes = on                  # Ensure WAL contains complete pages after checkpoints
fsync = on                              # Ensure data integrity
synchronous_commit = on                 # Wait for WAL writes

# MEMORY SETTINGS - OPTIMIZED FOR 32GB RAM
shared_buffers = 8GB                    # FIXED: 25% of RAM - reasonable
work_mem = 512MB                        # INCREASED: Better for index creation and parallel operations on large hypertables
maintenance_work_mem = 2GB              # INCREASED: Critical for large index creation on hypertables with many chunks
effective_cache_size = 24GB             # FIXED: Increased from 8GB to 75% of RAM
wal_buffers = 256MB                     # FIXED: Single setting, optimized for bulk operations

# CONNECTIONS & PARALLELISM - OPTIMIZED FOR 8 vCPUs
max_connections = 100                   # FIXED: Reduced from 200 to prevent memory pressure
max_worker_processes = 32               # FIXED: 4x CPU count for 8 cores
max_parallel_workers = 8                # FIXED: Reduced from 12 to keep one core free
max_parallel_workers_per_gather = 6     # INCREASED: Better for index creation on large hypertables
max_parallel_maintenance_workers = 6    # INCREASED: Better for index creation on large hypertables
max_locks_per_transaction = 512        # Default, but explicit

# WAL & CHECKPOINT SETTINGS - OPTIMIZED FOR 160GB DISK
max_wal_size = 16GB                     # FIXED: Increased from 3GB for sustained batch loads
min_wal_size = 2GB                      # Scaled from dev (1GB)
checkpoint_timeout = '10min'            # Reduce from 15min
checkpoint_completion_target = 0.9      # Keep existing (single instance)
wal_keep_size = '512MB'                 # FIXED: Replaces deprecated wal_keep_segments
wal_level = replica                     # Keep existing
archive_mode = on                       # Keep existing
archive_command = 'test ! -f /home/postgres/pgdata/wal_archive/%f && cp %p /home/postgres/pgdata/wal_archive/%f'  # UPDATED: moved archive inside PGDATA

# LOGGING ESSENTIALS - PRODUCTION OPTIMIZED
# DISABLED logging_collector to ensure logs go to stdout/stderr for kubectl/docker logs
logging_collector = off                 # Logs go to stdout/stderr for container logging
log_line_prefix = '%m [%p] %u@%d %i %e '  # FIXED: Improved prefix format
log_min_duration_statement = 1000       # Log queries > 1 second
log_statement = 'none'                  # Disable statement-level logging (quiet noisy console output)
log_checkpoints = on                    # Log checkpoint activity
log_lock_waits = on                     # Log lock waits for deadlock detection
log_temp_files = 0                      # Log ALL temp file usage
log_error_verbosity = verbose           # FIXED: More verbose for better debugging
log_min_messages = warning              # FIXED: Log warnings and above
track_io_timing = on                    # Measure I/O timing
track_functions = all                   # FIXED: Added for PL/pgSQL profiling
track_activity_query_size = 4096        # FIXED: Added for full query capture

# AUTO_EXPLAIN - REMOVED (extension not available in TimescaleDB image)

# PG_STAT_STATEMENTS CONFIGURATION
pg_stat_statements.track = all
pg_stat_statements.track_utility = on
pg_stat_statements.max = 10000

# TIMESCALEDB DEBUGGING
timescaledb.bgw_log_level = 'DEBUG1'    # Verbose background worker logs

# TIMESCALEDB PERFORMANCE OPTIMIZATIONS
timescaledb.enable_chunkwise_aggregation = on
timescaledb.vectorized_aggregation = on  # Keep if TimescaleDB >= 2.13
timescaledb.enable_merge_on_cagg_refresh = on
timescaledb.max_open_chunks_per_insert = 512  # Adjusted compromise to avoid cache mismatch
timescaledb.max_cached_chunks_per_hypertable = 2048  # INCREASED: Better for large hypertables with many chunks
timescaledb.enable_optimizations = on
timescaledb.enable_constraint_exclusion = on
timescaledb.enable_chunk_append = on
timescaledb.enable_ordered_append = on
timescaledb.enable_parallel_chunk_append = on
timescaledb.enable_runtime_exclusion = on
timescaledb.enable_transparent_decompression = on
timescaledb.max_tuples_decompressed_per_dml_transaction = 2000000  # INCREASED: Better for large operations
timescaledb.max_insert_batch_size = 15000  # INCREASED: Better for bulk operations

# CRITICAL: Connection & Query Limits
idle_in_transaction_session_timeout = '10min'
lock_timeout = '5min'
deadlock_timeout = '1s'                 # Single instance

# CRITICAL: Resource Protection
temp_file_limit = '4GB'
max_stack_depth = '4MB'                 # FIXED: Increased from 2MB for deep recursions

# AUTOVACUUM OPTIMIZATION FOR TIMESERIES
autovacuum = on
autovacuum_max_workers = 4              # FIXED: Appropriate for 8 cores
autovacuum_naptime = '2min'             # Keep existing
autovacuum_vacuum_threshold = 1000      # Keep existing
autovacuum_analyze_threshold = 500      # Keep existing
autovacuum_vacuum_scale_factor = 0.1
autovacuum_analyze_scale_factor = 0.05
autovacuum_vacuum_cost_limit = 2000     # FIXED: Appropriate for production
autovacuum_vacuum_cost_delay = 2ms      # FIXED: Appropriate for production

# TIMESCALEDB QUERY OPTIMIZATION
enable_partitionwise_aggregate = on
jit = off                              # DISABLED - can cause crashes on complex queries

# ADVANCED OPTIMIZATIONS
vacuum_cost_delay = 5ms                # Scaled from dev (10ms)
vacuum_cost_limit = 2000               # Scaled from dev (1000)
bgwriter_delay = 200ms                 # Keep existing
bgwriter_lru_maxpages = 500            # Scaled from dev (250)
bgwriter_flush_after = 256kB           # Keep existing
default_statistics_target = 100        # Keep existing

# CRITICAL: Prevent OOM conditions
huge_pages = off                       # Disable huge pages which can cause issues
shared_memory_type = mmap              # Use mmap for better memory management

# CRITICAL: Query Planning Limits
from_collapse_limit = 8                # Default, but explicit
join_collapse_limit = 8                # Default, but explicit
max_pred_locks_per_transaction = 64    # Default, but explicit
max_pred_locks_per_relation = 64       # Default, but explicit

# WAL OPTIMIZATION
wal_compression = on                   # Compress WAL for better I/O
wal_init_zero = off                    # Don't zero-fill WAL files for performance
wal_recycle = on                       # Recycle WAL files
checkpoint_flush_after = 256kB         # Flush more frequently during checkpoints

# CONNECTION POOLING OPTIMIZATION
tcp_keepalives_idle = 600              # 10 minutes
tcp_keepalives_interval = 30           # 30 seconds
tcp_keepalives_count = 3               # 3 attempts

# I/O OPTIMIZATION
effective_io_concurrency = 400         # INCREASED: Better for production workloads and index creation
maintenance_io_concurrency = 400       # INCREASED: Better for production workloads and index creation

# TIMESCALEDB OPTIMIZATIONS
random_page_cost = 1.1

# REMOVED INVALID/REDUNDANT PARAMETERS:
# - log_autovacuum (invalid GUC)
# - log_wal_info (invalid GUC)
# - Duplicate wal_buffers, checkpoint_completion_target, deadlock_timeout
# - log_duration, log_executor_stats, log_planner_stats, log_parser_stats (high overhead)
# - commit_delay, commit_siblings (negligible benefit on SSD)
