// Package marketdata provides OHLCV data processing functionality.
//
// IMPORTANT: This updater requires a maintenance window and cannot be run during market hours
// due to the following operations that take exclusive locks and will block any write operations:
//   - Index creation (CREATE INDEX without CONCURRENTLY for TimescaleDB compatibility)
//   - Bulk data loading with TimescaleDB restoring mode
//   - Compression policy management
//
// Recommended schedule: Run daily at 08:30 PT (11:30 ET) after market close
// and before the next trading session begins.
//
// Configuration Environment Variables:
//   - OHLCV_DB_QUERY_TIMEOUT: Timeout for database queries (default: 5m)
//   - OHLCV_FILE_PROCESS_TIMEOUT: Timeout for processing individual files (default: 10m)
//   - OHLCV_SETUP_TIMEOUT: Timeout for database setup operations (default: 2m)
//   - OHLCV_CLEANUP_TIMEOUT: Timeout for database cleanup operations (default: 5m)
//   - MAX_DB_WORKERS: Number of concurrent database workers (default: 4)
//   - S3_RATE_LIMIT: S3 API rate limit in requests per second (default: 1000)
//   - S3_BURST_LIMIT: S3 API burst limit (default: 1000)
package marketdata

import (
	"backend/internal/data"
	"backend/internal/data/postgres"
	"compress/gzip"
	"context"
	"crypto/tls"
	"encoding/csv"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"path/filepath"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/credentials"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/jackc/pgx/v4"
	"github.com/jackc/pgx/v4/pgxpool"
	"golang.org/x/time/rate"
)

// OHLCV table names as constants
const (
	TableOHLCV1M = "ohlcv_1m"
	//TableOHLCV1H = "ohlcv_1h"
	TableOHLCV1D = "ohlcv_1d"
	//TableOHLCV1W = "ohlcv_1w"
)

// OHLCV tables list for operations that need to process all tables
var OHLCVTables = []string{
	TableOHLCV1M,
	//TableOHLCV1H,
	TableOHLCV1D,
	//TableOHLCV1W,
}

// Configuration constants
const (
	DefaultMaxDBWorkers  = 4
	DefaultBatchSize     = 10000
	DefaultS3Bucket      = "flatfiles"
	DefaultLocalDataPath = "/tmp/polygon_data"
	DefaultS3RateLimit   = 1000 // requests per second
	DefaultS3BurstLimit  = 1000 // burst capacity

	// Timeout configurations for different operations
	DefaultDBQueryTimeout     = 5 * time.Minute  // Increased from 60s for large queries
	DefaultFileProcessTimeout = 10 * time.Minute // Timeout for processing individual files
	DefaultSetupTimeout       = 2 * time.Minute  // Timeout for DB setup operations
	DefaultCleanupTimeout     = 5 * time.Minute  // Timeout for DB cleanup operations
)

// S3 path prefixes for different timeframes
const (
	S3PrefixMinute = "us_stocks_sip/minute_aggs_v1"
	S3PrefixHour   = "us_stocks_sip/hour_aggs_v1"
	S3PrefixDay    = "us_stocks_sip/day_aggs_v1"
	S3PrefixWeek   = "us_stocks_sip/week_aggs_v1"
)

// S3Config holds configuration for S3 operations
type S3Config struct {
	Endpoint      string
	Bucket        string
	Key           string
	Secret        string
	Region        string
	MaxDBWorkers  int
	BatchSize     int
	LocalDataPath string
	RateLimit     rate.Limit
	BurstLimit    int
}

// S3FlatFileClient handles S3 operations for Polygon flat files
type S3FlatFileClient struct {
	client             *s3.Client
	config             S3Config
	limiter            *rate.Limiter
	invalidTickerCache map[string]bool
	invalidTickerMu    *sync.RWMutex
}

// FileManifest tracks processed files for idempotency
type FileManifest struct {
	ProcessedFiles map[string]FileInfo `json:"processed_files"`
	LastUpdated    time.Time           `json:"last_updated"`
	mu             sync.RWMutex
}

// FileInfo tracks file processing metadata
type FileInfo struct {
	Key         string    `json:"key"`
	Size        int64     `json:"size"`
	ProcessedAt time.Time `json:"processed_at"`
	RecordCount int64     `json:"record_count"`
	Checksum    string    `json:"checksum,omitempty"`
}

// TimeframeSpec defines timeframe processing configuration
type TimeframeSpec struct {
	Name       string
	S3Prefix   string
	TableName  string
	Timespan   string
	Multiplier int
}

// CSVColumnIndices holds the column indices for CSV parsing
type CSVColumnIndices struct {
	Ticker    int
	Timestamp int
	Open      int
	High      int
	Low       int
	Close     int
	Volume    int
}

// Global configuration
var (
	supportedTimeframes = []TimeframeSpec{
		{"1-minute", S3PrefixMinute, TableOHLCV1M, "minute", 1},
		{"1-day", S3PrefixDay, TableOHLCV1D, "day", 1},
		// Hour and week timeframes can be enabled as needed
		// {"1-hour", S3PrefixHour, TableOHLCV1H, "hour", 1},
		// {"1-week", S3PrefixWeek, TableOHLCV1W, "week", 1},
	}
)

// DownloadedFile represents a downloaded file ready for processing
type DownloadedFile struct {
	Key      string
	Path     string
	File     *os.File
	FileSize int64
}

// CSVCopySource implements pgx.CopyFromSource for streaming CSV to PostgreSQL COPY
type CSVCopySource struct {
	reader             *csv.Reader
	columnIndices      CSVColumnIndices
	securityCache      map[string]int
	latestDates        map[string]time.Time
	tableName          string
	conn               *data.Conn
	invalidTickerCache map[string]bool
	invalidTickerMu    *sync.RWMutex
	recordCount        int64
	nextRow            []interface{}
	hasNext            bool
	err                error
}

// NewCSVCopySource creates a new CSV copy source for streaming
func NewCSVCopySource(reader *csv.Reader, indices CSVColumnIndices, conn *data.Conn, tableName string, securityCache map[string]int, latestDates map[string]time.Time, invalidTickerCache map[string]bool, invalidTickerMu *sync.RWMutex) *CSVCopySource {
	return &CSVCopySource{
		reader:             reader,
		columnIndices:      indices,
		conn:               conn,
		tableName:          tableName,
		securityCache:      securityCache,
		latestDates:        latestDates,
		invalidTickerCache: invalidTickerCache,
		invalidTickerMu:    invalidTickerMu,
		hasNext:            true,
	}
}

// Next implements pgx.CopyFromSource.Next()
func (cs *CSVCopySource) Next() bool {
	if !cs.hasNext || cs.err != nil {
		return false
	}

	// Read next CSV record
	record, err := cs.reader.Read()
	if err != nil {
		if err == io.EOF {
			cs.hasNext = false
			return false
		}
		cs.err = err
		return false
	}

	// Transform record to PostgreSQL COPY format
	cs.nextRow, err = cs.transformRecord(record)
	if err != nil {
		// Skip invalid records but continue processing
		return cs.Next() // Try next record
	}

	cs.recordCount++
	return true
}

// Values implements pgx.CopyFromSource.Values()
func (cs *CSVCopySource) Values() ([]interface{}, error) {
	if cs.err != nil {
		return nil, cs.err
	}
	return cs.nextRow, nil
}

// Err implements pgx.CopyFromSource.Err()
func (cs *CSVCopySource) Err() error {
	return cs.err
}

// GetRecordCount returns the number of records processed
func (cs *CSVCopySource) GetRecordCount() int64 {
	return cs.recordCount
}

// transformRecord transforms a CSV record to PostgreSQL COPY format
func (cs *CSVCopySource) transformRecord(record []string) ([]interface{}, error) {
	// Get ticker and check if we should skip this record
	ticker := record[cs.columnIndices.Ticker]

	// Get security ID
	securityID, err := cs.getSecurityID(ticker)
	if err != nil {
		return nil, fmt.Errorf("failed to get security ID for ticker %s: %w", ticker, err)
	}

	// Check if we should skip this record based on latest date
	if cs.shouldSkipRecord(ticker, record[cs.columnIndices.Timestamp], securityID) {
		return nil, nil // Skip this record
	}

	// Parse timestamp
	timestamp, err := parseTimestamp(record[cs.columnIndices.Timestamp])
	if err != nil {
		return nil, fmt.Errorf("failed to parse timestamp: %w", err)
	}

	// Parse OHLCV values
	open, err := parseFloat(record[cs.columnIndices.Open])
	if err != nil {
		return nil, fmt.Errorf("failed to parse open: %w", err)
	}
	high, err := parseFloat(record[cs.columnIndices.High])
	if err != nil {
		return nil, fmt.Errorf("failed to parse high: %w", err)
	}
	low, err := parseFloat(record[cs.columnIndices.Low])
	if err != nil {
		return nil, fmt.Errorf("failed to parse low: %w", err)
	}
	close, err := parseFloat(record[cs.columnIndices.Close])
	if err != nil {
		return nil, fmt.Errorf("failed to parse close: %w", err)
	}
	volume, err := parseFloat(record[cs.columnIndices.Volume])
	if err != nil {
		return nil, fmt.Errorf("failed to parse volume: %w", err)
	}

	// Return values in PostgreSQL COPY order: timestamp, securityid, open, high, low, close, volume
	return []interface{}{
		timestamp,
		securityID,
		open,
		high,
		low,
		close,
		volume,
	}, nil
}

// getSecurityID gets or caches security ID for a ticker
func (cs *CSVCopySource) getSecurityID(ticker string) (int, error) {
	if cachedID, ok := cs.securityCache[ticker]; ok {
		return cachedID, nil
	}

	// Check if this ticker is known to be invalid
	cs.invalidTickerMu.RLock()
	if cs.invalidTickerCache[ticker] {
		cs.invalidTickerMu.RUnlock()
		return 0, fmt.Errorf("ticker %s not found in securities table", ticker)
	}
	cs.invalidTickerMu.RUnlock()

	securityID, err := postgres.GetCurrentSecurityID(cs.conn, ticker)
	if err != nil {
		// Cache invalid tickers to avoid repeated DB lookups
		cs.invalidTickerMu.Lock()
		cs.invalidTickerCache[ticker] = true
		cs.invalidTickerMu.Unlock()
		return 0, err
	}

	cs.securityCache[ticker] = securityID
	return securityID, nil
}

// shouldSkipRecord checks if a record should be skipped based on latest date
func (cs *CSVCopySource) shouldSkipRecord(ticker, timestampStr string, securityID int) bool {
	// Get latest date for this ticker if not cached
	if _, ok := cs.latestDates[ticker]; !ok {
		latestDate, err := getLatestDateForTicker(cs.conn, securityID, cs.tableName)
		if err != nil || latestDate == nil {
			cs.latestDates[ticker] = time.Time{} // No existing data
			return false
		}
		cs.latestDates[ticker] = *latestDate
	}

	// Parse record timestamp
	recordTime, err := parseTimestamp(timestampStr)
	if err != nil {
		return false // Don't skip if we can't parse
	}

	// Skip if record is older than or equal to latest date
	return !recordTime.After(cs.latestDates[ticker])
}

// getS3Config loads S3 configuration from environment variables
func getS3Config() S3Config {
	config := S3Config{
		Endpoint:      getEnvWithDefault("POLYGON_S3_ENDPOINT", "https://files.polygon.io"),
		Bucket:        getEnvWithDefault("S3_BUCKET", DefaultS3Bucket),
		Key:           os.Getenv("POLYGON_S3_KEY"),    // Required
		Secret:        os.Getenv("POLYGON_S3_SECRET"), // Required
		Region:        getEnvWithDefault("AWS_REGION", "us-east-1"),
		MaxDBWorkers:  getIntEnvWithDefault("MAX_DB_WORKERS", DefaultMaxDBWorkers),
		BatchSize:     getIntEnvWithDefault("BATCH_SIZE", DefaultBatchSize),
		LocalDataPath: getEnvWithDefault("LOCAL_DATA_PATH", DefaultLocalDataPath),
		RateLimit:     rate.Limit(getIntEnvWithDefault("S3_RATE_LIMIT", DefaultS3RateLimit)),
		BurstLimit:    getIntEnvWithDefault("S3_BURST_LIMIT", DefaultS3BurstLimit),
	}

	// Validate required fields
	if config.Key == "" {
		log.Fatal("POLYGON_S3_KEY environment variable is required")
	}
	if config.Secret == "" {
		log.Fatal("POLYGON_S3_SECRET environment variable is required")
	}

	return config
}

// getEnvWithDefault returns environment variable value or default
func getEnvWithDefault(key, defaultValue string) string {
	if value := os.Getenv(key); value != "" {
		return value
	}
	return defaultValue
}

// getIntEnvWithDefault returns integer environment variable value or default
func getIntEnvWithDefault(key string, defaultValue int) int {
	if value := os.Getenv(key); value != "" {
		if parsed, err := strconv.Atoi(value); err == nil {
			return parsed
		}
	}
	return defaultValue
}

// getDurationEnvWithDefault returns duration environment variable value or default
func getDurationEnvWithDefault(key string, defaultValue time.Duration) time.Duration {
	if value := os.Getenv(key); value != "" {
		if parsed, err := time.ParseDuration(value); err == nil {
			return parsed
		}
	}
	return defaultValue
}

// getTimeoutConfig returns timeout configuration from environment variables
func getTimeoutConfig() struct {
	DBQuery     time.Duration
	FileProcess time.Duration
	Setup       time.Duration
	Cleanup     time.Duration
} {
	return struct {
		DBQuery     time.Duration
		FileProcess time.Duration
		Setup       time.Duration
		Cleanup     time.Duration
	}{
		DBQuery:     getDurationEnvWithDefault("OHLCV_DB_QUERY_TIMEOUT", DefaultDBQueryTimeout),
		FileProcess: getDurationEnvWithDefault("OHLCV_FILE_PROCESS_TIMEOUT", DefaultFileProcessTimeout),
		Setup:       getDurationEnvWithDefault("OHLCV_SETUP_TIMEOUT", DefaultSetupTimeout),
		Cleanup:     getDurationEnvWithDefault("OHLCV_CLEANUP_TIMEOUT", DefaultCleanupTimeout),
	}
}

// NewS3FlatFileClient creates a new S3 client for Polygon flat files
func NewS3FlatFileClient(s3Config S3Config) (*S3FlatFileClient, error) {
	// Create AWS config with custom endpoint and TLS settings
	cfg, err := config.LoadDefaultConfig(context.TODO(),
		config.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(
			s3Config.Key,
			s3Config.Secret,
			"",
		)),
		config.WithRegion(s3Config.Region),
		config.WithEndpointResolverWithOptions(
			aws.EndpointResolverWithOptionsFunc(func(service, region string, options ...interface{}) (aws.Endpoint, error) {
				return aws.Endpoint{
					URL:               s3Config.Endpoint,
					SigningRegion:     region,
					HostnameImmutable: true, // Prevent hostname verification issues
				}, nil
			}),
		),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to load AWS config: %w", err)
	}

	// Create S3 client with custom HTTP client that skips TLS verification
	httpClient := &http.Client{
		Transport: &http.Transport{
			TLSClientConfig: &tls.Config{
				InsecureSkipVerify: true, // Skip TLS certificate verification
			},
		},
		Timeout: 30 * time.Second,
	}

	s3Client := s3.NewFromConfig(cfg, func(o *s3.Options) {
		o.HTTPClient = httpClient
	})

	// Create rate limiter
	limiter := rate.NewLimiter(s3Config.RateLimit, s3Config.BurstLimit)

	return &S3FlatFileClient{
		client:             s3Client,
		config:             s3Config,
		limiter:            limiter,
		invalidTickerCache: make(map[string]bool),
		invalidTickerMu:    &sync.RWMutex{},
	}, nil
}

// UpdateAllOHLCV is the consolidated OHLCV updater that processes all active tickers
// across all timeframes using S3 flat files with optimized bulk loading
//
// WARNING: This function requires a maintenance window and cannot be run during market hours
// due to exclusive locks taken during index creation and bulk loading operations.
// Recommended: Run daily at 08:30 PT (11:30 ET) after market close.
func UpdateAllOHLCV(conn *data.Conn) error {
	log.Println("Starting consolidated OHLCV update using S3 flat files...")
	start := time.Now()

	defer func() {
		log.Printf("Consolidated OHLCV update completed in %v", time.Since(start))
	}()

	// Initialize S3 client
	s3Config := getS3Config()
	s3Client, err := NewS3FlatFileClient(s3Config)
	if err != nil {
		return fmt.Errorf("failed to create S3 client: %w", err)
	}

	// Get timeout configuration
	timeouts := getTimeoutConfig()
	log.Printf("Using timeout configuration - DB Query: %v, File Process: %v, Setup: %v, Cleanup: %v",
		timeouts.DBQuery, timeouts.FileProcess, timeouts.Setup, timeouts.Cleanup)

	// Ensure database is in a good state before starting
	setupCtx, setupCancel := context.WithTimeout(context.Background(), timeouts.Setup)
	defer setupCancel()

	if err := EnsureDBStateOnStartup(setupCtx, conn.DB); err != nil {
		return fmt.Errorf("error ensuring database state: %w", err)
	}

	// Ensure local data directory exists
	if err := os.MkdirAll(s3Config.LocalDataPath, 0755); err != nil {
		return fmt.Errorf("failed to create local data directory: %w", err)
	}

	// Load or create file manifest
	manifest, err := loadFileManifest(s3Config.LocalDataPath)
	if err != nil {
		return fmt.Errorf("failed to load file manifest: %w", err)
	}

	// ---- use a fresh context for the long-running processing phase ----
	processingCtx := context.Background()

	// Process each timeframe
	for _, timeframe := range supportedTimeframes {
		log.Printf("Processing timeframe: %s", timeframe.Name)

		if err := s3Client.processTimeframe(processingCtx, conn, timeframe, manifest); err != nil {
			log.Printf("Error processing timeframe %s: %v", timeframe.Name, err)
			// Continue with other timeframes
		}
	}

	// Save updated manifest
	if err := saveFileManifest(manifest, s3Config.LocalDataPath); err != nil {
		log.Printf("Warning: failed to save file manifest: %v", err)
	}

	return nil
}

// processTimeframe processes all files for a specific timeframe
func (s3c *S3FlatFileClient) processTimeframe(ctx context.Context, conn *data.Conn, timeframe TimeframeSpec, manifest *FileManifest) error {
	// Generate date range for the past year
	toDate := time.Now()
	fromDate := toDate.AddDate(-1, 0, 0) // 1 year ago

	// Pre-fetch all active tickers to seed security cache
	log.Printf("Pre-fetching active tickers for %s timeframe...", timeframe.Name)
	activeTickers, err := getActiveTickers(conn)
	if err != nil {
		return fmt.Errorf("failed to get active tickers: %w", err)
	}

	// Build security cache from active tickers
	securityCache := make(map[string]int)
	for _, ticker := range activeTickers {
		securityID, err := postgres.GetCurrentSecurityID(conn, ticker)
		if err != nil {
			log.Printf("Warning: failed to get security ID for active ticker %s: %v", ticker, err)
			continue
		}
		securityCache[ticker] = securityID
	}
	log.Printf("Pre-loaded %d active tickers into security cache", len(securityCache))

	// Pre-load latest dates for all active tickers to avoid blocking during streaming
	log.Printf("Pre-loading latest dates for %s timeframe...", timeframe.Name)
	latestDates, err := preloadLatestDates(conn, timeframe.TableName, securityCache)
	if err != nil {
		log.Printf("Warning: failed to preload latest dates: %v", err)
		latestDates = make(map[string]time.Time) // Fallback to empty map
	} else {
		log.Printf("Pre-loaded latest dates for %d tickers", len(latestDates))
	}

	// List files for the timeframe and date range
	fileKeys, err := s3c.listFiles(ctx, timeframe.S3Prefix, fromDate, toDate)
	if err != nil {
		return fmt.Errorf("failed to list files for %s: %w", timeframe.Name, err)
	}

	log.Printf("Found %d files for %s timeframe", len(fileKeys), timeframe.Name)

	// Filter out already processed files
	newFiles := s3c.filterNewFiles(fileKeys, manifest)
	if len(newFiles) == 0 {
		log.Printf("No new files to process for %s timeframe", timeframe.Name)
		return nil
	}

	log.Printf("Processing %d new files for %s timeframe", len(newFiles), timeframe.Name)

	// Get timeout configuration for this timeframe
	timeouts := getTimeoutConfig()

	// ------------------------------------------------------------
	// Perform one-time DB setup (disable compression, drop indexes)
	// ------------------------------------------------------------
	setupCtx, setupCancel := context.WithTimeout(context.Background(), timeouts.Setup)
	defer setupCancel()

	if err := PreLoadSetup(setupCtx, conn.DB, timeframe.TableName); err != nil {
		return fmt.Errorf("failed to prepare table %s for bulk loading: %w", timeframe.TableName, err)
	}

	// Ensure cleanup is ALWAYS executed, even on error / panic
	defer func() {
		cleanupCtx, cleanupCancel := context.WithTimeout(context.Background(), timeouts.Cleanup)
		defer cleanupCancel()

		if err := PostLoadCleanup(cleanupCtx, conn.DB, timeframe.TableName); err != nil {
			log.Printf("Error restoring table %s after bulk load: %v", timeframe.TableName, err)
		}
		// Run ANALYZE after cleanup to update statistics
		if err := s3c.analyzeTable(cleanupCtx, conn.DB, timeframe.TableName); err != nil {
			log.Printf("Warning: failed to analyze table %s: %v", timeframe.TableName, err)
		}
	}()

	// Process files with bounded concurrency
	return s3c.processFilesWithWorkers(ctx, conn, timeframe, newFiles, manifest, securityCache, latestDates)
}

// listFiles lists all relevant files from S3 for a given timeframe and date range
func (s3c *S3FlatFileClient) listFiles(ctx context.Context, prefix string, fromDate, toDate time.Time) ([]string, error) {
	var allKeys []string

	// Generate year-month prefixes to search
	prefixes := s3c.generateDatePrefixes(prefix, fromDate, toDate)

	for _, datePrefix := range prefixes {
		// Rate limit the request
		if err := s3c.limiter.Wait(ctx); err != nil {
			return nil, fmt.Errorf("rate limiter error: %w", err)
		}

		// List objects with pagination
		paginator := s3.NewListObjectsV2Paginator(s3c.client, &s3.ListObjectsV2Input{
			Bucket: aws.String(s3c.config.Bucket),
			Prefix: aws.String(datePrefix),
		})

		for paginator.HasMorePages() {
			page, err := paginator.NextPage(ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to list objects with prefix %s: %w", datePrefix, err)
			}

			for _, obj := range page.Contents {
				if obj.Key != nil && s3c.shouldProcessFile(*obj.Key) {
					allKeys = append(allKeys, *obj.Key)
				}
			}
		}
	}

	return allKeys, nil
}

// generateDatePrefixes generates S3 prefixes for the given date range
func (s3c *S3FlatFileClient) generateDatePrefixes(basePrefix string, fromDate, toDate time.Time) []string {
	var prefixes []string

	// Generate monthly prefixes
	current := time.Date(fromDate.Year(), fromDate.Month(), 1, 0, 0, 0, 0, time.UTC)
	end := time.Date(toDate.Year(), toDate.Month(), 1, 0, 0, 0, 0, time.UTC)

	for current.Before(end) || current.Equal(end) {
		prefix := fmt.Sprintf("%s/%04d/%02d/", basePrefix, current.Year(), current.Month())
		prefixes = append(prefixes, prefix)
		current = current.AddDate(0, 1, 0)
	}

	return prefixes
}

// shouldProcessFile determines if a file should be processed based on its key
func (s3c *S3FlatFileClient) shouldProcessFile(key string) bool {
	// Only process .csv.gz files
	return strings.HasSuffix(key, ".csv.gz")
}

// filterNewFiles filters out files that have already been processed
func (s3c *S3FlatFileClient) filterNewFiles(fileKeys []string, manifest *FileManifest) []string {
	manifest.mu.RLock()
	defer manifest.mu.RUnlock()

	var newFiles []string
	for _, key := range fileKeys {
		if _, exists := manifest.ProcessedFiles[key]; !exists {
			newFiles = append(newFiles, key)
		}
	}

	return newFiles
}

// processFilesWithWorkers processes files using separate worker pools for downloads and DB operations
func (s3c *S3FlatFileClient) processFilesWithWorkers(ctx context.Context, conn *data.Conn, timeframe TimeframeSpec, fileKeys []string, manifest *FileManifest, securityCache map[string]int, latestDates map[string]time.Time) error {
	var dlWG, dbWG sync.WaitGroup
	var processed int64
	var errors []error
	var errorsMutex sync.Mutex

	fileQueue := make(chan string, s3c.config.MaxDBWorkers*2)

	// Start DB workers
	for i := 0; i < s3c.config.MaxDBWorkers; i++ {
		dbWG.Add(1)
		go func() {
			defer dbWG.Done()
			for fileKey := range fileQueue {
				// Check if parent context is canceled
				select {
				case <-ctx.Done():
					log.Printf("Parent context canceled, stopping file processing")
					return
				default:
					// Continue processing
				}

				// Retry failed files up to 2 times
				var err error
				for retry := 0; retry < 3; retry++ {
					err = s3c.processFileFromS3(ctx, conn, timeframe, fileKey, manifest, securityCache, latestDates)
					if err == nil {
						break // Success, no need to retry
					}

					// Check if it's a timeout error that we should retry
					if retry < 2 && (strings.Contains(err.Error(), "context deadline exceeded") ||
						strings.Contains(err.Error(), "timeout")) {
						log.Printf("Retrying file %s (attempt %d/3) due to timeout: %v", fileKey, retry+2, err)
						time.Sleep(time.Duration(retry+1) * 5 * time.Second) // Exponential backoff
						continue
					}
					break // Don't retry other types of errors
				}

				if err != nil {
					errorsMutex.Lock()
					errors = append(errors, fmt.Errorf("failed to process file %s after retries: %w", fileKey, err))
					errorsMutex.Unlock()
					log.Printf("Error processing file %s: %v", fileKey, err)
				}
				count := atomic.AddInt64(&processed, 1)
				if count%10 == 0 || count == int64(len(fileKeys)) {
					log.Printf("Processed %d/%d files for %s", count, len(fileKeys), timeframe.Name)
				}
			}
		}()
	}

	// Queue up file keys directly
	dlWG.Add(1)
	go func() {
		defer dlWG.Done()
		for _, key := range fileKeys {
			select {
			case <-ctx.Done():
				log.Printf("Parent context canceled during file queuing")
				return
			case fileQueue <- key:
				// Successfully queued
			}
		}
	}()

	// Once queuing is done, close the queue so DB workers can finish
	go func() {
		dlWG.Wait()
		close(fileQueue)
	}()

	// Wait for all DB workers to complete
	dbWG.Wait()

	if len(errors) > 0 {
		return fmt.Errorf("failed to process %d files: %v", len(errors), errors[:min(5, len(errors))])
	}
	return nil
}

// processFileFromS3 processes a file directly from S3 without local disk storage
func (s3c *S3FlatFileClient) processFileFromS3(ctx context.Context, conn *data.Conn, timeframe TimeframeSpec, fileKey string, manifest *FileManifest, securityCache map[string]int, latestDates map[string]time.Time) error {
	// Get timeout configuration
	timeouts := getTimeoutConfig()

	// Create an independent context for file processing that won't be canceled by parent context
	// This prevents "context canceled" errors when the parent operation completes
	fileCtx, fileCancel := context.WithTimeout(context.Background(), timeouts.FileProcess)
	defer fileCancel()

	// Get file size for manifest (optional)
	var fileSize int64
	headResp, err := s3c.client.HeadObject(fileCtx, &s3.HeadObjectInput{
		Bucket: aws.String(s3c.config.Bucket),
		Key:    aws.String(fileKey),
	})
	if err == nil && headResp.ContentLength != nil {
		fileSize = *headResp.ContentLength
	}

	// Process the file directly from S3
	recordCount, err := s3c.processCSVFromS3(fileCtx, conn, timeframe, fileKey, securityCache, latestDates)
	if err != nil {
		return fmt.Errorf("failed to process CSV from S3 %s: %w", fileKey, err)
	}

	// Update manifest
	manifest.mu.Lock()
	manifest.ProcessedFiles[fileKey] = FileInfo{
		Key:         fileKey,
		Size:        fileSize,
		ProcessedAt: time.Now(),
		RecordCount: recordCount,
	}
	manifest.LastUpdated = time.Now()
	manifest.mu.Unlock()

	log.Printf("Successfully processed file %s with %d records", fileKey, recordCount)
	return nil
}

// processCSVFromS3 processes a CSV file directly from S3 using streaming COPY
func (s3c *S3FlatFileClient) processCSVFromS3(ctx context.Context, conn *data.Conn, timeframe TimeframeSpec, fileKey string, securityCache map[string]int, latestDates map[string]time.Time) (int64, error) {
	// Clone caches to avoid data races between workers
	localSec := make(map[string]int, len(securityCache))
	localDates := make(map[string]time.Time)
	for k, v := range securityCache {
		localSec[k] = v
	}
	for k, v := range latestDates {
		localDates[k] = v
	}

	// Log connection pool status before starting
	poolStats := conn.DB.Stat()
	log.Printf("processCSVFromS3: file %s starting - Pool stats: Total=%d, Acquired=%d, Idle=%d",
		fileKey, poolStats.TotalConns(), poolStats.AcquiredConns(), poolStats.IdleConns())

	// Log context deadline to diagnose potential cancellations
	if deadline, ok := ctx.Deadline(); ok {
		log.Printf("processCSVFromS3: file %s has context deadline at %v (remaining %v)", fileKey, deadline, time.Until(deadline))
	} else {
		log.Printf("processCSVFromS3: file %s context has no deadline", fileKey)
	}

	dummyIndices := CSVColumnIndices{}

	copySource := NewS3StreamingCopySource(
		s3c.client,
		s3c.config.Bucket,
		fileKey,
		dummyIndices,
		conn,
		timeframe.TableName,
		localSec,
		localDates,
		s3c.invalidTickerCache,
		s3c.invalidTickerMu,
		s3c.limiter,
	)

	// Perform streaming COPY using pgx CopyFrom
	log.Printf("Starting streaming COPY for file %s", fileKey)
	startTime := time.Now()

	rowsAffected, err := conn.DB.CopyFrom(ctx,
		pgx.Identifier{timeframe.TableName},
		[]string{"timestamp", "securityid", "open", "high", "low", "close", "volume"},
		copySource)

	// Log connection pool status after completion
	poolStats = conn.DB.Stat()
	log.Printf("processCSVFromS3: file %s completed - Pool stats: Total=%d, Acquired=%d, Idle=%d",
		fileKey, poolStats.TotalConns(), poolStats.AcquiredConns(), poolStats.IdleConns())

	if err != nil {
		if errors.Is(err, context.Canceled) || strings.Contains(err.Error(), "context canceled") {
			log.Printf("processCSVFromS3: context canceled for file %s after %v", fileKey, time.Since(startTime))
		}
		return 0, fmt.Errorf("failed to perform streaming COPY from S3: %w", err)
	}

	duration := time.Since(startTime)
	skippedCount := copySource.GetSkippedCount()
	if skippedCount > 0 {
		log.Printf("S3 streaming COPY completed for %s: %d rows affected, %d records processed, %d skipped in %v",
			fileKey, rowsAffected, copySource.GetRecordCount(), skippedCount, duration)
	} else {
		log.Printf("S3 streaming COPY completed for %s: %d rows affected, %d records processed in %v",
			fileKey, rowsAffected, copySource.GetRecordCount(), duration)
	}
	return copySource.GetRecordCount(), nil
}

// analyzeTable runs ANALYZE on a table to update statistics
func (s3c *S3FlatFileClient) analyzeTable(ctx context.Context, conn *pgxpool.Pool, tableName string) error {
	log.Printf("Running ANALYZE on table %s...", tableName)
	_, err := conn.Exec(ctx, fmt.Sprintf("ANALYZE %s", tableName))
	if err != nil {
		return fmt.Errorf("failed to analyze table %s: %w", tableName, err)
	}
	log.Printf("ANALYZE completed for table %s", tableName)
	return nil
}

// loadFileManifest loads the file processing manifest
func loadFileManifest(dataPath string) (*FileManifest, error) {
	manifestPath := filepath.Join(dataPath, "manifest.json")

	// Create new manifest if file doesn't exist
	if _, err := os.Stat(manifestPath); os.IsNotExist(err) {
		return &FileManifest{
			ProcessedFiles: make(map[string]FileInfo),
			LastUpdated:    time.Now(),
		}, nil
	}

	// Load existing manifest
	data, err := os.ReadFile(manifestPath)
	if err != nil {
		return nil, fmt.Errorf("failed to read manifest file: %w", err)
	}

	var manifest FileManifest
	if err := json.Unmarshal(data, &manifest); err != nil {
		return nil, fmt.Errorf("failed to parse manifest JSON: %w", err)
	}

	// Ensure ProcessedFiles map is initialized
	if manifest.ProcessedFiles == nil {
		manifest.ProcessedFiles = make(map[string]FileInfo)
	}

	return &manifest, nil
}

// saveFileManifest saves the file processing manifest
func saveFileManifest(manifest *FileManifest, dataPath string) error {
	manifestPath := filepath.Join(dataPath, "manifest.json")

	// Lock for reading
	manifest.mu.RLock()
	defer manifest.mu.RUnlock()

	// Update last updated time
	manifest.LastUpdated = time.Now()

	// Marshal to JSON
	data, err := json.MarshalIndent(manifest, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to marshal manifest to JSON: %w", err)
	}

	// Write to file
	if err := os.WriteFile(manifestPath, data, 0644); err != nil {
		return fmt.Errorf("failed to write manifest file: %w", err)
	}

	return nil
}

// min returns the minimum of two integers
func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

// getActiveTickers retrieves all currently active tickers from the securities table
func getActiveTickers(conn *data.Conn) ([]string, error) {
	timeouts := getTimeoutConfig()
	ctx, cancel := context.WithTimeout(context.Background(), timeouts.DBQuery)
	defer cancel()

	query := `SELECT ticker FROM securities WHERE maxDate IS NULL`
	rows, err := conn.DB.Query(ctx, query)
	if err != nil {
		return nil, fmt.Errorf("error querying active tickers: %w", err)
	}
	defer rows.Close()

	var tickers []string
	for rows.Next() {
		var ticker string
		if err := rows.Scan(&ticker); err != nil {
			return nil, fmt.Errorf("error scanning ticker: %w", err)
		}
		tickers = append(tickers, ticker)
	}

	if err := rows.Err(); err != nil {
		return nil, fmt.Errorf("error iterating ticker rows: %w", err)
	}

	return tickers, nil
}

// getLatestDateForTicker gets the latest timestamp for a ticker in a specific OHLCV table
func getLatestDateForTicker(conn *data.Conn, securityID int, tableName string) (*time.Time, error) {
	timeouts := getTimeoutConfig()
	ctx, cancel := context.WithTimeout(context.Background(), timeouts.DBQuery)
	defer cancel()

	// First check if the table exists
	tableExistsQuery := `
		SELECT EXISTS (
			SELECT FROM information_schema.tables 
			WHERE table_schema = 'public' AND table_name = $1
		)`

	var tableExists bool
	err := conn.DB.QueryRow(ctx, tableExistsQuery, tableName).Scan(&tableExists)
	if err != nil {
		return nil, fmt.Errorf("error checking if table %s exists: %w", tableName, err)
	}

	if !tableExists {
		return nil, fmt.Errorf("table %s does not exist - run database migrations first", tableName)
	}

	// Query for the latest timestamp
	query := fmt.Sprintf("SELECT MAX(timestamp) FROM %s WHERE securityid = $1", tableName)

	var latestDate *time.Time
	err = conn.DB.QueryRow(ctx, query, securityID).Scan(&latestDate)
	if err != nil {
		return nil, fmt.Errorf("error querying latest date from %s for securityID %d: %w", tableName, securityID, err)
	}

	return latestDate, nil
}

// EnsureDBStateOnStartup verifies that all OHLCV tables have proper indexes and compression settings
// This is a safety net to ensure the database is in a good state after startup or crashes
// Returns error if any table cannot be properly configured, causing startup to fail
func EnsureDBStateOnStartup(ctx context.Context, conn *pgxpool.Pool) error {
	log.Println("ðŸ” Starting database state verification...")

	for _, table := range OHLCVTables {
		log.Printf("Checking table: %s", table)
		if err := PostLoadCleanup(ctx, conn, table); err != nil {
			log.Printf("âŒ Failed to verify table %s: %v", table, err)
			return fmt.Errorf("startup check failed on %s: %w", table, err)
		}
		log.Printf("âœ… Table %s verified successfully", table)
	}

	log.Println("âœ… Database state verification completed successfully")
	return nil
}

// PreLoadSetup prepares the database for bulk loading by disabling compression and dropping secondary indexes
func PreLoadSetup(ctx context.Context, conn *pgxpool.Pool, tbl string) error {
	log.Printf("ðŸ”§ Preparing table %s for bulk loading...", tbl)

	// Enable restoring mode so Timescale disables background jobs during bulk load
	log.Printf("Turning ON TimescaleDB restoring mode before bulk copy...")
	if _, err := conn.Exec(ctx, `SET timescaledb.restoring = on;`); err != nil {
		return fmt.Errorf("error turning on TimescaleDB restoring mode prior to bulk copy: %w", err)
	}

	// Decompress existing chunks before bulk loading
	log.Printf("Decompressing existing chunks for %s...", tbl)
	_, err := conn.Exec(ctx, fmt.Sprintf(`
		SELECT decompress_chunk(c, true)
		FROM show_chunks('%s', NULL, NOW()) c
		WHERE is_compressed = true
	`, tbl))
	if err != nil {
		log.Printf("Warning: Could not decompress chunks for %s: %v", tbl, err)
		// Don't fail the entire operation if decompression fails
	} else {
		log.Printf("Successfully decompressed chunks for %s", tbl)
	}

	// Try to remove compression policy if it exists
	log.Printf("Attempting to remove compression policy for %s...", tbl)
	_, err = conn.Exec(ctx, fmt.Sprintf("SELECT remove_compression_policy('%s')", tbl))
	if err != nil {
		log.Printf("Warning: Could not remove compression policy for %s: %v", tbl, err)
		// Don't fail the entire operation if compression policy removal fails
	} else {
		log.Printf("Successfully removed compression policy for %s", tbl)
	}

	// Drop secondary indexes but keep primary key and constraint indexes
	log.Printf("Dropping secondary indexes for %s...", tbl)
	// Build dynamic SQL without parameters â€“ DO blocks do not accept $ placeholders
	query := fmt.Sprintf(`
	DO $$
	DECLARE idx record;
	BEGIN
	  FOR idx IN
		SELECT indexname
		FROM pg_indexes
		WHERE schemaname = 'public'
		  AND tablename  = '%s'
		  AND indexname NOT ILIKE '%%_pkey'
		  AND indexname NOT ILIKE '%%_constraint'
	  LOOP
		EXECUTE format('DROP INDEX IF EXISTS %%I', idx.indexname);
		RAISE NOTICE 'Dropped index: %%', idx.indexname;
	  END LOOP;
	END$$;`, tbl)

	_, err = conn.Exec(ctx, query)
	if err != nil {
		return fmt.Errorf("error dropping indexes for %s: %w", tbl, err)
	}

	log.Printf("âœ… Table %s prepared for bulk loading", tbl)
	return nil
}

// PostLoadCleanup restores normal database operations by re-enabling compression and recreating indexes
//
// Note: This function takes exclusive locks during index creation and should only be run
// during maintenance windows, not during market hours when the database is actively used.
func PostLoadCleanup(ctx context.Context, conn *pgxpool.Pool, tbl string) error {
	log.Printf("ðŸ”§ Restoring normal operations for table %s...", tbl)

	// Turn off restoring mode
	log.Printf("Turning off TimescaleDB restoring mode...")
	_, err := conn.Exec(ctx, `SET timescaledb.restoring = off;`)
	if err != nil {
		return fmt.Errorf("error turning off restoring mode: %w", err)
	}

	// Recreate securityid index if it doesn't exist
	indexName := fmt.Sprintf("%s_securityid_idx", tbl)
	log.Printf("Checking if index %s exists for table %s...", indexName, tbl)

	// First check if the index exists
	var indexExists bool
	err = conn.QueryRow(ctx, `
		SELECT EXISTS (
			SELECT 1 FROM pg_indexes
			WHERE schemaname='public'
			  AND tablename=$1
			  AND indexname=$2
		)
	`, tbl, indexName).Scan(&indexExists)

	if err != nil {
		return fmt.Errorf("error checking if index exists for %s: %w", tbl, err)
	}

	if !indexExists {
		log.Printf("Creating index %s on table %s...", indexName, tbl)
		// Note: Using non-concurrent index creation for TimescaleDB compatibility
		// This requires a maintenance window as it takes an exclusive lock
		_, err = conn.Exec(ctx, fmt.Sprintf("CREATE INDEX %s ON %s (securityid)", indexName, tbl))
		if err != nil {
			return fmt.Errorf("error creating index %s for %s: %w", indexName, tbl, err)
		}
		log.Printf("Successfully created index %s", indexName)
	} else {
		log.Printf("Index %s already exists, skipping creation", indexName)
	}

	// Re-add compression policy if it doesn't exist
	// Use a DO block to handle cases where timescaledb_information.policy_stats doesn't exist
	log.Printf("Checking and adding compression policy for table %s...", tbl)
	_, err = conn.Exec(ctx, fmt.Sprintf(`
		DO $$
		DECLARE
		  table_name text := '%s';
		BEGIN
		  -- Try to add compression policy with different function signatures
		  BEGIN
			-- Try the newer function signature first
			PERFORM add_compression_policy(table_name, INTERVAL '84 hours');
			RAISE NOTICE 'Added compression policy for: %%', table_name;
		  EXCEPTION
			WHEN undefined_function THEN
			  -- Try the older function signature
			  BEGIN
				PERFORM add_compress_chunks_policy(table_name, INTERVAL '84 hours');
				RAISE NOTICE 'Added compression policy (legacy) for: %%', table_name;
			  EXCEPTION
				WHEN OTHERS THEN
				  RAISE WARNING 'Could not add compression policy for %%: %%', table_name, SQLERRM;
			  END;
			WHEN OTHERS THEN
			  RAISE WARNING 'Could not add compression policy for %%: %%', table_name, SQLERRM;
		  END;
		END$$;
	`, tbl))

	if err != nil {
		return fmt.Errorf("error managing compression policy for %s: %w", tbl, err)
	}

	// Run ANALYZE to update table statistics for optimal query planning
	log.Printf("Running ANALYZE on table %s to update statistics...", tbl)
	_, err = conn.Exec(ctx, fmt.Sprintf("ANALYZE %s", tbl))
	if err != nil {
		log.Printf("Warning: failed to analyze table %s: %v", tbl, err)
		// Don't fail the entire operation if ANALYZE fails
	} else {
		log.Printf("Successfully ran ANALYZE on table %s", tbl)
	}

	log.Printf("âœ… Normal operations restored for table %s", tbl)
	return nil
}

// S3StreamingCopySource implements pgx.CopyFromSource for streaming directly from S3
type S3StreamingCopySource struct {
	s3Client           *s3.Client
	bucket             string
	key                string
	columnIndices      CSVColumnIndices
	securityCache      map[string]int
	latestDates        map[string]time.Time
	tableName          string
	conn               *data.Conn
	invalidTickerCache map[string]bool
	invalidTickerMu    *sync.RWMutex
	recordCount        int64
	skippedCount       int64
	nextRow            []interface{}
	hasNext            bool
	err                error

	// Streaming components
	reader     *csv.Reader
	gzReader   *gzip.Reader
	headerRead bool
	limiter    *rate.Limiter
	body       io.ReadCloser
}

// NewS3StreamingCopySource creates a new streaming copy source from S3
func NewS3StreamingCopySource(s3Client *s3.Client, bucket, key string, indices CSVColumnIndices, conn *data.Conn, tableName string, securityCache map[string]int, latestDates map[string]time.Time, invalidTickerCache map[string]bool, invalidTickerMu *sync.RWMutex, limiter *rate.Limiter) *S3StreamingCopySource {
	return &S3StreamingCopySource{
		s3Client:           s3Client,
		bucket:             bucket,
		key:                key,
		columnIndices:      indices,
		conn:               conn,
		tableName:          tableName,
		securityCache:      securityCache,
		latestDates:        latestDates,
		invalidTickerCache: invalidTickerCache,
		invalidTickerMu:    invalidTickerMu,
		limiter:            limiter,
		hasNext:            true,
	}
}

// Next implements pgx.CopyFromSource.Next()
func (ss *S3StreamingCopySource) Next() bool {
	if !ss.hasNext || ss.err != nil {
		return false
	}

	// Initialize streaming components on first call
	if ss.reader == nil {
		if err := ss.initializeStream(); err != nil {
			ss.err = err
			return false
		}
	}

	// Add a safety counter to prevent infinite loops
	maxRetries := 1000
	retryCount := 0

	for {
		if retryCount >= maxRetries {
			ss.err = fmt.Errorf("exceeded maximum retry attempts for invalid records")
			return false
		}
		retryCount++

		record, err := ss.reader.Read()
		if err != nil {
			if err == io.EOF {
				ss.hasNext = false
				if ss.gzReader != nil {
					ss.gzReader.Close()
				}
				if ss.body != nil {
					ss.body.Close()
				}
				return false
			}
			ss.err = err
			return false
		}

		ss.nextRow, err = ss.transformRecord(record)
		if err != nil {
			// Log the error but continue processing
			// Only log errors for non-obvious invalid tickers to reduce spam
			if retryCount%100 == 0 && !strings.Contains(err.Error(), "invalid ticker:") {
				log.Printf("Warning: skipping invalid record in %s: %v", ss.key, err)
			}
			ss.skippedCount++
			continue // skip invalid records, try next
		}
		if ss.nextRow == nil {
			ss.skippedCount++
			continue // skip nil rows (filtered)
		}
		ss.recordCount++
		return true
	}
}

// Values implements pgx.CopyFromSource.Values()
func (ss *S3StreamingCopySource) Values() ([]interface{}, error) {
	if ss.err != nil {
		return nil, ss.err
	}
	return ss.nextRow, nil
}

// Err implements pgx.CopyFromSource.Err()
func (ss *S3StreamingCopySource) Err() error {
	return ss.err
}

// GetRecordCount returns the number of records processed
func (ss *S3StreamingCopySource) GetRecordCount() int64 {
	return ss.recordCount
}

// GetSkippedCount returns the number of records skipped
func (ss *S3StreamingCopySource) GetSkippedCount() int64 {
	return ss.skippedCount
}

// initializeStream sets up the S3 â†’ gzip â†’ CSV streaming pipeline
func (ss *S3StreamingCopySource) initializeStream() error {
	// Rate limit the S3 request
	if err := ss.limiter.Wait(context.Background()); err != nil {
		return fmt.Errorf("rate limiter error: %w", err)
	}

	// Create S3 object reader using GetObject instead of Download
	// Use a longer timeout for S3 operations
	timeouts := getTimeoutConfig()
	s3Ctx, s3Cancel := context.WithTimeout(context.Background(), timeouts.FileProcess)
	defer s3Cancel()

	s3Resp, err := ss.s3Client.GetObject(s3Ctx, &s3.GetObjectInput{
		Bucket: aws.String(ss.bucket),
		Key:    aws.String(ss.key),
	})
	if err != nil {
		return fmt.Errorf("failed to get S3 object: %w", err)
	}

	ss.body = s3Resp.Body // save for later closing

	// Create gzip reader from S3 response body
	ss.gzReader, err = gzip.NewReader(ss.body)
	if err != nil {
		ss.body.Close()
		return fmt.Errorf("failed to create gzip reader: %w", err)
	}

	// Create CSV reader
	ss.reader = csv.NewReader(ss.gzReader)

	// Read and parse header
	header, err := ss.reader.Read()
	if err != nil {
		ss.body.Close()
		return fmt.Errorf("failed to read CSV header: %w", err)
	}

	// Parse header to get column indices
	ss.columnIndices, err = ss.parseCSVHeader(header)
	if err != nil {
		ss.body.Close()
		return fmt.Errorf("failed to parse CSV header: %w", err)
	}

	ss.headerRead = true
	return nil
}

// transformRecord transforms a CSV record to PostgreSQL COPY format
func (ss *S3StreamingCopySource) transformRecord(record []string) ([]interface{}, error) {
	// Get ticker and check if we should skip this record
	ticker := record[ss.columnIndices.Ticker]

	// Get security ID
	securityID, err := ss.getSecurityID(ticker)
	if err != nil {
		return nil, fmt.Errorf("failed to get security ID for ticker %s: %w", ticker, err)
	}

	// Check if we should skip this record based on latest date
	if ss.shouldSkipRecord(ticker, record[ss.columnIndices.Timestamp], securityID) {
		return nil, nil // Skip this record
	}

	// Parse timestamp
	timestamp, err := parseTimestamp(record[ss.columnIndices.Timestamp])
	if err != nil {
		return nil, fmt.Errorf("failed to parse timestamp: %w", err)
	}

	// Parse OHLCV values
	open, err := parseFloat(record[ss.columnIndices.Open])
	if err != nil {
		return nil, fmt.Errorf("failed to parse open: %w", err)
	}
	high, err := parseFloat(record[ss.columnIndices.High])
	if err != nil {
		return nil, fmt.Errorf("failed to parse high: %w", err)
	}
	low, err := parseFloat(record[ss.columnIndices.Low])
	if err != nil {
		return nil, fmt.Errorf("failed to parse low: %w", err)
	}
	close, err := parseFloat(record[ss.columnIndices.Close])
	if err != nil {
		return nil, fmt.Errorf("failed to parse close: %w", err)
	}
	volume, err := parseFloat(record[ss.columnIndices.Volume])
	if err != nil {
		return nil, fmt.Errorf("failed to parse volume: %w", err)
	}

	// Return values in PostgreSQL COPY order: timestamp, securityid, open, high, low, close, volume
	return []interface{}{
		timestamp,
		securityID,
		open,
		high,
		low,
		close,
		volume,
	}, nil
}

// getSecurityID gets or caches security ID for a ticker
func (ss *S3StreamingCopySource) getSecurityID(ticker string) (int, error) {
	// Skip obviously invalid tickers to reduce noise
	if ticker == "" || len(ticker) == 1 {
		return 0, fmt.Errorf("invalid ticker: %s", ticker)
	}

	if cachedID, ok := ss.securityCache[ticker]; ok {
		return cachedID, nil
	}

	// Check if this ticker is known to be invalid
	ss.invalidTickerMu.RLock()
	if ss.invalidTickerCache[ticker] {
		ss.invalidTickerMu.RUnlock()
		return 0, fmt.Errorf("ticker %s not found in securities table", ticker)
	}
	ss.invalidTickerMu.RUnlock()

	// Check connection pool availability before making DB call
	poolStats := ss.conn.DB.Stat()
	if poolStats.IdleConns() == 0 {
		// If no idle connections available, skip this record to avoid blocking
		log.Printf("Warning: No idle connections in pool, skipping ticker %s", ticker)
		return 0, fmt.Errorf("no available database connections")
	}

	securityID, err := postgres.GetCurrentSecurityID(ss.conn, ticker)
	if err != nil {
		// Cache invalid tickers to avoid repeated DB lookups
		ss.invalidTickerMu.Lock()
		ss.invalidTickerCache[ticker] = true
		ss.invalidTickerMu.Unlock()
		return 0, err
	}

	ss.securityCache[ticker] = securityID
	return securityID, nil
}

// shouldSkipRecord checks if a record should be skipped based on latest date
func (ss *S3StreamingCopySource) shouldSkipRecord(ticker, timestampStr string, securityID int) bool {
	// Get latest date for this ticker if not cached
	if _, ok := ss.latestDates[ticker]; !ok {
		// During streaming, avoid database calls to prevent connection pool exhaustion
		// Instead, assume no existing data and let the record through
		// This is safe because the preload should have caught most cases
		ss.latestDates[ticker] = time.Time{} // No existing data
		return false
	}

	// Parse record timestamp
	recordTime, err := parseTimestamp(timestampStr)
	if err != nil {
		return false // Don't skip if we can't parse
	}

	// Skip if record is older than or equal to latest date
	return !recordTime.After(ss.latestDates[ticker])
}

// parseCSVHeader parses the CSV header to determine column indices
func (ss *S3StreamingCopySource) parseCSVHeader(header []string) (CSVColumnIndices, error) {
	indices := CSVColumnIndices{
		Ticker:    -1,
		Timestamp: -1,
		Open:      -1,
		High:      -1,
		Low:       -1,
		Close:     -1,
		Volume:    -1,
	}

	for i, col := range header {
		switch strings.ToLower(col) {
		case "ticker", "symbol":
			indices.Ticker = i
		case "timestamp", "datetime", "date", "window_start", "t", "time": // Added Polygon aliases
			indices.Timestamp = i
		case "open", "o":
			indices.Open = i
		case "high", "h":
			indices.High = i
		case "low", "l":
			indices.Low = i
		case "close", "c":
			indices.Close = i
		case "volume", "v":
			indices.Volume = i
		}
	}

	// Validate required columns
	if indices.Ticker == -1 {
		return indices, fmt.Errorf("ticker column not found in CSV header")
	}
	if indices.Timestamp == -1 {
		return indices, fmt.Errorf("timestamp column not found in CSV header")
	}
	if indices.Open == -1 || indices.High == -1 || indices.Low == -1 || indices.Close == -1 {
		return indices, fmt.Errorf("OHLC columns not found in CSV header")
	}
	if indices.Volume == -1 {
		return indices, fmt.Errorf("volume column not found in CSV header")
	}

	return indices, nil
}

// parseTimestamp parses various timestamp formats
func parseTimestamp(timestampStr string) (time.Time, error) {
	// Common timestamp formats for Polygon flat files
	formats := []string{
		"2006-01-02T15:04:05Z",
		"2006-01-02T15:04:05.000Z",
		"2006-01-02T15:04:05.000000Z",
		"2006-01-02 15:04:05",
		"2006-01-02",
		time.RFC3339,
		time.RFC3339Nano,
	}

	for _, format := range formats {
		if parsed, err := time.Parse(format, timestampStr); err == nil {
			return parsed.UTC(), nil
		}
	}

	// Try parsing as Unix timestamp (seconds, milliseconds, or nanoseconds)
	if unixTime, err := strconv.ParseInt(timestampStr, 10, 64); err == nil {
		switch {
		case unixTime > 1e14: // Nanoseconds
			sec := unixTime / 1e9
			nsec := unixTime % 1e9
			return time.Unix(sec, nsec).UTC(), nil
		case unixTime > 1e10: // Milliseconds
			return time.Unix(unixTime/1000, (unixTime%1000)*1e6).UTC(), nil
		default: // Seconds
			return time.Unix(unixTime, 0).UTC(), nil
		}
	}

	return time.Time{}, fmt.Errorf("unable to parse timestamp: %s", timestampStr)
}

// parseFloat parses a string to float64
func parseFloat(s string) (float64, error) {
	if s == "" {
		return 0, nil
	}
	return strconv.ParseFloat(s, 64)
}

// Implement preloadLatestDates helper
func preloadLatestDates(conn *data.Conn, tableName string, securityCache map[string]int) (map[string]time.Time, error) {
	timeouts := getTimeoutConfig()
	ctx, cancel := context.WithTimeout(context.Background(), timeouts.DBQuery)
	defer cancel()

	// Check if table exists
	tableExistsQuery := `SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_schema = 'public' AND table_name = $1)`
	var tableExists bool
	err := conn.DB.QueryRow(ctx, tableExistsQuery, tableName).Scan(&tableExists)
	if err != nil {
		return nil, err
	}
	if !tableExists {
		return make(map[string]time.Time), nil
	}

	// Build list of security IDs
	var securityIDs []int
	for _, id := range securityCache {
		securityIDs = append(securityIDs, id)
	}
	if len(securityIDs) == 0 {
		return make(map[string]time.Time), nil
	}

	query := fmt.Sprintf(`SELECT securityid, MAX(timestamp) FROM %s WHERE securityid = ANY($1) GROUP BY securityid`, tableName)
	rows, err := conn.DB.Query(ctx, query, securityIDs)
	if err != nil {
		return nil, err
	}
	defer rows.Close()

	securityIDToTicker := make(map[int]string)
	for ticker, id := range securityCache {
		securityIDToTicker[id] = ticker
	}

	latestDates := make(map[string]time.Time)
	for rows.Next() {
		var securityID int
		var latest *time.Time
		if err := rows.Scan(&securityID, &latest); err != nil {
			return nil, err
		}
		if ticker, ok := securityIDToTicker[securityID]; ok && latest != nil {
			latestDates[ticker] = *latest
		}
	}
	missing := len(securityIDs) - len(latestDates)
	log.Printf("preloadLatestDates: loaded latest dates for %d securities; %d have no existing data in %s", len(latestDates), missing, tableName)
	return latestDates, nil
}
