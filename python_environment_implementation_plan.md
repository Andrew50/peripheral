# Python Environment Implementation Plan
## LLM-Generated Code Execution for SQL-Based Classification

### Overview
This plan outlines three approaches to implement a Python environment that can safely execute arbitrary LLM-generated Python code for running classifiers on data retrieved via SQL statements.

### Core Requirements
- Execute arbitrary Python code generated by LLMs
- Run classifiers on data from SQL queries
- Package functionality in reusable functions
- Ensure security and isolation
- Handle errors gracefully
- Provide consistent API interface

---

## Approach 1: Docker-Based Sandboxed Environment

### Architecture
- **Main Application**: Python FastAPI/Flask service
- **Execution Environment**: Docker containers for code isolation  
- **Database**: PostgreSQL/MySQL with connection pooling
- **Queue System**: Redis/Celery for async execution
- **Storage**: Shared volumes for data exchange

### Implementation Strategy

#### 1.1 Core Components
```
├── main_service/
│   ├── api/              # REST API endpoints
│   ├── sql_executor/     # SQL query execution
│   ├── code_validator/   # Basic code safety checks
│   └── docker_manager/   # Container lifecycle management
├── execution_container/
│   ├── Dockerfile        # Python runtime with restricted permissions
│   ├── executor.py       # Code execution handler
│   └── requirements.txt  # Allowed packages only
└── shared_data/          # Volume for data exchange
```

#### 1.2 Security Features
- **Container Isolation**: Each execution runs in separate container
- **Resource Limits**: CPU, memory, and time constraints
- **Network Isolation**: No external network access
- **File System**: Read-only except for designated output directories
- **Package Restrictions**: Pre-approved ML/data science libraries only

#### 1.3 Execution Flow
1. Receive SQL query and classifier code via API
2. Execute SQL query and serialize results to JSON
3. Validate Python code for basic safety
4. Spin up Docker container with code and data
5. Execute classifier with timeout limits
6. Collect results and cleanup container
7. Return classification results

#### 1.4 Pros/Cons
**Pros:**
- Strong isolation and security
- Scalable container orchestration
- Easy to manage dependencies
- Good for production environments

**Cons:**
- Higher resource overhead
- Container startup latency
- Requires Docker infrastructure
- More complex deployment

---

## Approach 2: Process-Based Virtual Environment with RestrictedPython

### Architecture
- **Main Application**: Python application with process spawning
- **Execution**: Subprocess with RestrictedPython
- **Database**: Direct database connections
- **Sandboxing**: OS-level process isolation + RestrictedPython
- **Monitoring**: Process monitoring and resource limits

### Implementation Strategy

#### 2.1 Core Components
```
├── main_app/
│   ├── api_handler.py        # Request handling
│   ├── sql_manager.py        # Database operations
│   ├── code_sanitizer.py     # Code validation/restriction
│   ├── process_executor.py   # Subprocess management
│   └── classifier_runner.py  # Classifier execution logic
├── execution_env/
│   ├── restricted_globals.py # Safe built-ins and imports
│   ├── ml_functions.py       # Pre-approved ML utilities
│   └── data_handlers.py      # Data processing functions
└── venv/                     # Isolated Python environment
```

#### 2.2 Security Features
- **RestrictedPython**: Compile-time code restrictions
- **Process Isolation**: Separate process per execution
- **Import Control**: Whitelist of allowed modules
- **Resource Limits**: Using `resource` module for limits
- **Timeout Management**: Process termination after time limit

#### 2.3 Code Execution Framework
```python
# Safe execution wrapper
def execute_classifier(sql_query, classifier_code, timeout=30):
    # Get data from SQL
    data = execute_sql_query(sql_query)
    
    # Prepare restricted environment
    safe_globals = get_restricted_globals()
    safe_globals['data'] = data
    safe_globals['ml_utils'] = get_ml_utilities()
    
    # Compile and execute with restrictions  
    compiled_code = compile_restricted(classifier_code, '<string>', 'exec')
    
    # Execute in subprocess with limits
    result = run_in_subprocess(compiled_code, safe_globals, timeout)
    return result
```

#### 2.4 Pros/Cons
**Pros:**
- Lower resource overhead than containers
- Faster execution startup
- Fine-grained control over Python environment
- Good balance of security and performance

**Cons:**
- Less isolation than containers
- Platform-dependent security features
- Complex RestrictedPython configuration
- Potential for security bypasses

---

## Approach 3: Cloud Function-as-a-Service (FaaS)

### Architecture
- **Main Application**: Python web service (FastAPI/Flask)
- **Execution**: AWS Lambda, Google Cloud Functions, or Azure Functions
- **Database**: Cloud database with connection pooling
- **Queue**: Cloud messaging service for async processing
- **Storage**: Cloud storage for data exchange

### Implementation Strategy

#### 3.1 Core Components
```
├── main_service/
│   ├── api/                  # REST API endpoints
│   ├── sql_service/          # Database query service
│   ├── function_deployer/    # Dynamic function deployment
│   └── result_collector/     # Collect and process results
├── function_templates/
│   ├── lambda_template.py    # AWS Lambda template
│   ├── requirements.txt      # Function dependencies
│   └── deployment_config.yml # Infrastructure as code
└── shared_storage/           # S3/GCS buckets for data
```

#### 3.2 Dynamic Function Deployment
```python
def deploy_classifier_function(sql_query, classifier_code):
    # Execute SQL and upload data to cloud storage
    data_url = upload_query_results(sql_query)
    
    # Generate function code from template
    function_code = generate_lambda_code(classifier_code, data_url)
    
    # Deploy function with timeout and memory limits
    function_arn = deploy_to_lambda(function_code, timeout=30, memory=512)
    
    # Execute function and get results
    result = invoke_lambda_function(function_arn)
    
    # Cleanup function after execution
    cleanup_lambda_function(function_arn)
    
    return result
```

#### 3.3 Security Features
- **Cloud Provider Security**: Built-in sandboxing
- **IAM Permissions**: Restricted access to resources
- **Network Isolation**: Functions run in isolated environments
- **Resource Limits**: Automatic CPU/memory/time limits
- **Audit Logging**: Complete execution audit trail

#### 3.4 Pros/Cons
**Pros:**
- Maximum security isolation
- Auto-scaling and managed infrastructure
- No server maintenance required
- Built-in monitoring and logging
- Pay-per-execution model

**Cons:**
- Higher latency due to cold starts
- Cloud vendor lock-in
- Cost can scale with usage
- Limited execution time (typically 15 minutes max)
- Network dependency for all operations

---

## Recommended Implementation Strategy

### Phase 1: Start with Approach 2 (Process-Based)
- **Rationale**: Good balance of security, performance, and complexity
- **Implementation Time**: 2-3 weeks
- **Use Cases**: Development, testing, moderate production loads

### Phase 2: Upgrade to Approach 1 (Docker-Based)
- **Rationale**: Better security and scalability for production
- **Migration Path**: Containerize the process-based solution
- **Implementation Time**: 1-2 weeks additional
- **Use Cases**: High-volume production environments

### Phase 3: Consider Approach 3 (FaaS) for Special Cases
- **Rationale**: For maximum security or cloud-native architectures
- **Use Cases**: Highly sensitive data, compliance requirements
- **Implementation Time**: 3-4 weeks
- **Cost**: Evaluate based on execution volume

---

## Common Components Across All Approaches

### SQL Query Interface
```python
class SQLExecutor:
    def execute_query(self, query: str) -> List[Dict]:
        # Validate query (SELECT only, no mutations)
        # Execute with connection pooling
        # Return results as structured data
        pass
```

### Function Registry System
```python
class FunctionRegistry:
    def register_ml_functions(self):
        # Pre-approved scikit-learn, pandas, numpy functions
        # Custom utility functions for data processing
        # Safe statistical and mathematical operations
        pass
```

### Error Handling and Monitoring
- Comprehensive logging for all executions
- Error categorization (syntax, runtime, timeout, resource)
- Performance metrics collection
- Security incident detection and alerting

### API Interface
```python
# Standard API endpoint for all approaches
@app.post("/execute-classifier")
async def execute_classifier(
    sql_query: str,
    classifier_code: str,
    timeout: int = 30,
    max_memory: int = 512
) -> ClassificationResult:
    pass
```

This plan provides a comprehensive roadmap for implementing a secure, scalable Python environment for executing LLM-generated classification code on SQL query results. 